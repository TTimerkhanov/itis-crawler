Machine Learning with Python: Training and Testing the Neural Network with MNIST data set Python Machine Learning Tutorial Machine Learning Machine Learning Terminologyk-nearest Neighbor ClassifierNeural Networks from Scratch in PythonNeural Network in Python using NumypyBackpropagation in Neural NetworksConfusion MatrixTraining and Testing with MNISTDropout Neural NetworksNeural Networks with ScikitMachine Learning with Scikit and PythonIntroduction Naive Bayes ClassifierNaive Bayes Classifier with ScikitIntroduction into Text Classification using Naive BayesPython Implementation of Text ClassificationDecision TreesRegression TreesRandom ForestsBoosting AlgorithmPrincipal Component AnalysisLinear Discriminant AnalysisExpectation Maximization and Gaussian Mixture ModelIntroduction into TensorFlow Quotes “Artificial intelligence would be the ultimate version of Google. The ultimate search engine that would understand everything on the web. It would understand exactly what you wanted, and it would give you the right thing. We’re nowhere near doing that now. However, we can get incrementally closer to that, and that is basically what we work on.” (Larry Wall) “Machine intelligence is the last invention that humanity will ever need to make.” (Nick Bostrom) This website is created and maintained by: Bernd Klein, On site trainings in Europe, Canada and the US. This website is free of annoying ads. We want to keep it like this. You can help with your donation: The need for donations Bernd Klein on Facebook Search this website: Classroom Training Courses This website contains a free and extensive online tutorial by Bernd Klein, using material from his classroom Python training courses. If you are interested in an instructor-led classroom training course, you may have a look at the Python classes by Bernd Klein at Bodenseo. © kabliczech - Fotolia.com Quote of the Day:"Much of my work has come from being lazy. I didn't like writing programs, and so, when I was working on the IBM 701, writing programs for computing missile trajectories, I started work on a programming system to make it easier to write programs." (John W. Backus) If you have the choice working with Python 2 or Python 3, we recomend to switch to Python 3! You can read our Python Tutorial to see what the differences are. Data Protection Declaration Data Protection Declaration Previous Chapter: Confusion Matrix Next Chapter: Dropout Neural Networks Neural Network Testing with MNIST The MNIST database (Modified National Institute of Standards and Technology database) of handwritten digits consists of a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. Additionally, the black and white images from NIST were size-normalized and centered to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels. This database is well liked for training and testing in the field of machine learning and image processing. It is a remixed subset of the original NIST datasets. One half of the 60,000 training images consist of images from NIST's testing dataset and the other half from Nist's training set. The 10,000 images from the testing set are similarly assembled. The MNIST dataset is used by researchers to test and compare their research results with others. The lowest error rates in literature are as low as 0.21 percent.1 Reading the MNIST data setThe images from the data set have the size 28 x 28. They are saved in the csv data files mnist_train.csv and mnist_test.csv. Every line of these files consists of an image, i.e. 785 numbers between 0 and 255. The first number of each line is the label, i.e. the digit which is depicted in the image. The following 784 numbers are the pixels of the 28 x 28 image. %matplotlib inline import numpy as np import matplotlib.pyplot as plt image_size = 28 # width and length no_of_different_labels = 10 # i.e. 0, 1, 2, 3, ..., 9 image_pixels = image_size * image_size data_path = "data/mnist/" train_data = np.loadtxt(data_path + "mnist_train.csv", delimiter=",") test_data = np.loadtxt(data_path + "mnist_test.csv", delimiter=",") We map the values of the image data into the interval [0.01, 0.99] by dividing the train_data and test_data arrays by (255 * 0.99 + 0.01) This way, we have input values between 0 and 1 but not including 0 and 1. fac = 255 *0.99 + 0.01 train_imgs = np.asfarray(train_data[:, 1:]) / fac test_imgs = np.asfarray(test_data[:, 1:]) / fac train_labels = np.asfarray(train_data[:, :1]) test_labels = np.asfarray(test_data[:, :1]) We need the labels in our calculations in a one-hot representation. We have 10 digits from 0 to 9, i.e. lr = np.arange(10). Turning a label into one-hot representation can be achieved with the command: (lr==label).astype(np.int) We demonstrate this in the following: import numpy as np lr = np.arange(10) for label in range(10): one_hot = (lr==label).astype(np.int) print("label: ", label, " in one-hot representation: ", one_hot) label: 0 in one-hot representation: [1 0 0 0 0 0 0 0 0 0] label: 1 in one-hot representation: [0 1 0 0 0 0 0 0 0 0] label: 2 in one-hot representation: [0 0 1 0 0 0 0 0 0 0] label: 3 in one-hot representation: [0 0 0 1 0 0 0 0 0 0] label: 4 in one-hot representation: [0 0 0 0 1 0 0 0 0 0] label: 5 in one-hot representation: [0 0 0 0 0 1 0 0 0 0] label: 6 in one-hot representation: [0 0 0 0 0 0 1 0 0 0] label: 7 in one-hot representation: [0 0 0 0 0 0 0 1 0 0] label: 8 in one-hot representation: [0 0 0 0 0 0 0 0 1 0] label: 9 in one-hot representation: [0 0 0 0 0 0 0 0 0 1] We are ready now to turn our labelled images into one-hot representations. Instead of zeroes and one, we create 0.01 and 0.99, which will be better for our calculations: lr = np.arange(no_of_different_labels) # transform labels into one hot representation train_labels_one_hot = (lr==train_labels).astype(np.float) test_labels_one_hot = (lr==test_labels).astype(np.float) # we don't want zeroes and ones in the labels neither: train_labels_one_hot[train_labels_one_hot==0] = 0.01 train_labels_one_hot[train_labels_one_hot==1] = 0.99 test_labels_one_hot[test_labels_one_hot==0] = 0.01 test_labels_one_hot[test_labels_one_hot==1] = 0.99 Before we start using the MNIST data sets with our neural network, we will have a look at same images: for i in range(10): img = train_imgs[i].reshape((28,28)) plt.imshow(img, cmap="Greys") plt.show() Dumping the Data for Faster ReloadYou may have noticed that it is quite slow to read in the data from the csv files. We will save the data in binary format with the dump function from the pickle module: import pickle with open("data/mnist/pickled_mnist.pkl", "bw") as fh: data = (train_imgs, test_imgs, train_labels, test_labels, train_labels_one_hot, test_labels_one_hot) pickle.dump(data, fh) We are able now to read in the data by using pickle.load. This is a lot faster than using loadtxt on the csv files: import pickle with open("data/mnist/pickled_mnist.pkl", "br") as fh: data = pickle.load(fh) train_imgs = data[0] test_imgs = data[1] train_labels = data[2] test_labels = data[3] train_labels_one_hot = data[4] test_labels_one_hot = data[5] image_size = 28 # width and length no_of_different_labels = 10 # i.e. 0, 1, 2, 3, ..., 9 image_pixels = image_size * image_size Classifying the DataWe will use the following neuronal network class for our first classification: import numpy as np @np.vectorize def sigmoid(x): return 1 / (1 + np.e ** -x) activation_function = sigmoid from scipy.stats import truncnorm def truncated_normal(mean=0, sd=1, low=0, upp=10): return truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd) class NeuralNetwork: def __init__(self, no_of_in_nodes, no_of_out_nodes, no_of_hidden_nodes, learning_rate): self.no_of_in_nodes = no_of_in_nodes self.no_of_out_nodes = no_of_out_nodes self.no_of_hidden_nodes = no_of_hidden_nodes self.learning_rate = learning_rate self.create_weight_matrices() def create_weight_matrices(self): """ A method to initialize the weight matrices of the neural network """ rad = 1 / np.sqrt(self.no_of_in_nodes) X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad) self.wih = X.rvs((self.no_of_hidden_nodes, self.no_of_in_nodes)) rad = 1 / np.sqrt(self.no_of_hidden_nodes) X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad) self.who = X.rvs((self.no_of_out_nodes, self.no_of_hidden_nodes)) def train(self, input_vector, target_vector): """ input_vector and target_vector can be tuple, list or ndarray """ input_vector = np.array(input_vector, ndmin=2).T target_vector = np.array(target_vector, ndmin=2).T output_vector1 = np.dot(self.wih, input_vector) output_hidden = activation_function(output_vector1) output_vector2 = np.dot(self.who, output_hidden) output_network = activation_function(output_vector2) output_errors = target_vector - output_network # update the weights: tmp = output_errors * output_network \ * (1.0 - output_network) tmp = self.learning_rate * np.dot(tmp, output_hidden.T) self.who += tmp # calculate hidden errors: hidden_errors = np.dot(self.who.T, output_errors) # update the weights: tmp = hidden_errors * output_hidden * \ (1.0 - output_hidden) self.wih += self.learning_rate \ * np.dot(tmp, input_vector.T) def run(self, input_vector): # input_vector can be tuple, list or ndarray input_vector = np.array(input_vector, ndmin=2).T output_vector = np.dot(self.wih, input_vector) output_vector = activation_function(output_vector) output_vector = np.dot(self.who, output_vector) output_vector = activation_function(output_vector) return output_vector def confusion_matrix(self, data_array, labels): cm = np.zeros((10, 10), int) for i in range(len(data_array)): res = self.run(data_array[i]) res_max = res.argmax() target = labels[i][0] cm[res_max, int(target)] += 1 return cm def precision(self, label, confusion_matrix): col = confusion_matrix[:, label] return confusion_matrix[label, label] / col.sum() def recall(self, label, confusion_matrix): row = confusion_matrix[label, :] return confusion_matrix[label, label] / row.sum() def evaluate(self, data, labels): corrects, wrongs = 0, 0 for i in range(len(data)): res = self.run(data[i]) res_max = res.argmax() if res_max == labels[i]: corrects += 1 else: wrongs += 1 return corrects, wrongs ANN = NeuralNetwork(no_of_in_nodes = image_pixels, no_of_out_nodes = 10, no_of_hidden_nodes = 100, learning_rate = 0.1) for i in range(len(train_imgs)): ANN.train(train_imgs[i], train_labels_one_hot[i]) for i in range(20): res = ANN.run(test_imgs[i]) print(test_labels[i], np.argmax(res), np.max(res)) [ 7.] 7 0.994814077597 [ 2.] 2 0.603771682531 [ 1.] 1 0.982015463842 [ 0.] 0 0.990292276621 [ 4.] 4 0.937906824567 [ 1.] 1 0.981472191404 [ 4.] 4 0.987517056632 [ 9.] 9 0.9842185187 [ 5.] 6 0.420854281372 [ 9.] 9 0.984952163258 [ 0.] 0 0.961267648873 [ 6.] 6 0.917602299303 [ 9.] 9 0.987062413114 [ 0.] 0 0.992667140202 [ 1.] 1 0.981738961657 [ 5.] 5 0.946351366465 [ 9.] 9 0.988766822771 [ 7.] 7 0.994488061024 [ 3.] 3 0.72374375894 [ 4.] 4 0.993429959931 corrects, wrongs = ANN.evaluate(train_imgs, train_labels) print("accruracy train: ", corrects / ( corrects + wrongs)) corrects, wrongs = ANN.evaluate(test_imgs, test_labels) print("accruracy: test", corrects / ( corrects + wrongs)) cm = ANN.confusion_matrix(train_imgs, train_labels) print(cm) for i in range(10): print("digit: ", i, "precision: ", ANN.precision(i, cm), "recall: ", ANN.recall(i, cm)) accruracy train: 0.94945 accruracy: test 0.9491 [[5816 1 48 18 16 57 46 19 27 34] [ 1 6630 53 23 13 27 14 74 93 10] [ 3 22 5507 70 14 15 4 51 21 5] [ 5 28 99 5795 2 110 1 27 73 64] [ 9 13 54 6 5480 41 8 39 23 66] [ 4 1 5 52 1 4948 35 1 6 5] [ 26 2 44 19 53 83 5782 4 27 2] [ 0 10 38 40 4 6 0 5822 1 28] [ 48 22 96 62 9 65 28 25 5478 26] [ 11 13 14 46 250 69 0 203 102 5709]] digit: 0 precision: 0.981934830322 recall: 0.956264386715 digit: 1 precision: 0.983387718778 recall: 0.955606803113 digit: 2 precision: 0.924303457536 recall: 0.964110644258 digit: 3 precision: 0.945196542163 recall: 0.934074790458 digit: 4 precision: 0.938034919548 recall: 0.954870186444 digit: 5 precision: 0.912746725696 recall: 0.978252273626 digit: 6 precision: 0.977019263265 recall: 0.956967891427 digit: 7 precision: 0.929289704709 recall: 0.978651874265 digit: 8 precision: 0.936250213639 recall: 0.934971838198 digit: 9 precision: 0.959657085224 recall: 0.889668069191 Multiple RunsWe can repeat the training multiple times. Each run is called an "epoch". epochs = 3 NN = NeuralNetwork(no_of_in_nodes = image_pixels, no_of_out_nodes = 10, no_of_hidden_nodes = 100, learning_rate = 0.1) for epoch in range(epochs): print("epoch: ", epoch) for i in range(len(train_imgs)): NN.train(train_imgs[i], train_labels_one_hot[i]) corrects, wrongs = NN.evaluate(train_imgs, train_labels) print("accruracy train: ", corrects / ( corrects + wrongs)) corrects, wrongs = NN.evaluate(test_imgs, test_labels) print("accruracy: test", corrects / ( corrects + wrongs)) epoch: 0 accruracy train: 0.9478 accruracy: test 0.9468 epoch: 1 accruracy train: 0.9626333333333333 accruracy: test 0.9589 epoch: 2 accruracy train: 0.96765 accruracy: test 0.9625 We want to do the multiple training of the training set inside of our network. To this purpose we rewrite the method train and add a method train_single. train_single is more or less what we called 'train' before. Whereas the new 'train' method is doing the epoch counting. For testing purposes, we save the weight matrices after each epoch in the list intermediate_weights. This list is returned as the output of train: import numpy as np @np.vectorize def sigmoid(x): return 1 / (1 + np.e ** -x) activation_function = sigmoid from scipy.stats import truncnorm def truncated_normal(mean=0, sd=1, low=0, upp=10): return truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd) class NeuralNetwork: def __init__(self, no_of_in_nodes, no_of_out_nodes, no_of_hidden_nodes, learning_rate): self.no_of_in_nodes = no_of_in_nodes self.no_of_out_nodes = no_of_out_nodes self.no_of_hidden_nodes = no_of_hidden_nodes self.learning_rate = learning_rate self.create_weight_matrices() def create_weight_matrices(self): """ A method to initialize the weight matrices of the neural network""" rad = 1 / np.sqrt(self.no_of_in_nodes) X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad) self.wih = X.rvs((self.no_of_hidden_nodes, self.no_of_in_nodes)) rad = 1 / np.sqrt(self.no_of_hidden_nodes) X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad) self.who = X.rvs((self.no_of_out_nodes, self.no_of_hidden_nodes)) def train_single(self, input_vector, target_vector): """ input_vector and target_vector can be tuple, list or ndarray """ output_vectors = [] input_vector = np.array(input_vector, ndmin=2).T target_vector = np.array(target_vector, ndmin=2).T output_vector1 = np.dot(self.wih, input_vector) output_hidden = activation_function(output_vector1) output_vector2 = np.dot(self.who, output_hidden) output_network = activation_function(output_vector2) output_errors = target_vector - output_network # update the weights: tmp = output_errors * output_network * \ (1.0 - output_network) tmp = self.learning_rate * np.dot(tmp, output_hidden.T) self.who += tmp # calculate hidden errors: hidden_errors = np.dot(self.who.T, output_errors) # update the weights: tmp = hidden_errors * output_hidden * (1.0 - output_hidden) self.wih += self.learning_rate * np.dot(tmp, input_vector.T) def train(self, data_array, labels_one_hot_array, epochs=1, intermediate_results=False): intermediate_weights = [] for epoch in range(epochs): print("*", end="") for i in range(len(data_array)): self.train_single(data_array[i], labels_one_hot_array[i]) if intermediate_results: intermediate_weights.append((self.wih.copy(), self.who.copy())) return intermediate_weights def confusion_matrix(self, data_array, labels): cm = {} for i in range(len(data_array)): res = self.run(data_array[i]) res_max = res.argmax() target = labels[i][0] if (target, res_max) in cm: cm[(target, res_max)] += 1 else: cm[(target, res_max)] = 1 return cm def run(self, input_vector): """ input_vector can be tuple, list or ndarray """ input_vector = np.array(input_vector, ndmin=2).T output_vector = np.dot(self.wih, input_vector) output_vector = activation_function(output_vector) output_vector = np.dot(self.who, output_vector) output_vector = activation_function(output_vector) return output_vector def evaluate(self, data, labels): corrects, wrongs = 0, 0 for i in range(len(data)): res = self.run(data[i]) res_max = res.argmax() if res_max == labels[i]: corrects += 1 else: wrongs += 1 return corrects, wrongs epochs = 10 ANN = NeuralNetwork(no_of_in_nodes = image_pixels, no_of_out_nodes = 10, no_of_hidden_nodes = 100, learning_rate = 0.15) weights = ANN.train(train_imgs, train_labels_one_hot, epochs=epochs, intermediate_results=True) ********** cm = ANN.confusion_matrix(train_imgs, train_labels) print(ANN.run(train_imgs[i])) [[ 1.26008523e-02] [ 1.36319944e-02] [ 5.93165174e-03] [ 5.65000996e-03] [ 2.72212976e-03] [ 3.26118452e-03] [ 4.16713243e-04] [ 1.03747031e-03] [ 9.29099059e-01] [ 8.70810282e-03]] cm = list(cm.items()) print(sorted(cm)) [((0.0, 0), 5813), ((0.0, 2), 5), ((0.0, 3), 6), ((0.0, 4), 7), ((0.0, 5), 7), ((0.0, 6), 14), ((0.0, 8), 57), ((0.0, 9), 14), ((1.0, 1), 6650), ((1.0, 2), 13), ((1.0, 3), 13), ((1.0, 4), 22), ((1.0, 5), 1), ((1.0, 6), 3), ((1.0, 7), 10), ((1.0, 8), 28), ((1.0, 9), 2), ((2.0, 0), 25), ((2.0, 1), 30), ((2.0, 2), 5643), ((2.0, 3), 77), ((2.0, 4), 13), ((2.0, 5), 10), ((2.0, 6), 10), ((2.0, 7), 34), ((2.0, 8), 97), ((2.0, 9), 19), ((3.0, 0), 6), ((3.0, 1), 4), ((3.0, 2), 22), ((3.0, 3), 5898), ((3.0, 4), 5), ((3.0, 5), 25), ((3.0, 6), 3), ((3.0, 7), 33), ((3.0, 8), 75), ((3.0, 9), 60), ((4.0, 0), 3), ((4.0, 1), 8), ((4.0, 2), 6), ((4.0, 4), 5711), ((4.0, 6), 13), ((4.0, 7), 4), ((4.0, 8), 6), ((4.0, 9), 91), ((5.0, 0), 13), ((5.0, 1), 3), ((5.0, 2), 4), ((5.0, 3), 42), ((5.0, 4), 13), ((5.0, 5), 5252), ((5.0, 6), 22), ((5.0, 7), 4), ((5.0, 8), 47), ((5.0, 9), 21), ((6.0, 0), 19), ((6.0, 1), 6), ((6.0, 2), 7), ((6.0, 4), 18), ((6.0, 5), 24), ((6.0, 6), 5784), ((6.0, 8), 48), ((6.0, 9), 12), ((7.0, 0), 3), ((7.0, 1), 18), ((7.0, 2), 17), ((7.0, 3), 6), ((7.0, 4), 40), ((7.0, 5), 3), ((7.0, 6), 6), ((7.0, 7), 5991), ((7.0, 8), 26), ((7.0, 9), 155), ((8.0, 0), 13), ((8.0, 1), 20), ((8.0, 2), 7), ((8.0, 3), 30), ((8.0, 4), 22), ((8.0, 5), 14), ((8.0, 6), 17), ((8.0, 7), 5), ((8.0, 8), 5661), ((8.0, 9), 62), ((9.0, 0), 19), ((9.0, 1), 3), ((9.0, 2), 3), ((9.0, 3), 29), ((9.0, 4), 30), ((9.0, 5), 8), ((9.0, 6), 1), ((9.0, 7), 21), ((9.0, 8), 19), ((9.0, 9), 5816)] for i in range(epochs): print("epoch: ", i) ANN.wih = weights[i][0] ANN.who = weights[i][1] corrects, wrongs = ANN.evaluate(train_imgs, train_labels) print("accruracy train: ", corrects / ( corrects + wrongs)) corrects, wrongs = ANN.evaluate(test_imgs, test_labels) print("accruracy: test", corrects / ( corrects + wrongs)) epoch: 0 accruracy train: 0.9423333333333334 accruracy: test 0.9422 epoch: 1 accruracy train: 0.95965 accruracy: test 0.9555 epoch: 2 accruracy train: 0.9658 accruracy: test 0.957 epoch: 3 accruracy train: 0.9683333333333334 accruracy: test 0.9621 epoch: 4 accruracy train: 0.9685333333333334 accruracy: test 0.9598 epoch: 5 accruracy train: 0.9720666666666666 accruracy: test 0.9621 epoch: 6 accruracy train: 0.9710833333333333 accruracy: test 0.9609 epoch: 7 accruracy train: 0.9731166666666666 accruracy: test 0.9628 epoch: 8 accruracy train: 0.9706 accruracy: test 0.9591 epoch: 9 accruracy train: 0.9703166666666667 accruracy: test 0.9579 With Bias Nodes import numpy as np @np.vectorize def sigmoid(x): return 1 / (1 + np.e ** -x) activation_function = sigmoid from scipy.stats import truncnorm def truncated_normal(mean=0, sd=1, low=0, upp=10): return truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd) class NeuralNetwork: def __init__(self, no_of_in_nodes, no_of_out_nodes, no_of_hidden_nodes, learning_rate, bias=None ): self.no_of_in_nodes = no_of_in_nodes self.no_of_out_nodes = no_of_out_nodes self.no_of_hidden_nodes = no_of_hidden_nodes self.learning_rate = learning_rate self.bias = bias self.create_weight_matrices() def create_weight_matrices(self): """ A method to initialize the weight matrices of the neural network with optional bias nodes """ bias_node = 1 if self.bias else 0 rad = 1 / np.sqrt(self.no_of_in_nodes + bias_node) X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad) self.wih = X.rvs((self.no_of_hidden_nodes, self.no_of_in_nodes + bias_node)) rad = 1 / np.sqrt(self.no_of_hidden_nodes + bias_node) X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad) self.who = X.rvs((self.no_of_out_nodes, self.no_of_hidden_nodes + bias_node)) def train(self, input_vector, target_vector): """ input_vector and target_vector can be tuple, list or ndarray """ bias_node = 1 if self.bias else 0 if self.bias: # adding bias node to the end of the inpuy_vector input_vector = np.concatenate((input_vector, [self.bias]) ) input_vector = np.array(input_vector, ndmin=2).T target_vector = np.array(target_vector, ndmin=2).T output_vector1 = np.dot(self.wih, input_vector) output_hidden = activation_function(output_vector1) if self.bias: output_hidden = np.concatenate((output_hidden, [[self.bias]]) ) output_vector2 = np.dot(self.who, output_hidden) output_network = activation_function(output_vector2) output_errors = target_vector - output_network # update the weights: tmp = output_errors * output_network * (1.0 - output_network) tmp = self.learning_rate * np.dot(tmp, output_hidden.T) self.who += tmp # calculate hidden errors: hidden_errors = np.dot(self.who.T, output_errors) # update the weights: tmp = hidden_errors * output_hidden * (1.0 - output_hidden) if self.bias: x = np.dot(tmp, input_vector.T)[:-1,:] else: x = np.dot(tmp, input_vector.T) self.wih += self.learning_rate * x def run(self, input_vector): """ input_vector can be tuple, list or ndarray """ if self.bias: # adding bias node to the end of the inpuy_vector input_vector = np.concatenate((input_vector, [1]) ) input_vector = np.array(input_vector, ndmin=2).T output_vector = np.dot(self.wih, input_vector) output_vector = activation_function(output_vector) if self.bias: output_vector = np.concatenate( (output_vector, [[1]]) ) output_vector = np.dot(self.who, output_vector) output_vector = activation_function(output_vector) return output_vector def evaluate(self, data, labels): corrects, wrongs = 0, 0 for i in range(len(data)): res = self.run(data[i]) res_max = res.argmax() if res_max == labels[i]: corrects += 1 else: wrongs += 1 return corrects, wrongs ANN = NeuralNetwork(no_of_in_nodes=image_pixels, no_of_out_nodes=10, no_of_hidden_nodes=200, learning_rate=0.1, bias=None) for i in range(len(train_imgs)): ANN.train(train_imgs[i], train_labels_one_hot[i]) for i in range(20): res = ANN.run(test_imgs[i]) print(test_labels[i], np.argmax(res), np.max(res)) [ 7.] 7 0.987360189348 [ 2.] 2 0.749410818011 [ 1.] 1 0.990936932334 [ 0.] 0 0.982162095094 [ 4.] 4 0.972015930565 [ 1.] 1 0.986745897455 [ 4.] 4 0.979223439296 [ 9.] 9 0.964589260844 [ 5.] 6 0.42949267086 [ 9.] 9 0.971961659342 [ 0.] 0 0.979675421627 [ 6.] 6 0.843565954906 [ 9.] 9 0.989992702963 [ 0.] 0 0.98947745372 [ 1.] 1 0.988959403957 [ 5.] 5 0.889105662605 [ 9.] 9 0.99279904523 [ 7.] 7 0.985973174432 [ 3.] 3 0.711112673804 [ 4.] 4 0.993171047267 corrects, wrongs = ANN.evaluate(train_imgs, train_labels) print("accruracy train: ", corrects / ( corrects + wrongs)) corrects, wrongs = ANN.evaluate(test_imgs, test_labels) print("accruracy: test", corrects / ( corrects + wrongs)) accruracy train: 0.9504666666666667 accruracy: test 0.9493 Version with Bias and Epochs: import numpy as np @np.vectorize def sigmoid(x): return 1 / (1 + np.e ** -x) activation_function = sigmoid from scipy.stats import truncnorm def truncated_normal(mean=0, sd=1, low=0, upp=10): return truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd) class NeuralNetwork: def __init__(self, no_of_in_nodes, no_of_out_nodes, no_of_hidden_nodes, learning_rate, bias=None ): self.no_of_in_nodes = no_of_in_nodes self.no_of_out_nodes = no_of_out_nodes self.no_of_hidden_nodes = no_of_hidden_nodes self.learning_rate = learning_rate self.bias = bias self.create_weight_matrices() def create_weight_matrices(self): """ A method to initialize the weight matrices of the neural network with optional bias nodes""" bias_node = 1 if self.bias else 0 rad = 1 / np.sqrt(self.no_of_in_nodes + bias_node) X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad) self.wih = X.rvs((self.no_of_hidden_nodes, self.no_of_in_nodes + bias_node)) rad = 1 / np.sqrt(self.no_of_hidden_nodes + bias_node) X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad) self.who = X.rvs((self.no_of_out_nodes, self.no_of_hidden_nodes + bias_node)) def train_single(self, input_vector, target_vector): """ input_vector and target_vector can be tuple, list or ndarray """ bias_node = 1 if self.bias else 0 if self.bias: # adding bias node to the end of the inpuy_vector input_vector = np.concatenate( (input_vector, [self.bias]) ) output_vectors = [] input_vector = np.array(input_vector, ndmin=2).T target_vector = np.array(target_vector, ndmin=2).T output_vector1 = np.dot(self.wih, input_vector) output_hidden = activation_function(output_vector1) if self.bias: output_hidden = np.concatenate((output_hidden, [[self.bias]]) ) output_vector2 = np.dot(self.who, output_hidden) output_network = activation_function(output_vector2) output_errors = target_vector - output_network # update the weights: tmp = output_errors * output_network * (1.0 - output_network) tmp = self.learning_rate * np.dot(tmp, output_hidden.T) self.who += tmp # calculate hidden errors: hidden_errors = np.dot(self.who.T, output_errors) # update the weights: tmp = hidden_errors * output_hidden * (1.0 - output_hidden) if self.bias: x = np.dot(tmp, input_vector.T)[:-1,:] else: x = np.dot(tmp, input_vector.T) self.wih += self.learning_rate * x def train(self, data_array, labels_one_hot_array, epochs=1, intermediate_results=False): intermediate_weights = [] for epoch in range(epochs): for i in range(len(data_array)): self.train_single(data_array[i], labels_one_hot_array[i]) if intermediate_results: intermediate_weights.append((self.wih.copy(), self.who.copy())) return intermediate_weights def run(self, input_vector): # input_vector can be tuple, list or ndarray if self.bias: # adding bias node to the end of the inpuy_vector input_vector = np.concatenate( (input_vector, [self.bias]) ) input_vector = np.array(input_vector, ndmin=2).T output_vector = np.dot(self.wih, input_vector) output_vector = activation_function(output_vector) if self.bias: output_vector = np.concatenate( (output_vector, [[self.bias]]) ) output_vector = np.dot(self.who, output_vector) output_vector = activation_function(output_vector) return output_vector def evaluate(self, data, labels): corrects, wrongs = 0, 0 for i in range(len(data)): res = self.run(data[i]) res_max = res.argmax() if res_max == labels[i]: corrects += 1 else: wrongs += 1 return corrects, wrongs epochs = 12 network = NeuralNetwork(no_of_in_nodes=image_pixels, no_of_out_nodes=10, no_of_hidden_nodes=100, learning_rate=0.1, bias=None) weights = network.train(train_imgs, train_labels_one_hot, epochs=epochs, intermediate_results=True) for epoch in range(epochs): print("epoch: ", epoch) network.wih = weights[epoch][0] network.who = weights[epoch][1] corrects, wrongs = network.evaluate(train_imgs, train_labels) print("accruracy train: ", corrects / ( corrects + wrongs)) corrects, wrongs = network.evaluate(test_imgs, test_labels) print("accruracy test: ", corrects / ( corrects + wrongs)) epoch: 0 accruracy train: 0.9442166666666667 accruracy test: 0.9438 epoch: 1 accruracy train: 0.9585333333333333 accruracy test: 0.9533 epoch: 2 accruracy train: 0.9651833333333333 accruracy test: 0.9578 epoch: 3 accruracy train: 0.96995 accruracy test: 0.9598 epoch: 4 accruracy train: 0.9728333333333333 accruracy test: 0.9629 epoch: 5 accruracy train: 0.9733833333333334 accruracy test: 0.9604 epoch: 6 accruracy train: 0.9707333333333333 accruracy test: 0.9582 epoch: 7 accruracy train: 0.9736333333333334 accruracy test: 0.9621 epoch: 8 accruracy train: 0.9760333333333333 accruracy test: 0.9641 epoch: 9 accruracy train: 0.9779 accruracy test: 0.9629 epoch: 10 accruracy train: 0.9776333333333334 accruracy test: 0.9629 epoch: 11 accruracy train: 0.97755 accruracy test: 0.9625 epochs = 12 with open("nist_tests.csv", "w") as fh_out: for hidden_nodes in [20, 50, 100, 120, 150]: for learning_rate in [0.01, 0.05, 0.1, 0.2]: for bias in [None, 0.5]: network = NeuralNetwork(no_of_in_nodes=image_pixels, no_of_out_nodes=10, no_of_hidden_nodes=hidden_nodes, learning_rate=learning_rate, bias=bias) weights = network.train(train_imgs, train_labels_one_hot, epochs=epochs, intermediate_results=True) for epoch in range(epochs): print("*", end="") network.wih = weights[epoch][0] network.who = weights[epoch][1] train_corrects, train_wrongs = network.evaluate(train_imgs, train_labels) test_corrects, test_wrongs = network.evaluate(test_imgs, test_labels) outstr = str(hidden_nodes) + " " + str(learning_rate) + " " + str(bias) outstr += " " + str(epoch) + " " outstr += str(train_corrects / (train_corrects + train_wrongs)) + " " outstr += str(train_wrongs / (train_corrects + train_wrongs)) + " " outstr += str(test_corrects / (test_corrects + test_wrongs)) + " " outstr += str(test_wrongs / (test_corrects + test_wrongs)) fh_out.write(outstr + "\n" ) fh_out.flush() The file nist_tests_20_50_100_120_150.csv contains the results from a run of the previous program. Networks with multiple hidden layersWe will write a new neural network class, in which we can define an arbitrary number of hidden layers. The code is also improved, because the weight matrices are now build inside of a loop instead redundant code: import numpy as np from scipy.special import expit as activation_function from scipy.stats import truncnorm def truncated_normal(mean=0, sd=1, low=0, upp=10): return truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd) class NeuralNetwork: def __init__(self, network_structure, # ie. [input_nodes, hidden1_nodes, ... , hidden_n_nodes, output_nodes] learning_rate, bias=None ): self.structure = network_structure self.learning_rate = learning_rate self.bias = bias self.create_weight_matrices() def create_weight_matrices(self): bias_node = 1 if self.bias else 0 self.weights_matrices = [] layer_index = 1 no_of_layers = len(self.structure) while layer_index < no_of_layers: nodes_in = self.structure[layer_index-1] nodes_out = self.structure[layer_index] n = (nodes_in + bias_node) * nodes_out rad = 1 / np.sqrt(nodes_in) X = truncated_normal(mean=2, sd=1, low=-rad, upp=rad) wm = X.rvs(n).reshape((nodes_out, nodes_in + bias_node)) self.weights_matrices.append(wm) layer_index += 1 def train(self, input_vector, target_vector): """ input_vector and target_vector can be tuple, list or ndarray """ no_of_layers = len(self.structure) input_vector = np.array(input_vector, ndmin=2).T layer_index = 0 # The output/input vectors of the various layers: res_vectors = [input_vector] while layer_index < no_of_layers - 1: in_vector = res_vectors[-1] if self.bias: # adding bias node to the end of the 'input'_vector in_vector = np.concatenate( (in_vector, [[self.bias]]) ) res_vectors[-1] = in_vector x = np.dot(self.weights_matrices[layer_index], in_vector) out_vector = activation_function(x) # the output of one layer is the input of the next one: res_vectors.append(out_vector) layer_index += 1 layer_index = no_of_layers - 1 target_vector = np.array(target_vector, ndmin=2).T # The input vectors to the various layers output_errors = target_vector - out_vector while layer_index > 0: out_vector = res_vectors[layer_index] in_vector = res_vectors[layer_index-1] if self.bias and not layer_index==(no_of_layers-1): out_vector = out_vector[:-1,:].copy() tmp = output_errors * out_vector * (1.0 - out_vector) tmp = np.dot(tmp, in_vector.T) #if self.bias: # tmp = tmp[:-1,:] self.weights_matrices[layer_index-1] += self.learning_rate * tmp output_errors = np.dot(self.weights_matrices[layer_index-1].T, output_errors) if self.bias: output_errors = output_errors[:-1,:] layer_index -= 1 def run(self, input_vector): # input_vector can be tuple, list or ndarray no_of_layers = len(self.structure) if self.bias: # adding bias node to the end of the inpuy_vector input_vector = np.concatenate( (input_vector, [self.bias]) ) in_vector = np.array(input_vector, ndmin=2).T layer_index = 1 # The input vectors to the various layers while layer_index < no_of_layers: x = np.dot(self.weights_matrices[layer_index-1], in_vector) out_vector = activation_function(x) # input vector for next layer in_vector = out_vector if self.bias: in_vector = np.concatenate( (in_vector, [[self.bias]]) ) layer_index += 1 return out_vector def evaluate(self, data, labels): corrects, wrongs = 0, 0 for i in range(len(data)): res = self.run(data[i]) res_max = res.argmax() if res_max == labels[i]: corrects += 1 else: wrongs += 1 return corrects, wrongs ANN = NeuralNetwork(network_structure=[image_pixels, 50, 50, 10], learning_rate=0.1, bias=None) for i in range(len(train_imgs)): ANN.train(train_imgs[i], train_labels_one_hot[i]) corrects, wrongs = ANN.evaluate(train_imgs, train_labels) print("accruracy train: ", corrects / ( corrects + wrongs)) corrects, wrongs = ANN.evaluate(test_imgs, test_labels) print("accruracy: test", corrects / ( corrects + wrongs)) accruracy train: 0.8851 accruracy: test 0.8879 Networks with multiple hidden layers and Epochs import numpy as np from scipy.special import expit as activation_function from scipy.stats import truncnorm def truncated_normal(mean=0, sd=1, low=0, upp=10): return truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd) class NeuralNetwork: def __init__(self, network_structure, # ie. [input_nodes, hidden1_nodes, ... , hidden_n_nodes, output_nodes] learning_rate, bias=None ): self.structure = network_structure self.learning_rate = learning_rate self.bias = bias self.create_weight_matrices() def create_weight_matrices(self): X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5) bias_node = 1 if self.bias else 0 self.weights_matrices = [] layer_index = 1 no_of_layers = len(self.structure) while layer_index < no_of_layers: nodes_in = self.structure[layer_index-1] nodes_out = self.structure[layer_index] n = (nodes_in + bias_node) * nodes_out rad = 1 / np.sqrt(nodes_in) X = truncated_normal(mean=2, sd=1, low=-rad, upp=rad) wm = X.rvs(n).reshape((nodes_out, nodes_in + bias_node)) self.weights_matrices.append(wm) layer_index += 1 def train_single(self, input_vector, target_vector): # input_vector and target_vector can be tuple, list or ndarray no_of_layers = len(self.structure) input_vector = np.array(input_vector, ndmin=2).T layer_index = 0 # The output/input vectors of the various layers: res_vectors = [input_vector] while layer_index < no_of_layers - 1: in_vector = res_vectors[-1] if self.bias: # adding bias node to the end of the 'input'_vector in_vector = np.concatenate( (in_vector, [[self.bias]]) ) res_vectors[-1] = in_vector x = np.dot(self.weights_matrices[layer_index], in_vector) out_vector = activation_function(x) res_vectors.append(out_vector) layer_index += 1 layer_index = no_of_layers - 1 target_vector = np.array(target_vector, ndmin=2).T # The input vectors to the various layers output_errors = target_vector - out_vector while layer_index > 0: out_vector = res_vectors[layer_index] in_vector = res_vectors[layer_index-1] if self.bias and not layer_index==(no_of_layers-1): out_vector = out_vector[:-1,:].copy() tmp = output_errors * out_vector * (1.0 - out_vector) tmp = np.dot(tmp, in_vector.T) #if self.bias: # tmp = tmp[:-1,:] self.weights_matrices[layer_index-1] += self.learning_rate * tmp output_errors = np.dot(self.weights_matrices[layer_index-1].T, output_errors) if self.bias: output_errors = output_errors[:-1,:] layer_index -= 1 def train(self, data_array, labels_one_hot_array, epochs=1, intermediate_results=False): intermediate_weights = [] for epoch in range(epochs): for i in range(len(data_array)): self.train_single(data_array[i], labels_one_hot_array[i]) if intermediate_results: intermediate_weights.append((self.wih.copy(), self.who.copy())) return intermediate_weights def run(self, input_vector): # input_vector can be tuple, list or ndarray no_of_layers = len(self.structure) if self.bias: # adding bias node to the end of the inpuy_vector input_vector = np.concatenate( (input_vector, [self.bias]) ) in_vector = np.array(input_vector, ndmin=2).T layer_index = 1 # The input vectors to the various layers while layer_index < no_of_layers: x = np.dot(self.weights_matrices[layer_index-1], in_vector) out_vector = activation_function(x) # input vector for next layer in_vector = out_vector if self.bias: in_vector = np.concatenate( (in_vector, [[self.bias]]) ) layer_index += 1 return out_vector def evaluate(self, data, labels): corrects, wrongs = 0, 0 for i in range(len(data)): res = self.run(data[i]) res_max = res.argmax() if res_max == labels[i]: corrects += 1 else: wrongs += 1 return corrects, wrongs epochs = 3 ANN = NeuralNetwork(network_structure=[image_pixels, 80, 80, 10], learning_rate=0.01, bias=None) ANN.train(train_imgs, train_labels_one_hot, epochs=epochs) The previous code returned the following: [] corrects, wrongs = ANN.evaluate(train_imgs, train_labels) print("accruracy train: ", corrects / ( corrects + wrongs)) corrects, wrongs = ANN.evaluate(test_imgs, test_labels) print("accruracy: test", corrects / ( corrects + wrongs)) accruracy train: 0.9564166666666667 accruracy: test 0.953 Footnotes 1 Wan, Li; Matthew Zeiler; Sixin Zhang; Yann LeCun; Rob Fergus (2013). Regularization of Neural Network using DropConnect. International Conference on Machine Learning(ICML). Previous Chapter: Confusion Matrix Next Chapter: Dropout Neural Networks © 2011 - 2018, Bernd Klein, Bodenseo; Design by Denise Mitchinson adapted for python-course.eu by Bernd Klein