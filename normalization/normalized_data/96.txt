machin
learn
with
python
:
neural
network
with
scikit
python
machin
learn
tutori
machin
learn
machin
learn
terminologyk-nearest
neighbor
classifierneur
network
from
scratch
in
pythonneur
network
in
python
use
numypybackpropag
in
neural
networksconfus
matrixtrain
and
test
with
mnistdropout
neural
networksneur
network
with
scikitmachin
learn
with
scikit
and
pythonintroduct
naiv
bay
classifierna
bay
classifi
with
scikitintroduct
into
text
classif
use
naiv
bayespython
implement
of
text
classificationdecis
treesregress
treesrandom
forestsboost
algorithmprincip
compon
analysislinear
discrimin
analysisexpect
maxim
and
gaussian
mixtur
modelintroduct
into
tensorflow
thi
websit
is
creat
by
:
python
train
cours
in
toronto
,
canada
On
site
train
in
europ
,
canada
and
the
US
.
thi
websit
is
free
of
annoy
ad
.
We
want
to
keep
it
like
thi
.
you
can
help
with
your
donat
:
the
need
for
donat
bernd
klein
on
facebook
search
thi
websit
:
classroom
train
cours
thi
websit
contain
a
free
and
extens
onlin
tutori
by
bernd
klein
,
use
materi
from
hi
classroom
python
train
cours
.
If
you
are
interest
in
an
instructor-l
classroom
train
cours
,
you
may
have
a
look
at
the
python
class
by
bernd
klein
at
bodenseo
.
Â©
kabliczech
-
fotolia.com
quot
of
the
day
:
''
the
digit
revolut
is
far
more
signific
than
the
invent
of
write
or
even
of
print
.
''
(
dougla
engelbart
)
If
you
have
the
choic
work
with
python
2
or
python
3
,
we
recomend
to
switch
to
python
3
!
you
can
read
our
python
tutori
to
see
what
the
differ
are
.
data
protect
declar
data
protect
declar
previou
chapter
:
dropout
neural
network
next
chapter
:
machin
learn
with
scikit
and
python
neural
network
with
scikitperceptron
class
We
will
start
with
the
perceptron
class
contain
in
scikit-learn
.
We
will
use
it
on
the
iri
dataset
,
which
we
had
alreadi
use
in
our
chapter
on
k-nearest
neighbor
import
numpi
as
np
from
sklearn.dataset
import
load_iri
from
sklearn.linear_model
import
perceptron
iri
=
load_iri
(
)
print
(
iris.data
[
:3
]
)
print
(
iris.data
[
15:18
]
)
print
(
iris.data
[
37:40
]
)
#
we
extract
onli
the
length
and
width
of
the
petal
:
X
=
iris.data
[
:
,
(
2
,
3
)
]
[
[
5.1
3.5
1.4
0.2
]
[
4.9
3
.
1.4
0.2
]
[
4.7
3.2
1.3
0.2
]
]
[
[
5.7
4.4
1.5
0.4
]
[
5.4
3.9
1.3
0.4
]
[
5.1
3.5
1.4
0.3
]
]
[
[
4.9
3.1
1.5
0.1
]
[
4.4
3
.
1.3
0.2
]
[
5.1
3.4
1.5
0.2
]
]
iris.label
contain
the
label
0
,
1
and
2
correspond
three
speci
of
iri
flower
:
iri
setosa
,
iri
virginica
and
iri
versicolor
.
print
(
iris.target
)
[
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
]
We
turn
the
three
class
into
two
class
,
i.e
.
iri
setosa
not
iri
setosa
(
thi
mean
iri
virginica
or
iri
versicolor
)
y
=
(
iris.target==0
)
.astyp
(
np.int8
)
print
(
y
)
[
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
]
We
creat
now
a
perceptron
and
fit
the
data
X
and
y
:
p
=
perceptron
(
random_state=42
,
max_iter=10
)
p.fit
(
X
,
y
)
the
abov
python
code
return
the
follow
output
:
perceptron
(
alpha=0.0001
,
class_weight=non
,
eta0=1.0
,
fit_intercept=tru
,
max_iter=10
,
n_iter=non
,
n_jobs=1
,
penalty=non
,
random_state=42
,
shuffle=tru
,
tol=non
,
verbose=0
,
warm_start=fals
)
In
[
]
:
now
,
we
are
readi
for
predict
:
valu
=
[
[
1.5
,
0.1
]
,
[
1.8
,
0.4
]
,
[
1.3,0.2
]
]
for
valu
in
X
:
pred
=
p.predict
(
[
valu
]
)
print
(
[
pred
]
)
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
1
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
[
array
(
[
0
]
,
dtype=int8
)
]
#
#
#
multi-lay
perceptron
We
will
continu
with
exampl
use
the
multilay
perceptron
(
mlp
)
.
the
multilay
perceptron
(
mlp
)
is
a
feedforward
artifici
neural
network
model
that
map
set
of
input
data
onto
a
set
of
appropri
output
.
An
mlp
consist
of
multipl
layer
and
each
layer
is
fulli
connect
to
the
follow
one
.
the
node
of
the
layer
are
neuron
use
nonlinear
activ
function
,
except
for
the
node
of
the
input
layer
.
there
can
be
one
or
more
non-linear
hidden
layer
between
the
input
and
the
output
layer
.
from
sklearn.neural_network
import
mlpclassifi
X
=
[
[
0.
,
0
.
]
,
[
0.
,
1
.
]
,
[
1.
,
0
.
]
,
[
1.
,
1
.
]
]
y
=
[
0
,
0
,
0
,
1
]
clf
=
mlpclassifi
(
solver='lbfg
'
,
alpha=1e-5
,
hidden_layer_sizes=
(
5
,
2
)
,
random_state=1
)
print
(
clf.fit
(
X
,
y
)
)
mlpclassifi
(
activation='relu
'
,
alpha=1e-05
,
batch_size='auto
'
,
beta_1=0.9
,
beta_2=0.999
,
early_stopping=fals
,
epsilon=1e-08
,
hidden_layer_sizes=
(
5
,
2
)
,
learning_rate='const
'
,
learning_rate_init=0.001
,
max_iter=200
,
momentum=0.9
,
nesterovs_momentum=tru
,
power_t=0.5
,
random_state=1
,
shuffle=tru
,
solver='lbfg
'
,
tol=0.0001
,
validation_fraction=0.1
,
verbose=fals
,
warm_start=fals
)
the
follow
diagram
depict
the
neural
network
,
that
we
have
train
for
our
classifi
clf
.
We
have
two
input
node
$
x_0
$
and
$
x_1
$
,
call
the
input
layer
,
and
one
output
neuron
'out
'
.
We
have
two
hidden
layer
the
first
one
with
the
neuron
$
H_
{
00
}
$
...
$
H_
{
04
}
$
and
the
second
hidden
layer
consist
of
$
H_
{
10
}
$
and
$
H_
{
11
}
$
.
each
neuron
of
the
hidden
layer
and
the
output
neuron
possess
a
correspond
bia
,
i.e
.
$
B_
{
00
}
$
is
the
correspond
bia
to
the
neuron
$
H_
{
00
}
$
,
$
B_
{
01
}
$
is
the
correspond
bia
to
the
neuron
$
H_
{
01
}
$
and
so
on
.
each
neuron
of
the
hidden
layer
receiv
the
output
from
everi
neuron
of
the
previou
layer
and
transform
these
valu
with
a
weight
linear
summat
$
$
\sum_
{
i=0
}
^
{
n-1
}
w_ix_i
=
w_0x_0
+
w_1x_1
+
...
+
w_
{
n-1
}
x_
{
n-1
}
$
$
into
an
output
valu
,
where
n
is
the
number
of
neuron
of
the
layer
and
$
w_i
$
correspond
to
the
ith
compon
of
the
weight
vector
.
the
output
layer
receiv
the
valu
from
the
last
hidden
layer
.
It
also
perform
a
linear
summat
,
but
a
non-linear
activ
function
$
$
g
(
\cdot
)
:
R
\rightarrow
R
$
$
like
the
hyperbol
tan
function
will
be
appli
to
the
summat
result
.
the
attribut
coefs_
contain
a
list
of
weight
matric
for
everi
layer
.
the
weight
matrix
at
index
i
hold
the
weight
between
the
layer
i
and
layer
i
+
1.
print
(
``
weight
between
input
and
first
hidden
layer
:
''
)
print
(
clf.coefs_
[
0
]
)
print
(
``
\nweight
between
first
hidden
and
second
hidden
layer
:
''
)
print
(
clf.coefs_
[
1
]
)
weight
between
input
and
first
hidden
layer
:
[
[
-0.14203691
-1.18304359
-0.85567518
-4.53250719
-0.60466275
]
[
-0.69781111
-3.5850093
-0.26436018
-4.39161248
0.06644423
]
]
weight
between
first
hidden
and
second
hidden
layer
:
[
[
0.29179638
-0.14155284
]
[
4.02666592
-0.61556475
]
[
-0.51677234
0.51479708
]
[
7.37215202
-0.31936965
]
[
0.32920668
0.64428109
]
]
the
summat
formula
of
the
neuron
h00
is
defin
by
:
$
$
\sum_
{
i=0
}
^
{
n-1
}
w_ix_i
=
w_0x_0
+
w_1x_1
+
w_
{
B_
{
11
}
}
*
B_
{
11
}
$
$
which
can
be
written
as
$
$
\sum_
{
i=0
}
^
{
n-1
}
w_ix_i
=
w_0x_0
+
w_1x_1
+
w_
{
B_
{
11
}
}
$
$
becaus
$
B_
{
11
}
=
1
$
.
We
can
get
the
valu
for
$
w_0
$
and
$
w_1
$
from
clf.coefs_
like
thi
:
$
w_0
=
$
clf.coefs_
[
0
]
[
0
]
[
0
]
and
$
w_1
=
$
clf.coefs_
[
0
]
[
1
]
[
0
]
print
(
``
w0
=
``
,
clf.coefs_
[
0
]
[
0
]
[
0
]
)
print
(
``
w1
=
``
,
clf.coefs_
[
0
]
[
1
]
[
0
]
)
w0
=
-0.14203691267827162
w1
=
-0.6978111149778682
the
weight
vector
of
$
H_
{
00
}
$
can
be
access
with
clf.coefs_
[
0
]
[
:
,0
]
the
abov
python
code
return
the
follow
result
:
array
(
[
-0.14203691
,
-0.69781111
]
)
We
can
gener
the
abov
to
access
a
neuron
$
H_
{
ij
}
$
in
the
follow
way
:
for
i
in
rang
(
len
(
clf.coefs_
)
)
:
number_neurons_in_lay
=
clf.coefs_
[
i
]
.shape
[
1
]
for
j
in
rang
(
number_neurons_in_lay
)
:
weight
=
clf.coefs_
[
i
]
[
:
,j
]
print
(
i
,
j
,
weight
,
end=
''
,
``
)
print
(
)
print
(
)
0
0
[
-0.14203691
-0.69781111
]
,
0
1
[
-1.18304359
-3.5850093
]
,
0
2
[
-0.85567518
-0.26436018
]
,
0
3
[
-4.53250719
-4.39161248
]
,
0
4
[
-0.60466275
0.06644423
]
,
1
0
[
0.29179638
4.02666592
-0.51677234
7.37215202
0.32920668
]
,
1
1
[
-0.14155284
-0.61556475
0.51479708
-0.31936965
0.64428109
]
,
2
0
[
-4.96774269
-0.86330397
]
,
intercepts_
is
a
list
of
bia
vector
,
where
the
vector
at
index
i
repres
the
bia
valu
ad
to
layer
i+1
.
print
(
``
bia
valu
for
first
hidden
layer
:
''
)
print
(
clf.intercepts_
[
0
]
)
print
(
``
\nbia
valu
for
second
hidden
layer
:
''
)
print
(
clf.intercepts_
[
1
]
)
bia
valu
for
first
hidden
layer
:
[
-0.14962269
-0.59232707
-0.5472481
7.02667699
-0.87510813
]
bia
valu
for
second
hidden
layer
:
[
-3.61417672
-0.76834882
]
the
main
reason
,
whi
we
train
a
classifi
is
to
predict
result
for
new
sampl
.
We
can
do
thi
with
the
predict
method
.
the
method
return
a
predict
class
for
a
sampl
,
in
our
case
a
``
0
''
or
a
``
1
''
:
result
=
clf.predict
(
[
[
0
,
0
]
,
[
0
,
1
]
,
[
1
,
0
]
,
[
0
,
1
]
,
[
1
,
1
]
,
[
2.
,
2
.
]
,
[
1.3
,
1.3
]
,
[
2
,
4.8
]
]
)
instead
of
just
look
at
the
class
result
,
we
can
also
use
the
predict_proba
method
to
get
the
probabl
estim
.
prob_result
=
clf.predict_proba
(
[
[
0
,
0
]
,
[
0
,
1
]
,
[
1
,
0
]
,
[
0
,
1
]
,
[
1
,
1
]
,
[
2.
,
2
.
]
,
[
1.3
,
1.3
]
,
[
2
,
4.8
]
]
)
print
(
prob_result
)
[
[
1.00000000e+000
5.25723951e-101
]
[
1.00000000e+000
3.71534882e-031
]
[
1.00000000e+000
6.47069178e-029
]
[
1.00000000e+000
3.71534882e-031
]
[
2.07145538e-004
9.99792854e-001
]
[
2.07145538e-004
9.99792854e-001
]
[
2.07145538e-004
9.99792854e-001
]
[
2.07145538e-004
9.99792854e-001
]
]
prob_result
[
i
]
[
0
]
give
us
the
probabl
for
the
class0
,
i.e
.
a
``
0
''
and
result
[
i
]
[
1
]
the
probabilti
for
a
``
1
''
.
i
correspond
to
the
ith
sampl
.
anoth
examplew
will
popul
two
cluster
(
class0
and
class1
)
in
a
two
dimension
space
.
import
numpi
as
np
from
matplotlib
import
pyplot
as
plt
npoint
=
50
X
,
Y
=
[
]
,
[
]
#
class
0
x.append
(
np.random.uniform
(
low=-2.5
,
high=2.3
,
size=
(
npoint
,
)
)
)
y.append
(
np.random.uniform
(
low=-1.7
,
high=2.8
,
size=
(
npoint
,
)
)
)
#
class
1
x.append
(
np.random.uniform
(
low=-7.2
,
high=-4.4
,
size=
(
npoint
,
)
)
)
y.append
(
np.random.uniform
(
low=3
,
high=6.5
,
size=
(
npoint
,
)
)
)
learnset
=
[
]
learnlabel
=
[
]
for
i
in
rang
(
2
)
:
#
ad
point
of
class
i
to
learnset
point
=
zip
(
X
[
i
]
,
Y
[
i
]
)
for
p
in
point
:
learnset.append
(
p
)
learnlabels.append
(
i
)
npoints_test
=
3
*
npoint
testx
=
np.random.uniform
(
low=-7.2
,
high=5
,
size=
(
npoints_test
,
)
)
testi
=
np.random.uniform
(
low=-4
,
high=9
,
size=
(
npoints_test
,
)
)
testset
=
[
]
point
=
zip
(
testx
,
testi
)
for
p
in
point
:
testset.append
(
p
)
colour
=
[
``
b
''
,
``
r
''
]
for
i
in
rang
(
2
)
:
plt.scatter
(
X
[
i
]
,
Y
[
i
]
,
c=colour
[
i
]
)
plt.scatter
(
testx
,
testi
,
c=
''
g
''
)
plt.show
(
)
<
matplotlib.figure.figur
at
0x7fb2cb1b2710
>
We
will
train
a
mlpclassifi
for
our
two
class
:
import
matplotlib.pyplot
as
plt
from
sklearn.dataset
import
fetch_mldata
from
sklearn.neural_network
import
mlpclassifi
mlp
=
mlpclassifi
(
hidden_layer_sizes=
(
20
,
3
)
,
max_iter=150
,
alpha=1e-4
,
solver='sgd
'
,
verbose=10
,
tol=1e-4
,
random_state=1
,
learning_rate_init=.1
)
mlp.fit
(
learnset
,
learnlabel
)
print
(
``
train
set
score
:
%
f
''
%
mlp.score
(
learnset
,
learnlabel
)
)
print
(
``
test
set
score
:
%
f
''
%
mlp.score
(
learnset
,
learnlabel
)
)
mlp.classes_
iter
1
,
loss
=
0.48325862
iter
2
,
loss
=
0.45324866
iter
3
,
loss
=
0.41898499
iter
4
,
loss
=
0.38372788
iter
5
,
loss
=
0.35561031
iter
6
,
loss
=
0.32467330
iter
7
,
loss
=
0.29337259
iter
8
,
loss
=
0.26390492
iter
9
,
loss
=
0.23779012
iter
10
,
loss
=
0.21386262
iter
11
,
loss
=
0.19170901
iter
12
,
loss
=
0.17135334
iter
13
,
loss
=
0.15297210
iter
14
,
loss
=
0.13668185
iter
15
,
loss
=
0.12225318
iter
16
,
loss
=
0.10954544
iter
17
,
loss
=
0.09839438
iter
18
,
loss
=
0.08862823
iter
19
,
loss
=
0.08008047
iter
20
,
loss
=
0.07260359
iter
21
,
loss
=
0.06606468
iter
22
,
loss
=
0.06034591
iter
23
,
loss
=
0.05536010
iter
24
,
loss
=
0.05098744
iter
25
,
loss
=
0.04714394
iter
26
,
loss
=
0.04375574
iter
27
,
loss
=
0.04075942
iter
28
,
loss
=
0.03810138
iter
29
,
loss
=
0.03574362
iter
30
,
loss
=
0.03365348
iter
31
,
loss
=
0.03178624
iter
32
,
loss
=
0.03012046
iter
33
,
loss
=
0.02862332
iter
34
,
loss
=
0.02727324
iter
35
,
loss
=
0.02605317
iter
36
,
loss
=
0.02494736
iter
37
,
loss
=
0.02394162
iter
38
,
loss
=
0.02302351
iter
39
,
loss
=
0.02218237
iter
40
,
loss
=
0.02140916
iter
41
,
loss
=
0.02069633
iter
42
,
loss
=
0.02004016
iter
43
,
loss
=
0.01943476
iter
44
,
loss
=
0.01887299
iter
45
,
loss
=
0.01835048
iter
46
,
loss
=
0.01786336
iter
47
,
loss
=
0.01740824
iter
48
,
loss
=
0.01698209
iter
49
,
loss
=
0.01658222
iter
50
,
loss
=
0.01620626
iter
51
,
loss
=
0.01585210
iter
52
,
loss
=
0.01551782
iter
53
,
loss
=
0.01520174
iter
54
,
loss
=
0.01490232
iter
55
,
loss
=
0.01461819
iter
56
,
loss
=
0.01434814
iter
57
,
loss
=
0.01409104
iter
58
,
loss
=
0.01384591
iter
59
,
loss
=
0.01361182
iter
60
,
loss
=
0.01338797
iter
61
,
loss
=
0.01317361
iter
62
,
loss
=
0.01296805
iter
63
,
loss
=
0.01277070
iter
64
,
loss
=
0.01258099
iter
65
,
loss
=
0.01239843
iter
66
,
loss
=
0.01222253
iter
67
,
loss
=
0.01205289
iter
68
,
loss
=
0.01188916
iter
69
,
loss
=
0.01173095
iter
70
,
loss
=
0.01157792
iter
71
,
loss
=
0.01142976
iter
72
,
loss
=
0.01128620
iter
73
,
loss
=
0.01114699
iter
74
,
loss
=
0.01101189
iter
75
,
loss
=
0.01088069
iter
76
,
loss
=
0.01075317
iter
77
,
loss
=
0.01062917
iter
78
,
loss
=
0.01050850
iter
79
,
loss
=
0.01039100
iter
80
,
loss
=
0.01027653
iter
81
,
loss
=
0.01016495
iter
82
,
loss
=
0.01005613
iter
83
,
loss
=
0.00994995
iter
84
,
loss
=
0.00984629
iter
85
,
loss
=
0.00974506
iter
86
,
loss
=
0.00964614
iter
87
,
loss
=
0.00954945
iter
88
,
loss
=
0.00945489
train
loss
did
not
improv
more
than
tol=0.000100
for
two
consecut
epoch
.
stop
.
train
set
score
:
1.000000
test
set
score
:
1.000000
We
receiv
the
follow
result
:
array
(
[
0
,
1
]
)
predict
=
clf.predict
(
testset
)
predict
the
previou
python
code
return
the
follow
output
:
array
(
[
0
,
0
,
0
,
1
,
1
,
0
,
1
,
0
,
0
,
0
,
1
,
1
,
0
,
1
,
1
,
1
,
0
,
1
,
0
,
1
,
1
,
0
,
0
,
0
,
0
,
0
,
1
,
0
,
1
,
0
,
0
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
0
,
0
,
0
,
0
,
0
,
1
,
0
,
0
,
0
,
0
,
1
,
0
,
1
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
1
,
0
,
0
,
1
,
1
,
1
,
0
,
1
,
0
,
1
,
1
,
1
,
1
,
1
,
0
,
0
,
0
,
1
,
1
,
0
,
0
,
1
,
1
,
0
,
0
,
0
,
1
,
1
,
1
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
1
,
0
,
0
,
0
,
1
,
1
,
1
,
0
,
1
,
1
,
0
,
0
,
1
,
0
,
1
,
0
,
1
,
1
,
0
,
1
,
0
,
1
,
0
,
1
,
0
,
1
,
0
,
1
,
1
,
0
,
0
,
0
,
0
,
1
,
1
,
0
,
1
,
0
,
0
,
0
,
1
,
0
,
1
,
1
,
0
,
0
,
1
,
0
,
0
,
0
,
1
,
0
,
1
,
1
]
)
testset
=
np.array
(
testset
)
testset
[
predictions==1
]
colour
=
[
'
#
c0ffff
'
,
``
#
ffc8c8
''
]
for
i
in
rang
(
2
)
:
plt.scatter
(
X
[
i
]
,
Y
[
i
]
,
c=colour
[
i
]
)
colour
=
[
``
b
''
,
``
r
''
]
for
i
in
rang
(
2
)
:
cl
=
testset
[
predictions==i
]
Xt
,
Yt
=
zip
(
*cl
)
plt.scatter
(
Xt
,
Yt
,
marker=
''
D
''
,
c=colour
[
i
]
)
mnist
datasetw
have
alreadi
use
the
mnist
dataset
in
the
chapter
test
with
mnist
of
our
tutori
.
you
will
also
find
some
explan
about
thi
dataset
.
We
want
to
appli
the
mlpclassifi
on
the
mnist
data
.
So
far
we
have
use
our
local
store
mnist
data
.
sklearn
provid
also
thi
dataset
,
as
we
can
see
in
the
follow
:
import
matplotlib.pyplot
as
plt
from
sklearn.dataset
import
fetch_mldata
from
sklearn.neural_network
import
mlpclassifi
mnist
=
fetch_mldata
(
``
mnist
origin
''
)
X
,
y
=
mnist.data
/
255.
,
mnist.target
x_train
,
x_test
=
X
[
:60000
]
,
X
[
60000
:
]
y_train
,
y_test
=
y
[
:60000
]
,
y
[
60000
:
]
mlp
=
mlpclassifi
(
hidden_layer_sizes=
(
100
,
)
,
max_iter=40
,
alpha=1e-4
,
solver='sgd
'
,
verbose=10
,
tol=1e-4
,
random_state=1
,
learning_rate_init=.1
)
mlp.fit
(
x_train
,
y_train
)
print
(
``
train
set
score
:
%
f
''
%
mlp.score
(
x_train
,
y_train
)
)
print
(
``
test
set
score
:
%
f
''
%
mlp.score
(
x_test
,
y_test
)
)
fig
,
axe
=
plt.subplot
(
4
,
4
)
#
use
global
min
/
max
to
ensur
all
weight
are
shown
on
the
same
scale
vmin
,
vmax
=
mlp.coefs_
[
0
]
.min
(
)
,
mlp.coefs_
[
0
]
.max
(
)
for
coef
,
ax
in
zip
(
mlp.coefs_
[
0
]
.T
,
axes.ravel
(
)
)
:
ax.matshow
(
coef.reshap
(
28
,
28
)
,
cmap=plt.cm.gray
,
vmin=.5
*
vmin
,
vmax=.5
*
vmax
)
ax.set_xtick
(
(
)
)
ax.set_ytick
(
(
)
)
plt.show
(
)
iter
1
,
loss
=
0.29711511
iter
2
,
loss
=
0.12543994
iter
3
,
loss
=
0.08891995
iter
4
,
loss
=
0.06980587
iter
5
,
loss
=
0.05722261
iter
6
,
loss
=
0.04768470
iter
7
,
loss
=
0.03988128
iter
8
,
loss
=
0.03484239
iter
9
,
loss
=
0.02850733
iter
10
,
loss
=
0.02373436
iter
11
,
loss
=
0.02096870
iter
12
,
loss
=
0.01726910
iter
13
,
loss
=
0.01428864
iter
14
,
loss
=
0.01236551
iter
15
,
loss
=
0.00987732
iter
16
,
loss
=
0.00843697
iter
17
,
loss
=
0.00738563
iter
18
,
loss
=
0.00642474
iter
19
,
loss
=
0.00526446
iter
20
,
loss
=
0.00438302
iter
21
,
loss
=
0.00376373
iter
22
,
loss
=
0.00345448
iter
23
,
loss
=
0.00302641
iter
24
,
loss
=
0.00269291
iter
25
,
loss
=
0.00255057
iter
26
,
loss
=
0.00235068
iter
27
,
loss
=
0.00223805
iter
28
,
loss
=
0.00208284
iter
29
,
loss
=
0.00196621
iter
30
,
loss
=
0.00185587
iter
31
,
loss
=
0.00176521
iter
32
,
loss
=
0.00169159
iter
33
,
loss
=
0.00163580
train
loss
did
not
improv
more
than
tol=0.000100
for
two
consecut
epoch
.
stop
.
train
set
score
:
1.000000
test
set
score
:
0.980400
import
pickl
with
open
(
``
data/mnist/pickled_mnist.pkl
''
,
``
br
''
)
as
fh
:
data
=
pickle.load
(
fh
)
train_img
=
data
[
0
]
test_img
=
data
[
1
]
train_label
=
data
[
2
]
test_label
=
data
[
3
]
train_labels_one_hot
=
data
[
4
]
test_labels_one_hot
=
data
[
5
]
image_s
=
28
#
width
and
length
no_of_different_label
=
10
#
i.e
.
0
,
1
,
2
,
3
,
...
,
9
image_pixel
=
image_s
*
image_s
mlp
=
mlpclassifi
(
hidden_layer_sizes=
(
100
,
)
,
max_iter=40
,
alpha=1e-4
,
solver='sgd
'
,
verbose=10
,
tol=1e-4
,
random_state=1
,
learning_rate_init=.1
)
mlp.fit
(
train_img
,
train_label
)
print
(
``
train
set
score
:
%
f
''
%
mlp.score
(
train_img
,
train_label
)
)
print
(
``
test
set
score
:
%
f
''
%
mlp.score
(
test_img
,
test_label
)
)
/home/bernd/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:912
:
dataconversionwarn
:
A
column-vector
y
wa
pass
when
a
1d
array
wa
expect
.
pleas
chang
the
shape
of
y
to
(
n_sampl
,
)
,
for
exampl
use
ravel
(
)
.
y
=
column_or_1d
(
y
,
warn=tru
)
iter
1
,
loss
=
0.29308647
iter
2
,
loss
=
0.12126145
iter
3
,
loss
=
0.08665577
iter
4
,
loss
=
0.06916886
iter
5
,
loss
=
0.05734882
iter
6
,
loss
=
0.04697824
iter
7
,
loss
=
0.04005900
iter
8
,
loss
=
0.03370386
iter
9
,
loss
=
0.02848827
iter
10
,
loss
=
0.02453574
iter
11
,
loss
=
0.02058716
iter
12
,
loss
=
0.01649971
iter
13
,
loss
=
0.01408953
iter
14
,
loss
=
0.01173909
iter
15
,
loss
=
0.00925713
iter
16
,
loss
=
0.00879338
iter
17
,
loss
=
0.00687255
iter
18
,
loss
=
0.00578659
iter
19
,
loss
=
0.00492355
iter
20
,
loss
=
0.00414159
iter
21
,
loss
=
0.00358124
iter
22
,
loss
=
0.00324285
iter
23
,
loss
=
0.00299358
iter
24
,
loss
=
0.00268943
iter
25
,
loss
=
0.00248878
iter
26
,
loss
=
0.00229525
iter
27
,
loss
=
0.00218314
iter
28
,
loss
=
0.00203129
iter
29
,
loss
=
0.00190647
iter
30
,
loss
=
0.00180089
iter
31
,
loss
=
0.00175467
iter
32
,
loss
=
0.00165441
iter
33
,
loss
=
0.00159778
iter
34
,
loss
=
0.00152206
iter
35
,
loss
=
0.00146529
train
loss
did
not
improv
more
than
tol=0.000100
for
two
consecut
epoch
.
stop
.
train
set
score
:
1.000000
test
set
score
:
0.980400
previou
chapter
:
dropout
neural
network
next
chapter
:
machin
learn
with
scikit
and
python
Â©
2011
-
2018
,
bernd
klein
,
bodenseo
;
design
by
denis
mitchinson
adapt
for
python-course.eu
by
bernd
klein
