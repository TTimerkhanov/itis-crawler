machin
learn
with
python
:
machin
learn
with
scikit
and
python
python
machin
learn
tutori
machin
learn
machin
learn
terminologyk-nearest
neighbor
classifierneur
network
from
scratch
in
pythonneur
network
in
python
use
numypybackpropag
in
neural
networksconfus
matrixtrain
and
test
with
mnistdropout
neural
networksneur
network
with
scikitmachin
learn
with
scikit
and
pythonintroduct
naiv
bay
classifierna
bay
classifi
with
scikitintroduct
into
text
classif
use
naiv
bayespython
implement
of
text
classificationdecis
treesregress
treesrandom
forestsboost
algorithmprincip
compon
analysislinear
discrimin
analysisexpect
maxim
and
gaussian
mixtur
modelintroduct
into
tensorflow
thi
websit
is
creat
by
:
python
train
cours
in
toronto
,
canada
On
site
train
in
europ
,
canada
and
the
US
.
thi
websit
is
free
of
annoy
ad
.
We
want
to
keep
it
like
thi
.
you
can
help
with
your
donat
:
the
need
for
donat
bernd
klein
on
facebook
search
thi
websit
:
classroom
train
cours
thi
websit
contain
a
free
and
extens
onlin
tutori
by
bernd
klein
,
use
materi
from
hi
classroom
python
train
cours
.
If
you
are
interest
in
an
instructor-l
classroom
train
cours
,
you
may
have
a
look
at
the
python
class
by
bernd
klein
at
bodenseo
.
©
kabliczech
-
fotolia.com
quot
of
the
day
:
man
is
the
best
comput
we
can
put
aboard
a
spacecraft
...
and
the
onli
one
that
can
be
mass
produc
with
unskil
labor
.
(
wernher
von
braun
)
If
you
have
the
choic
work
with
python
2
or
python
3
,
we
recomend
to
switch
to
python
3
!
you
can
read
our
python
tutori
to
see
what
the
differ
are
.
data
protect
declar
data
protect
declar
previou
chapter
:
neural
network
with
scikit
next
chapter
:
introduct
naiv
bay
classifi
scikit
scikit-learn
is
a
python
modul
merg
classic
machin
learn
algorithm
with
the
world
of
scientif
python
packag
(
numpi
,
scipi
,
matplotlib
)
.
our
learn
set
:
``
digit
''
%
matplotlib
inlin
import
numpi
as
np
from
sklearn
import
dataset
#
iri
=
datasets.load_iri
(
)
digit
=
datasets.load_digit
(
)
print
(
type
(
digit
)
)
<
class
'sklearn.datasets.base.bunch
'
>
the
digit
dataset
is
a
dictionary-lik
object
,
contain
the
actual
data
and
some
metadata
.
print
(
digits.data
)
[
[
0
.
0
.
5
.
...
,
0
.
0
.
0
.
]
[
0
.
0
.
0
.
...
,
10
.
0
.
0
.
]
[
0
.
0
.
0
.
...
,
16
.
9
.
0
.
]
...
,
[
0
.
0
.
1
.
...
,
6
.
0
.
0
.
]
[
0
.
0
.
2
.
...
,
12
.
0
.
0
.
]
[
0
.
0
.
10
.
...
,
12
.
1
.
0
.
]
]
digits.data
contain
the
featur
,
i.e
.
imag
of
handwritten
imag
of
digit
,
which
can
be
use
for
classif
.
digits.target
the
python
code
abov
return
the
follow
:
array
(
[
0
,
1
,
2
,
...
,
8
,
9
,
8
]
)
len
(
digits.data
)
,
len
(
digits.target
)
We
receiv
the
follow
output
:
(
1797
,
1797
)
digits.target
contain
the
label
,
i.e
.
digit
from
0
to
9
for
the
digit
of
digits.data
.
the
data
``
digit
''
is
a
2
D
array
with
the
shape
(
number
of
sampl
,
number
of
featur
)
.
In
our
case
,
a
sampl
is
an
imag
of
shape
(
8
,
8
)
:
print
(
digits.target
[
0
]
,
digits.data
[
0
]
)
print
(
digits.imag
[
0
]
)
0
[
0
.
0
.
5
.
13
.
9
.
1
.
0
.
0
.
0
.
0
.
13
.
15
.
10
.
15
.
5
.
0
.
0
.
3
.
15
.
2
.
0
.
11
.
8
.
0
.
0
.
4
.
12
.
0
.
0
.
8
.
8
.
0
.
0
.
5
.
8
.
0
.
0
.
9
.
8
.
0
.
0
.
4
.
11
.
0
.
1
.
12
.
7
.
0
.
0
.
2
.
14
.
5
.
10
.
12
.
0
.
0
.
0
.
0
.
6
.
13
.
10
.
0
.
0
.
0
.
]
[
[
0
.
0
.
5
.
13
.
9
.
1
.
0
.
0
.
]
[
0
.
0
.
13
.
15
.
10
.
15
.
5
.
0
.
]
[
0
.
3
.
15
.
2
.
0
.
11
.
8
.
0
.
]
[
0
.
4
.
12
.
0
.
0
.
8
.
8
.
0
.
]
[
0
.
5
.
8
.
0
.
0
.
9
.
8
.
0
.
]
[
0
.
4
.
11
.
0
.
1
.
12
.
7
.
0
.
]
[
0
.
2
.
14
.
5
.
10
.
12
.
0
.
0
.
]
[
0
.
0
.
6
.
13
.
10
.
0
.
0
.
0
.
]
]
learn
and
predict
We
want
to
predict
for
a
given
imag
,
which
digit
it
depict
.
our
data
set
contain
sampl
for
the
class
0
(
zero
)
to
9
(
nine
)
.
We
will
use
these
sampl
to
fit
an
estim
so
that
we
can
predict
unseen
sampl
as
well
.
In
scikit-learn
,
an
estim
for
classif
is
a
python
object
that
implement
the
method
fit
(
X
,
y
)
and
predict
(
T
)
.
An
exampl
of
an
estim
is
the
class
sklearn.svm.svc
that
implement
support
vector
classif
.
the
constructor
of
an
estim
take
as
argument
the
paramet
of
the
model
,
but
for
the
time
be
,
we
will
consid
the
estim
as
a
black
box
:
from
sklearn
import
svm
#
import
support
vector
machin
classifi
=
svm.svc
(
gamma=0.001
,
c=100
.
)
classifier.fit
(
digits.data
[
:
-3
]
,
digits.target
[
:
-3
]
)
the
previou
python
code
return
the
follow
:
svc
(
c=100.0
,
cache_size=200
,
class_weight=non
,
coef0=0.0
,
decision_function_shape=non
,
degree=3
,
gamma=0.001
,
kernel='rbf
'
,
max_iter=-1
,
probability=fals
,
random_state=non
,
shrinking=tru
,
tol=0.001
,
verbose=fals
)
the
classifi
,
which
we
have
creat
with
svm.svc
,
is
an
estim
object
.
In
gener
the
scikit-learn
api
provid
estim
object
,
which
can
be
ani
object
that
can
learn
from
data
.
learn
can
be
done
by
classif
,
regress
or
cluster
algorithm
or
a
transform
that
extracts/filt
use
featur
from
raw
data
.
all
estim
object
expos
a
fit
method
that
take
a
dataset
(
usual
a
2-d
array
)
:
classifier.predict
(
digits.data
[
-3
:
]
)
thi
get
us
the
follow
output
:
array
(
[
8
,
9
,
8
]
)
digits.target
[
-3
:
]
the
abov
code
return
the
follow
result
:
array
(
[
8
,
9
,
8
]
)
digits.data
[
-3
]
the
previou
code
return
the
follow
output
:
array
(
[
0.
,
0.
,
1.
,
11.
,
15.
,
1.
,
0.
,
0.
,
0.
,
0.
,
13.
,
16.
,
8.
,
2.
,
1.
,
0.
,
0.
,
0.
,
16.
,
15.
,
10.
,
16.
,
5.
,
0.
,
0.
,
0.
,
8.
,
16.
,
16.
,
7.
,
0.
,
0.
,
0.
,
0.
,
9.
,
16.
,
16.
,
4.
,
0.
,
0.
,
0.
,
0.
,
16.
,
14.
,
16.
,
15.
,
0.
,
0.
,
0.
,
0.
,
15.
,
15.
,
15.
,
16.
,
0.
,
0.
,
0.
,
0.
,
2.
,
9.
,
13.
,
6.
,
0.
,
0
.
]
)
import
matplotlib.pyplot
as
plt
from
pil
import
imag
img
=
image.fromarray
(
np.uint8
(
digits.imag
[
-2
]
)
)
plt.gray
(
)
plt.imshow
(
img
)
plt.show
(
)
plt.imshow
(
digits.imag
[
-2
]
,
cmap=plt.cm.gray_r
)
thi
get
us
the
follow
:
<
matplotlib.image.axesimag
at
0x7f5ef7c42898
>
iri
datasetth
iri
flower
data
set
is
a
multivari
data
set
introduc
by
ronald
fisher
in
hi
1936
paper
``
the
use
of
multipl
measur
in
taxonom
problem
as
an
exampl
of
linear
discrimin
analysi
.
''
the
data
set
consist
of
50
sampl
from
each
of
three
speci
of
iri
iri
setosa
,
iri
virginica
and
iri
versicolor
)
.
four
featur
were
measur
from
each
sampl
the
length
and
the
width
of
the
sepal
and
petal
,
in
centimetr
.
base
on
the
combin
of
these
four
featur
,
fisher
develop
a
linear
discrimin
model
to
distinguish
the
speci
from
each
other
.
save
train
modelsit
's
possibl
to
keep
a
train
model
persist
with
the
pickl
modul
.
In
the
follow
exampl
,
we
want
to
demonstr
how
to
learn
a
classifi
and
save
it
for
later
usag
with
the
pickl
modul
of
python
:
from
sklearn
import
svm
,
dataset
import
pickl
iri
=
datasets.load_iri
(
)
clf
=
svm.svc
(
)
X
,
y
=
iris.data
,
iris.target
clf.fit
(
X
,
y
)
the
previou
code
return
the
follow
output
:
svc
(
c=1.0
,
cache_size=200
,
class_weight=non
,
coef0=0.0
,
decision_function_shape=non
,
degree=3
,
gamma='auto
'
,
kernel='rbf
'
,
max_iter=-1
,
probability=fals
,
random_state=non
,
shrinking=tru
,
tol=0.001
,
verbose=fals
)
fname
=
open
(
``
classifiers/iris.pkl
''
,
``
bw
''
)
pickle.dump
(
clf
,
fname
)
#
load
the
save
classifi
:
fname
=
open
(
``
classifiers/iris.pkl
''
,
``
br
''
)
clf2
=
pickle.load
(
fname
)
clf2.predict
(
iris.data
[
:
:5
]
)
We
receiv
the
follow
output
:
array
(
[
0
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
2
,
2
,
2
,
2
,
2
,
2
,
2
,
2
,
2
,
2
]
)
iris.target
[
:
:5
]
the
abov
code
return
the
follow
result
:
array
(
[
0
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
2
,
2
,
2
,
2
,
2
,
2
,
2
,
2
,
2
,
2
]
)
now
,
we
will
do
the
same
with
joblib
packag
from
sklearn.extern
.
joblib
is
more
effici
on
big
data
:
from
sklearn.extern
import
joblib
joblib.dump
(
clf
,
'classifiers/iris2.pkl
'
)
clf3
=
joblib.load
(
'classifiers/iris2.pkl
'
)
clf3.predict
(
iris.data
[
:
:5
]
)
the
abov
code
return
the
follow
output
:
array
(
[
0
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
1
,
2
,
2
,
2
,
2
,
2
,
2
,
2
,
2
,
2
,
2
]
)
statistical-learn
for
scientif
data
processingw
saw
that
the
``
iri
dataset
''
consist
of
150
observ
of
iris
,
i.e
.
the
sampl
.
each
oberserv
is
describ
by
four
featur
(
the
length
and
the
width
of
the
sepal
and
petal
)
.
In
gener
,
we
can
say
that
scikit-learn
deal
with
learn
inform
from
one
or
more
dataset
that
are
repres
as
2D
array
.
such
an
array
can
be
seen
as
a
list
of
multi-dimension
observ
.
the
first
axi
of
such
an
array
is
the
sampl
axi
and
the
second
one
is
the
featur
axi
.
supervis
learningsupervis
learn
consist
in
the
task
of
find
or
deduc
a
function
from
label
train
data
.
the
train
data
consist
of
a
set
of
train
exampl
.
In
other
word
:
We
have
the
actual
data
X
and
the
correspond
``
target
''
y
,
also
call
``
label
''
.
often
y
is
a
one
dimension
array
.
An
estim
in
scikit-learn
provid
a
fit
method
to
fit
the
model
:
fit
(
X
,
y
)
.
It
also
suppli
a
predict
method
which
return
predict
label
y
for
(
unlabel
)
observ
X
:
predict
(
X
)
--
>
y
.
instanc
base
learn
--
-k-nearest-neighborinst
base
learn
work
directli
on
the
learn
sampl
,
instead
of
creat
rule
compar
to
other
classif
method
.
way
of
work
:
each
new
instanc
is
compar
with
the
alreadi
exist
instanc
.
the
instanc
are
compar
by
use
a
distanc
metric
.
the
instanc
with
the
closest
distanc
valu
detwermin
the
class
for
the
new
instanc
.
thi
classif
method
is
call
nearest-neighbor
classif
.
In
[
]
:
#
#
#
k-nearest-neighbor
from
scratch
In
[
]
:
import
numpi
as
np
from
sklearn
import
dataset
iri
=
datasets.load_iri
(
)
iris_x
=
iris.data
iris_i
=
iris.target
print
(
iris_x
[
:8
]
)
We
creat
a
learnsetfrom
the
set
abov
.
We
use
permut
from
np.random
to
split
the
data
randomli
:
np.random.se
(
42
)
indic
=
np.random.permut
(
len
(
iris_x
)
)
n_training_sampl
=
12
iris_x_train
=
iris_x
[
indic
[
:
-n_training_sampl
]
]
iris_y_train
=
iris_i
[
indic
[
:
-n_training_sampl
]
]
iris_x_test
=
iris_x
[
indic
[
-n_training_sampl
:
]
]
iris_y_test
=
iris_i
[
indic
[
-n_training_sampl
:
]
]
print
(
iris_x_test
)
[
[
5.7
2.8
4.1
1.3
]
[
6.5
3
.
5.5
1.8
]
[
6.3
2.3
4.4
1.3
]
[
6.4
2.9
4.3
1.3
]
[
5.6
2.8
4.9
2
.
]
[
5.9
3
.
5.1
1.8
]
[
5.4
3.4
1.7
0.2
]
[
6.1
2.8
4
.
1.3
]
[
4.9
2.5
4.5
1.7
]
[
5.8
4
.
1.2
0.2
]
[
5.8
2.6
4
.
1.2
]
[
7.1
3
.
5.9
2.1
]
]
To
determin
the
similar
between
to
instanc
,
we
need
a
distanc
function
.
In
our
exampl
,
the
euclidean
distanc
is
ideal
:
def
distanc
(
instance1
,
instance2
)
:
#
just
in
case
,
if
the
instanc
are
list
or
tupl
:
instance1
=
np.array
(
instance1
)
instance2
=
np.array
(
instance2
)
return
np.linalg.norm
(
instance1
-
instance2
)
print
(
distanc
(
[
4
,
3
,
2
]
,
[
1
,
1,1
]
)
)
3.74165738677
def
get_neighbor
(
training_set
,
test_inst
,
k
)
:
distanc
=
[
]
for
training_inst
in
training_set
:
dist
=
distanc
(
test_inst
,
training_inst
[
:
-1
]
)
distances.append
(
(
training_inst
,
dist
)
)
distances.sort
(
key=lambda
x
:
x
[
1
]
)
neighbor
=
[
]
for
i
in
rang
(
k
)
:
neighbors.append
(
distanc
[
i
]
[
0
]
)
return
neighbor
train_set
=
[
(
1
,
2
,
2
,
'appl
'
)
,
(
-3
,
-2
,
0
,
'banana
'
)
,
(
1
,
1
,
3
,
'appl
'
)
,
(
-3
,
-3
,
-1
,
'banana
'
)
]
k
=
1
for
test_inst
in
[
(
0
,
0
,
0
)
,
(
2
,
2
,
2
)
,
(
-3
,
-1
,
0
)
]
:
neighbor
=
get_neighbor
(
train_set
,
test_inst
,
2
)
print
(
test_inst
,
neighbor
)
(
0
,
0
,
0
)
[
(
1
,
2
,
2
,
'appl
'
)
,
(
1
,
1
,
3
,
'appl
'
)
]
(
2
,
2
,
2
)
[
(
1
,
2
,
2
,
'appl
'
)
,
(
1
,
1
,
3
,
'appl
'
)
]
(
-3
,
-1
,
0
)
[
(
-3
,
-2
,
0
,
'banana
'
)
,
(
-3
,
-3
,
-1
,
'banana
'
)
]
In
[
]
:
previou
chapter
:
neural
network
with
scikit
next
chapter
:
introduct
naiv
bay
classifi
©
2011
-
2018
,
bernd
klein
,
bodenseo
;
design
by
denis
mitchinson
adapt
for
python-course.eu
by
bernd
klein
