machin
learn
with
python
:
introduct
naiv
bay
classifi
python
machin
learn
tutori
machin
learn
machin
learn
terminologyk-nearest
neighbor
classifierneur
network
from
scratch
in
pythonneur
network
in
python
use
numypybackpropag
in
neural
networksconfus
matrixtrain
and
test
with
mnistdropout
neural
networksneur
network
with
scikitmachin
learn
with
scikit
and
pythonintroduct
naiv
bay
classifierna
bay
classifi
with
scikitintroduct
into
text
classif
use
naiv
bayespython
implement
of
text
classificationdecis
treesregress
treesrandom
forestsboost
algorithmprincip
compon
analysislinear
discrimin
analysisexpect
maxim
and
gaussian
mixtur
modelintroduct
into
tensorflow
thoma
bay
the
man
behind
the
bay
'
theorem
is
thoma
bay
.
He
wa
born
in
1701
or
1702
and
die
on
the
7th
of
april
1761
.
the
pictur
on
the
top
of
thi
page
might
be
a
portrait
of
him
,
but
it
is
not
sure
.
He
ha
publish
two
import
work
in
hi
lifetim
:
divin
benevol
,
or
an
attempt
to
prove
that
the
princip
end
of
the
divin
provid
and
govern
is
the
happi
of
hi
creatur
(
1731
)
An
introduct
to
the
doctrin
of
fluxion
,
and
a
defenc
of
the
mathematician
against
the
object
of
the
author
of
the
analyst
(
publish
anonym
in
1736
)
He
is
most
well
known
for
the
work
he
did
in
hi
later
year
,
in
which
he
took
a
deep
interest
in
probabl
.
thi
websit
is
creat
by
:
python
train
cours
in
toronto
,
canada
On
site
train
in
europ
,
canada
and
the
US
.
thi
websit
is
free
of
annoy
ad
.
We
want
to
keep
it
like
thi
.
you
can
help
with
your
donat
:
the
need
for
donat
bernd
klein
on
facebook
search
thi
websit
:
classroom
train
cours
thi
websit
contain
a
free
and
extens
onlin
tutori
by
bernd
klein
,
use
materi
from
hi
classroom
python
train
cours
.
If
you
are
interest
in
an
instructor-l
classroom
train
cours
,
you
may
have
a
look
at
the
python
class
by
bernd
klein
at
bodenseo
.
©
kabliczech
-
fotolia.com
quot
of
the
day
:
''
I
will
,
in
fact
,
claim
that
the
differ
between
a
bad
programm
and
a
good
one
is
whether
he
consid
hi
code
or
hi
data
structur
more
import
.
bad
programm
worri
about
the
code
.
good
programm
worri
about
data
structur
and
their
relationship
''
(
linux
torvald
)
If
you
have
the
choic
work
with
python
2
or
python
3
,
we
recomend
to
switch
to
python
3
!
you
can
read
our
python
tutori
to
see
what
the
differ
are
.
data
protect
declar
data
protect
declar
previou
chapter
:
machin
learn
with
scikit
and
python
next
chapter
:
naiv
bay
classifi
with
scikit
naiv
bay
classifierdefinit
In
machin
learn
,
a
bay
classifi
is
a
simpl
probabilist
classifi
,
which
is
base
on
appli
bay
'
theorem
.
the
featur
model
use
by
a
naiv
bay
classifi
make
strong
independ
assumpt
.
thi
mean
that
the
exist
of
a
particular
featur
of
a
class
is
independ
or
unrel
to
the
exist
of
everi
other
featur
.
definit
of
independ
event
:
two
event
E
and
F
are
independ
,
if
both
E
and
F
have
posit
probabl
and
if
P
(
e|f
)
=
P
(
E
)
and
P
(
f|e
)
=
P
(
F
)
As
we
have
state
in
our
definit
,
the
naiv
bay
classifi
is
base
on
the
bay
'
theorem
.
the
bay
theorem
is
base
on
the
condit
probabl
,
which
we
will
defin
now
:
condit
probabl
$
P
(
a|b
)
$
stand
for
``
the
condit
probabl
of
A
given
B
''
,
or
``
the
probabl
of
A
under
the
condit
B
''
,
i.e
.
the
probabl
of
some
event
A
under
the
assumpt
that
the
event
B
took
place
.
when
in
a
random
experi
the
event
B
is
known
to
have
occur
,
the
possibl
outcom
of
the
experi
are
reduc
to
B
,
and
henc
the
probabl
of
the
occurr
of
A
is
chang
from
the
uncondit
probabl
into
the
condit
probabl
given
B
.
the
joint
probabl
is
the
probabl
of
two
event
in
conjunct
.
that
is
,
it
is
the
probabl
of
both
event
togeth
.
there
are
three
notat
for
the
joint
probabl
of
A
and
B
.
It
can
be
written
as
$
P
(
A
∩
B
)
$
$
P
(
AB
)
$
or
$
P
(
A
,
B
)
$
the
condit
probabl
is
defin
by
$
$
P
(
a|b
)
=
\frac
{
P
(
A
∩
B
)
}
{
P
(
B
)
}
$
$
exampl
for
condit
probabilitygerman
swiss
speakerther
are
about
8.4
million
peopl
live
in
switzerland
.
about
64
%
of
them
speak
german
.
there
are
about
7500
million
peopl
on
earth
.
If
some
alien
randomli
beam
up
an
earthl
,
what
are
the
chanc
that
he
is
a
german
speak
swiss
?
We
have
the
event
S
:
be
swiss
GS
:
german
speak
the
probabl
for
a
randomli
chosen
person
to
be
swiss
:
$
$
P
(
S
)
=
\frac
{
8.4
}
{
7500
}
=
0.00112
$
$
If
we
know
that
somebodi
is
swiss
,
the
probabl
of
speak
german
is
0.64
.
thi
correspond
to
the
condit
probabl
$
$
P
(
GS
|
S
)
=
0.64
$
$
So
the
probabl
of
the
earthl
be
swiss
and
speak
german
,
can
be
calcul
by
the
formula
:
$
$
P
(
GS
|
S
)
=
\frac
{
P
(
GS
∩
S
)
}
{
P
(
S
)
}
$
$
insert
the
valu
from
abov
give
us
:
$
$
0.64
=
\frac
{
P
(
GS
∩
S
)
}
{
0.00112
}
$
$
and
$
$
P
(
GS
∩
S
)
=
0.0007168
$
$
So
our
alien
end
up
with
a
chanc
of
0.07168
%
of
get
a
german
speak
swiss
person
.
fals
posit
and
fals
negativesa
medic
research
lab
propos
a
screen
to
test
a
larg
group
of
peopl
for
a
diseas
.
An
argument
against
such
screen
is
the
problem
of
fals
posit
screen
result
.
suppos
0,1
%
of
the
group
suffer
from
the
diseas
,
and
the
rest
is
well
:
$
$
P
(
``
sick
''
)
=
0,1
%
=
0.01
$
$
and
$
$
P
(
``
well
''
)
=
99,9
%
=
0.999
$
$
the
follow
is
true
for
a
screen
test
:
If
you
have
the
diseas
,
the
test
will
be
posit
99
%
of
the
time
,
and
if
you
do
n't
have
it
,
the
test
will
be
neg
99
%
of
the
time
:
P
(
``
test
posit
''
|
``
well
''
)
=
1
%
and
P
(
``
test
neg
''
|
``
well
''
)
=
99
%
.
final
,
suppos
that
when
the
test
is
appli
to
a
person
have
the
diseas
,
there
is
a
1
%
chanc
of
a
fals
neg
result
(
and
99
%
chanc
of
get
a
true
posit
result
)
,
i.e
.
P
(
``
test
neg
''
|
``
sick
''
)
=
1
%
and
P
(
``
test
posit
''
|
``
sick
''
)
=
99
%
sick
healthi
total
test
result
posit
99
999
1098
test
result
neg
1
98901
98902
total
100
99900
100000
there
are
999
fals
posit
and
1
fals
neg
.
problem
:
In
mani
case
even
medic
profession
assum
that
``
if
you
have
thi
sick
,
the
test
will
be
posit
in
99
%
of
the
time
and
if
you
do
n't
have
it
,
the
test
will
be
neg
99
%
of
the
time
.
out
of
the
1098
case
that
report
posit
result
onli
99
(
9
%
)
case
are
correct
and
999
case
are
fals
posit
(
91
%
)
,
i.e
.
if
a
person
get
a
posit
test
result
,
the
probabl
that
he
or
she
actual
ha
the
diseas
is
just
about
9
%
.
P
(
``
sick
''
|
``
test
posit
''
)
=
99
/
1098
=
9.02
%
bay
'
theoremw
calcul
the
condit
probabl
$
P
(
GS
|
S
)
$
,
which
wa
the
probabl
that
a
person
speak
german
,
if
he
or
she
is
known
to
be
swiss
.
To
calcul
thi
we
use
the
follow
equat
:
$
$
P
(
GS
|
S
)
=
\frac
{
P
(
GS
,
S
)
}
{
P
(
S
)
}
$
$
what
about
calcul
the
probabl
$
P
(
S
|
GS
)
$
,
i.e
.
the
probabl
that
somebodi
is
swiss
under
the
assumpt
that
the
person
speek
german
?
the
equat
look
like
thi
:
$
$
P
(
S|
GS
)
=
\frac
{
P
(
GS
,
S
)
}
{
P
(
GS
)
}
$
$
let
's
isol
on
both
equat
$
P
(
GS
,
S
)
$
:
$
$
P
(
GS
,
S
)
=
P
(
GS
|
S
)
P
(
S
)
$
$
$
$
P
(
GS
,
S
)
=
P
(
S
|
GS
)
P
(
GS
)
$
$
As
the
left
side
are
equal
,
the
right
side
have
to
be
equal
as
well
:
$
$
P
(
GS
|
S
)
*
P
(
S
)
=
P
(
S
|
GS
)
P
(
GS
)
$
$
thi
equat
can
be
transform
into
:
$
$
P
(
S
|
GS
)
=
\frac
{
P
(
GS
|
S
)
P
(
S
)
}
{
P
(
GS
)
}
$
$
the
result
correspont
to
bay
'
theorem
To
solv
our
problem
,
-
i.e
.
the
probabl
that
a
person
is
swiss
,
if
we
know
that
he
or
she
speak
german
-
all
we
have
to
do
is
calcul
the
right
side
.
We
know
alreadi
from
our
previou
exercis
that
$
$
P
(
GS
|
S
)
=
0.64
$
$
and
$
$
P
(
S
)
=
0.00112
$
$
the
number
of
german
nativ
speaker
in
the
world
correspond
to
101
million
,
so
we
know
that
$
$
P
(
GS
)
=
\frac
{
101
}
{
7500
}
=
0.0134667
$
$
final
,
we
can
calcul
$
P
(
S
|
GS
)
$
by
substitut
the
valu
in
our
equat
:
$
$
P
(
S
|
GS
)
=
\frac
{
P
(
GS
|
S
)
P
(
S
)
}
{
P
(
GS
)
}
=
\frac
{
0.64
*
0.00112
}
{
0.0134667
}
=
0.0532276
$
$
there
are
about
8.4
million
peopl
live
in
switzerland
.
about
64
%
of
them
speak
german
.
there
are
about
7500
million
peopl
on
earth
.
If
the
some
alien
randomli
beam
up
an
earthl
,
what
are
the
chanc
that
he
is
a
german
speak
swiss
?
We
have
the
event
$
S
$
:
be
swiss
$
GS
$
:
german
speak
$
$
P
(
S
)
=
\frac
{
8.4
}
{
7500
}
=
0.00112
$
$
$
$
P
(
a|b
)
=
\frac
{
P
(
b|a
)
P
(
A
)
}
{
P
(
B
)
}
$
$
$
P
(
a|b
)
$
is
the
condit
probabl
of
$
A
$
,
given
$
B
$
(
posterior
probabl
)
,
$
P
(
B
)
$
is
the
prior
probabl
of
$
B
$
and
$
P
(
A
)
$
the
prior
probabl
of
$
A
$
.
$
P
(
b|a
)
$
is
the
condit
probabl
of
$
B
$
given
$
A
$
,
call
the
likely-hood
.
An
advantag
of
the
naiv
bay
classifi
is
that
it
requir
onli
a
small
amount
of
train
data
to
estim
the
paramet
necessari
for
classif
.
becaus
independ
variabl
are
assum
,
onli
the
varianc
of
the
variabl
for
each
class
need
to
be
determin
and
not
the
entir
covari
matrix
.
naiv
bay
classifierintroductori
exercis
let
's
set
out
on
a
journey
by
train
to
creat
our
first
veri
simpl
naiv
bay
classifi
.
let
us
assum
we
are
in
the
citi
of
hamburg
and
we
want
to
travel
to
munich
.
We
will
have
to
chang
train
in
frankfurt
am
main
.
We
know
from
previou
train
journey
that
our
train
from
hamburg
might
be
delay
and
the
we
will
not
catch
our
connect
train
in
frankfurt
.
the
probabl
that
we
will
not
be
in
time
for
our
connect
train
depend
on
how
high
our
possibl
delay
will
be
.
the
connect
train
will
not
wait
for
more
than
five
minut
.
sometim
the
other
train
is
delay
as
well
.
the
follow
list
'in_tim
'
(
the
train
from
hamburg
arriv
in
time
to
catch
the
connect
train
to
munich
)
and
'too_lat
'
(
connect
train
is
miss
)
are
data
show
the
situat
over
some
week
.
the
first
compon
of
each
tupl
show
the
minut
the
train
wa
late
and
the
second
compon
show
the
number
of
time
thi
occur
.
#
the
tupl
consist
of
(
delay
time
of
train1
,
number
of
time
)
#
tupl
are
(
minut
,
number
of
time
)
in_tim
=
[
(
0
,
22
)
,
(
1
,
19
)
,
(
2
,
17
)
,
(
3
,
18
)
,
(
4
,
16
)
,
(
5
,
15
)
,
(
6
,
9
)
,
(
7
,
7
)
,
(
8
,
4
)
,
(
9
,
3
)
,
(
10
,
3
)
,
(
11
,
2
)
]
too_lat
=
[
(
6
,
6
)
,
(
7
,
9
)
,
(
8
,
12
)
,
(
9
,
17
)
,
(
10
,
18
)
,
(
11
,
15
)
,
(
12,16
)
,
(
13
,
7
)
,
(
14
,
8
)
,
(
15
,
5
)
]
%
matplotlib
inlin
import
matplotlib.pyplot
as
plt
X
,
Y
=
zip
(
*in_tim
)
X2
,
Y2
=
zip
(
*too_lat
)
bar_width
=
0.9
plt.bar
(
X
,
Y
,
bar_width
,
color=
''
blue
''
,
alpha=0.75
,
label=
''
in
time
''
)
bar_width
=
0.8
plt.bar
(
X2
,
Y2
,
bar_width
,
color=
''
red
''
,
alpha=0.75
,
label=
''
too
late
''
)
plt.legend
(
loc='upp
right
'
)
plt.show
(
)
from
thi
data
we
can
deduc
that
the
probabl
of
catch
the
connect
train
if
we
are
one
minut
late
is
1
,
becaus
we
had
19
success
case
experienc
and
no
miss
,
i.e
.
there
is
no
tupl
with
1
as
the
first
compon
in
'too_lat
'
.
We
will
denot
the
event
``
train
arriv
in
time
to
catch
the
connect
train
''
with
$
S
$
(
success
)
and
the
'unlucki
'
event
``
train
arriv
too
late
to
catch
the
connect
train
''
with
$
M
$
(
miss
)
We
can
now
defin
the
probabl
``
catch
the
train
given
that
we
are
1
minut
late
''
formal
:
$
$
P
(
S
|
1
)
=
19
/
19
=
1
$
$
We
use
the
fact
that
the
tupl
$
(
1
,
19
)
$
is
in
'in_tim
'
and
there
is
no
tupl
with
the
first
compon
1
in
'too_lat
'
It
's
get
critic
for
catch
the
connect
train
to
munich
,
if
we
are
6
minut
late
.
yet
,
the
chanc
are
still
60
%
:
$
$
P
(
S
|
6
)
=
9
/
9
+
6
=
0.6
$
$
accordingli
,
the
probabl
for
miss
the
train
know
that
we
are
6
minut
late
is
:
$
$
P
(
M
|
6
)
=
6
/
9
+
6
=
0.4
$
$
We
can
write
a
'classifi
'
function
,
which
will
give
the
probabl
for
catch
the
connect
train
:
in_time_dict
=
dict
(
in_tim
)
too_late_dict
=
dict
(
too_lat
)
def
catch_the_train
(
min
)
:
s
=
in_time_dict.get
(
min
,
0
)
if
s
==
0
:
return
0
els
:
m
=
too_late_dict.get
(
min
,
0
)
return
s
/
(
s
+
m
)
for
minut
in
rang
(
-1
,
13
)
:
print
(
minut
,
catch_the_train
(
minut
)
)
-1
0
0
1.0
1
1.0
2
1.0
3
1.0
4
1.0
5
1.0
6
0.6
7
0.4375
8
0.25
9
0.15
10
0.14285714285714285
11
0.11764705882352941
12
0
A
naiv
bay
classifi
exampleget
the
data
readyw
will
use
a
file
call
'person_data.txt
'
.
It
contain
100
random
person
data
,
male
and
femal
,
with
bodi
size
,
weight
and
gender
tag
.
import
numpi
as
np
gender
=
[
``
male
''
,
``
femal
''
]
person
=
[
]
with
open
(
``
data/person_data.txt
''
)
as
fh
:
for
line
in
fh
:
persons.append
(
line.strip
(
)
.split
(
)
)
firstnam
=
{
}
height
=
{
}
for
gender
in
gender
:
firstnam
[
gender
]
=
[
x
[
0
]
for
x
in
person
if
x
[
4
]
==gender
]
height
[
gender
]
=
[
x
[
2
]
for
x
in
person
if
x
[
4
]
==gender
]
height
[
gender
]
=
np.array
(
height
[
gender
]
,
np.int
)
for
gender
in
(
``
femal
''
,
``
male
''
)
:
print
(
gender
+
``
:
''
)
print
(
firstnam
[
gender
]
[
:10
]
)
print
(
height
[
gender
]
[
:10
]
)
femal
:
[
'stephani
'
,
'cynthia
'
,
'katherin
'
,
'elizabeth
'
,
'carol
'
,
'christina
'
,
'beverli
'
,
'sharon
'
,
'denis
'
,
'rebecca
'
]
[
149
174
183
138
145
161
179
162
148
196
]
male
:
[
'randi
'
,
'jessi
'
,
'david
'
,
'stephen
'
,
'jerri
'
,
'billi
'
,
'earl
'
,
'todd
'
,
'martin
'
,
'kenneth
'
]
[
184
175
187
192
204
180
184
174
177
200
]
warn
:
there
might
be
some
confus
between
a
python
class
and
a
naiv
bay
class
.
We
tri
to
avoid
it
by
say
explicitli
what
is
meant
,
whenev
possibl
!
design
a
featur
classw
will
now
defin
a
python
class
``
featur
''
for
the
featur
,
which
we
will
use
for
classif
later
.
the
featur
class
need
a
label
,
e.g
.
``
height
''
or
``
firstnam
''
.
If
the
featur
valu
are
numer
we
may
want
to
``
bin
''
them
to
reduc
the
number
of
possibl
featur
valu
.
the
height
from
our
person
have
a
huge
rang
and
we
have
onli
50
measur
valu
for
our
naiv
bay
class
``
male
''
and
``
femal
''
.
We
will
bin
them
into
rang
``
130
to
134
''
,
``
135
to
139
''
,
``
140
to
144
''
and
so
on
by
set
bin_width
to
5
.
there
is
no
way
of
bin
the
first
name
,
so
bin_width
will
be
set
to
none
.
the
method
frequenc
return
the
number
of
occurr
for
a
certain
featur
valu
or
a
bin
rang
.
from
collect
import
counter
import
numpi
as
np
class
featur
:
def
__init__
(
self
,
data
,
name=non
,
bin_width=non
)
:
self.nam
=
name
self.bin_width
=
bin_width
if
bin_width
:
self.min
,
self.max
=
min
(
data
)
,
max
(
data
)
bin
=
np.arang
(
(
self.min
//
bin_width
)
*
bin_width
,
(
self.max
//
bin_width
)
*
bin_width
,
bin_width
)
freq
,
bin
=
np.histogram
(
data
,
bin
)
self.freq_dict
=
dict
(
zip
(
bin
,
freq
)
)
self.freq_sum
=
sum
(
freq
)
els
:
self.freq_dict
=
dict
(
counter
(
data
)
)
self.freq_sum
=
sum
(
self.freq_dict.valu
(
)
)
def
frequenc
(
self
,
valu
)
:
if
self.bin_width
:
valu
=
(
valu
//
self.bin_width
)
*
self.bin_width
if
valu
in
self.freq_dict
:
return
self.freq_dict
[
valu
]
els
:
return
0
We
will
creat
now
two
featur
class
featur
for
the
height
valu
of
the
person
data
set
.
one
featur
class
contain
the
height
for
the
naiv
bay
class
``
male
''
and
one
the
height
for
the
class
``
femal
''
:
ft
=
{
}
for
gender
in
gender
:
ft
[
gender
]
=
featur
(
height
[
gender
]
,
name=gend
,
bin_width=5
)
print
(
gender
,
ft
[
gender
]
.freq_dict
)
male
{
160
:
5
,
195
:
2
,
180
:
5
,
165
:
4
,
200
:
3
,
185
:
8
,
170
:
6
,
155
:
1
,
190
:
8
,
175
:
7
}
femal
{
160
:
8
,
130
:
1
,
165
:
11
,
135
:
1
,
170
:
7
,
140
:
0
,
175
:
2
,
145
:
3
,
180
:
4
,
150
:
5
,
185
:
0
,
155
:
7
}
bar
chart
of
frequenc
distributionw
print
out
the
frequenc
of
our
bin
,
but
it
is
a
lot
better
to
see
these
valu
dipict
in
a
bar
chart
.
We
will
do
thi
with
the
follow
code
:
for
gender
in
gender
:
frequenc
=
list
(
ft
[
gender
]
.freq_dict.item
(
)
)
frequencies.sort
(
key=lambda
x
:
x
[
1
]
)
X
,
Y
=
zip
(
*frequenc
)
color
=
``
blue
''
if
gender==
''
male
''
els
``
red
''
bar_width
=
4
if
gender==
''
male
''
els
3
plt.bar
(
X
,
Y
,
bar_width
,
color=color
,
alpha=0.75
,
label=gend
)
plt.legend
(
loc='upp
right
'
)
plt.show
(
)
We
have
to
design
now
a
naiv
bay
class
in
python
.
We
will
call
it
nbclass
.
An
nbclass
contain
one
or
more
featur
class
.
the
name
of
the
nbclass
will
be
store
in
self.nam
.
class
nbclass
:
def
__init__
(
self
,
name
,
*featur
)
:
self.featur
=
featur
self.nam
=
name
def
probability_value_given_featur
(
self
,
feature_valu
,
featur
)
:
``
''
''
p_value_given_featur
return
the
probabl
p
for
a
feature_valu
'valu
'
of
the
featur
to
occurr
correspond
to
P
(
d_i
|
p_j
)
where
d_i
is
a
featur
variabl
of
the
featur
i
``
''
''
if
feature.freq_sum
==
0
:
return
0
els
:
return
feature.frequ
(
feature_valu
)
/
feature.freq_sum
In
the
follow
code
,
we
will
creat
nbclass
with
one
featur
,
i.e
.
the
height
featur
.
We
will
use
the
featur
class
of
ft
,
which
we
have
previous
creat
:
cl
=
{
}
for
gender
in
gender
:
cl
[
gender
]
=
nbclass
(
gender
,
ft
[
gender
]
)
the
final
step
for
creat
a
simpl
naiv
bay
classifi
consist
in
write
a
class
'classifi
'
,
which
will
use
our
class
'nbclass
'
and
'featur
'
.
class
classifi
:
def
__init__
(
self
,
*nbclass
)
:
self.nbclass
=
nbclass
def
prob
(
self
,
*d
,
best_only=tru
)
:
nbclass
=
self.nbclass
probability_list
=
[
]
for
nbclass
in
nbclass
:
ftr
=
nbclass.featur
prob
=
1
for
i
in
rang
(
len
(
ftr
)
)
:
prob
*=
nbclass.probability_value_given_featur
(
d
[
i
]
,
ftr
[
i
]
)
probability_list.append
(
(
prob
,
nbclass.nam
)
)
prob_valu
=
[
f
[
0
]
for
f
in
probability_list
]
prob_sum
=
sum
(
prob_valu
)
if
prob_sum==0
:
number_class
=
len
(
self.nbclass
)
pl
=
[
]
for
prob_el
in
probability_list
:
pl.append
(
(
(
1
/
number_class
)
,
prob_el
[
1
]
)
)
probability_list
=
pl
els
:
probability_list
=
[
(
p
[
0
]
/
prob_sum
,
p
[
1
]
)
for
p
in
probability_list
]
if
best_onli
:
return
max
(
probability_list
)
els
:
return
probability_list
We
will
creat
a
classifi
with
one
featur
class
'height
'
.
We
check
it
with
valu
between
130
and
220
cm
.
c
=
classifi
(
cl
[
``
male
''
]
,
cl
[
``
femal
''
]
)
for
i
in
rang
(
130
,
220
,
5
)
:
print
(
i
,
c.prob
(
i
,
best_only=fals
)
)
130
[
(
0.0
,
'male
'
)
,
(
1.0
,
'femal
'
)
]
135
[
(
0.0
,
'male
'
)
,
(
1.0
,
'femal
'
)
]
140
[
(
0.5
,
'male
'
)
,
(
0.5
,
'femal
'
)
]
145
[
(
0.0
,
'male
'
)
,
(
1.0
,
'femal
'
)
]
150
[
(
0.0
,
'male
'
)
,
(
1.0
,
'femal
'
)
]
155
[
(
0.125
,
'male
'
)
,
(
0.875
,
'femal
'
)
]
160
[
(
0.38461538461538469
,
'male
'
)
,
(
0.61538461538461542
,
'femal
'
)
]
165
[
(
0.26666666666666666
,
'male
'
)
,
(
0.73333333333333328
,
'femal
'
)
]
170
[
(
0.46153846153846162
,
'male
'
)
,
(
0.53846153846153855
,
'femal
'
)
]
175
[
(
0.77777777777777779
,
'male
'
)
,
(
0.22222222222222224
,
'femal
'
)
]
180
[
(
0.55555555555555558
,
'male
'
)
,
(
0.44444444444444448
,
'femal
'
)
]
185
[
(
1.0
,
'male
'
)
,
(
0.0
,
'femal
'
)
]
190
[
(
1.0
,
'male
'
)
,
(
0.0
,
'femal
'
)
]
195
[
(
1.0
,
'male
'
)
,
(
0.0
,
'femal
'
)
]
200
[
(
1.0
,
'male
'
)
,
(
0.0
,
'femal
'
)
]
205
[
(
0.5
,
'male
'
)
,
(
0.5
,
'femal
'
)
]
210
[
(
0.5
,
'male
'
)
,
(
0.5
,
'femal
'
)
]
215
[
(
0.5
,
'male
'
)
,
(
0.5
,
'femal
'
)
]
there
are
no
person
-
neither
male
nor
femal
-
in
our
learn
set
,
with
a
bodi
height
between
140
and
144
.
thi
is
the
reason
,
whi
our
classifi
ca
n't
base
it
result
on
learn
data
and
therefor
come
back
with
a
fify-fifti
result
.
We
can
also
train
a
classifi
with
our
firstnam
:
ft
=
{
}
cl
=
{
}
for
gender
in
gender
:
fts_name
=
featur
(
firstnam
[
gender
]
,
name=gend
)
cl
[
gender
]
=
nbclass
(
gender
,
fts_name
)
c
=
classifi
(
cl
[
``
male
''
]
,
cl
[
``
femal
''
]
)
testnam
=
[
'edgar
'
,
'benjamin
'
,
'fred
'
,
'albert
'
,
'laura
'
,
'maria
'
,
'paula
'
,
'sharon
'
,
'jessi
'
]
for
name
in
testnam
:
print
(
name
,
c.prob
(
name
)
)
edgar
(
0.5
,
'male
'
)
benjamin
(
1.0
,
'male
'
)
fred
(
1.0
,
'male
'
)
albert
(
1.0
,
'male
'
)
laura
(
1.0
,
'femal
'
)
maria
(
1.0
,
'femal
'
)
paula
(
1.0
,
'femal
'
)
sharon
(
1.0
,
'femal
'
)
jessi
(
0.6666666666666667
,
'femal
'
)
the
name
``
jessi
''
is
an
ambigu
name
.
there
are
about
66
boy
per
100
girl
with
thi
name
.
We
can
learn
from
the
previou
classif
result
that
the
probabl
for
the
name
``
jessi
''
be
``
femal
''
is
about
two-third
,
which
is
calcul
from
our
data
set
``
person
''
:
[
person
for
person
in
person
if
person
[
0
]
==
``
jessi
''
]
after
have
execut
the
python
code
abov
we
receiv
the
follow
result
:
[
[
'jessi
'
,
'morgan
'
,
'175
'
,
'67.0
'
,
'male
'
]
,
[
'jessi
'
,
'bell
'
,
'165
'
,
'65
'
,
'femal
'
]
,
[
'jessi
'
,
'washington
'
,
'159
'
,
'56
'
,
'femal
'
]
,
[
'jessi
'
,
'davi
'
,
'174
'
,
'45
'
,
'femal
'
]
,
[
'jessi
'
,
'johnson
'
,
'165
'
,
'30.0
'
,
'male
'
]
,
[
'jessi
'
,
'thoma
'
,
'168
'
,
'69
'
,
'femal
'
]
]
jessi
washington
is
onli
159
cm
tall
.
If
we
have
a
look
at
the
result
of
our
classifi
,
train
with
height
,
we
see
that
the
likelihood
for
a
person
159
cm
tall
of
be
``
femal
''
is
0.875
.
So
what
about
an
unknown
person
call
``
jessi
''
and
be
159
cm
tall
?
Is
thi
person
femal
or
male
?
To
answer
thi
question
,
we
will
train
an
naiv
bay
classifi
with
two
featur
class
,
i.e
.
height
and
firstnam
:
cl
=
{
}
for
gender
in
gender
:
fts_height
=
featur
(
height
[
gender
]
,
name=
''
height
''
,
bin_width=5
)
fts_name
=
featur
(
firstnam
[
gender
]
,
name=
''
name
''
)
cl
[
gender
]
=
nbclass
(
gender
,
fts_name
,
fts_height
)
c
=
classifi
(
cl
[
``
male
''
]
,
cl
[
``
femal
''
]
)
for
d
in
[
(
``
maria
''
,
140
)
,
(
``
anthoni
''
,
200
)
,
(
``
anthoni
''
,
153
)
,
(
``
jessi
''
,
188
)
,
(
``
jessi
''
,
159
)
,
(
``
jessi
''
,
160
)
]
:
print
(
d
,
c.prob
(
*d
,
best_only=fals
)
)
(
'maria
'
,
140
)
[
(
0.5
,
'male
'
)
,
(
0.5
,
'femal
'
)
]
(
'anthoni
'
,
200
)
[
(
1.0
,
'male
'
)
,
(
0.0
,
'femal
'
)
]
(
'anthoni
'
,
153
)
[
(
0.5
,
'male
'
)
,
(
0.5
,
'femal
'
)
]
(
'jessi
'
,
188
)
[
(
1.0
,
'male
'
)
,
(
0.0
,
'femal
'
)
]
(
'jessi
'
,
159
)
[
(
0.066666666666666666
,
'male
'
)
,
(
0.93333333333333335
,
'femal
'
)
]
(
'jessi
'
,
160
)
[
(
0.23809523809523817
,
'male
'
)
,
(
0.76190476190476197
,
'femal
'
)
]
the
underli
theoryour
classifi
from
the
previou
exampl
is
base
on
the
bay
theorem
:
$
$
P
(
c_j
|
d
)
=
\frac
{
P
(
d
|
c_j
)
P
(
c_j
)
}
{
P
(
d
)
}
$
$
where
$
P
(
c_j
|
d
)
$
is
the
probabl
of
instanc
d
be
in
class
c_j
,
it
is
the
result
we
want
to
calcul
with
our
classifi
$
P
(
d
|
c_j
)
$
is
the
probabl
of
gener
the
instanc
d
,
if
the
class
$
c_j
$
is
given
$
P
(
c_j
)
$
is
the
probabl
for
the
occurr
of
class
$
c_j
$
We
did
n't
use
it
in
our
classifi
,
becaus
both
class
in
our
exampl
have
been
equal
like
.
P
(
d
)
is
the
probabl
for
the
occurr
of
an
instanc
d
It
's
not
need
in
the
calcul
,
becaus
it
is
the
same
for
all
class
.
We
had
use
onli
one
featur
in
our
previou
exampl
,
i.e
.
the
'height
'
or
the
name
.
It
's
possibl
to
defin
a
bay
classifi
with
multipl
featur
,
e.g
.
$
d
=
(
d_1
,
d_2
,
...
,
d_n
)
$
We
get
the
follow
formula
:
$
$
P
(
c_j
|
d
)
=
\frac
{
1
}
{
P
(
d
)
}
\displaystyl
\prod_
{
i=1
}
^
{
n
}
P
(
d_i
|
c_j
)
P
(
c_j
)
$
$
$
\frac
{
1
}
{
P
(
d
)
}
$
is
onli
depend
on
the
valu
of
$
d_1
,
d_2
,
...
d_n
$
.
thi
mean
that
it
is
a
constant
as
the
valu
of
the
featur
variabl
are
known
.
In
[
]
:
previou
chapter
:
machin
learn
with
scikit
and
python
next
chapter
:
naiv
bay
classifi
with
scikit
©
2011
-
2018
,
bernd
klein
,
bodenseo
;
design
by
denis
mitchinson
adapt
for
python-course.eu
by
bernd
klein
