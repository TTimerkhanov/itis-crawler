machin
learn
with
python
:
neural
network
with
python
use
numpi
python
machin
learn
tutori
machin
learn
machin
learn
terminologyk-nearest
neighbor
classifierneur
network
from
scratch
in
pythonneur
network
in
python
use
numypybackpropag
in
neural
networksconfus
matrixtrain
and
test
with
mnistdropout
neural
networksneur
network
with
scikitmachin
learn
with
scikit
and
pythonintroduct
naiv
bay
classifierna
bay
classifi
with
scikitintroduct
into
text
classif
use
naiv
bayespython
implement
of
text
classificationdecis
treesregress
treesrandom
forestsboost
algorithmprincip
compon
analysislinear
discrimin
analysisexpect
maxim
and
gaussian
mixtur
modelintroduct
into
tensorflow
quot
“
artifici
intellig
would
be
the
ultim
version
of
googl
.
the
ultim
search
engin
that
would
understand
everyth
on
the
web
.
It
would
understand
exactli
what
you
want
,
and
it
would
give
you
the
right
thing
.
We
’
re
nowher
near
do
that
now
.
howev
,
we
can
get
increment
closer
to
that
,
and
that
is
basic
what
we
work
on.
”
(
larri
wall
)
“
machin
intellig
is
the
last
invent
that
human
will
ever
need
to
make.
”
(
nick
bostrom
)
thi
websit
is
creat
and
maintain
by
:
bernd
klein
,
On
site
train
in
europ
,
canada
and
the
US
.
thi
websit
is
free
of
annoy
ad
.
We
want
to
keep
it
like
thi
.
you
can
help
with
your
donat
:
the
need
for
donat
bernd
klein
on
facebook
search
thi
websit
:
classroom
train
cours
thi
websit
contain
a
free
and
extens
onlin
tutori
by
bernd
klein
,
use
materi
from
hi
classroom
python
train
cours
.
If
you
are
interest
in
an
instructor-l
classroom
train
cours
,
you
may
have
a
look
at
the
python
class
by
bernd
klein
at
bodenseo
.
©
kabliczech
-
fotolia.com
quot
of
the
day
:
''
A
program
languag
is
for
think
about
program
,
not
for
express
program
you
've
alreadi
thought
of
.
It
should
be
a
pencil
,
not
a
pen.
``
(
paul
graham
)
If
you
have
the
choic
work
with
python
2
or
python
3
,
we
recomend
to
switch
to
python
3
!
you
can
read
our
python
tutori
to
see
what
the
differ
are
.
data
protect
declar
data
protect
declar
previou
chapter
:
neural
network
from
scratch
in
python
next
chapter
:
backpropag
in
neural
network
neural
network
use
python
and
numpi
introduct
We
have
introduc
the
basic
idea
about
neuron
network
in
the
previou
chapter
of
our
tutori
.
We
point
out
the
similar
between
neuron
and
neural
network
in
biolog
.
We
also
introduc
veri
small
artici
neural
network
and
introduc
decis
boundari
and
the
xor
problem
.
the
focu
in
our
previou
chapter
had
not
been
on
effici
.
We
will
introduc
a
neural
network
class
in
python
in
thi
chapter
,
which
will
use
the
power
and
effici
data
structur
of
numpi
.
thi
way
,
we
get
a
more
effici
network
than
in
our
previou
chapter
.
when
we
say
``
more
effici
''
,
we
do
not
mean
that
the
artifici
neural
network
encount
in
thi
chaper
of
our
tutori
are
effici
and
readi
for
real
life
usag
.
they
are
still
quit
slow
compar
to
implement
from
sklearn
for
exampl
.
the
focu
is
to
implement
a
veri
basic
neural
network
and
by
do
thi
explain
the
basic
idea
.
We
want
to
demonstr
simpl
and
easi
to
grasp
network
.
idea
like
how
the
signal
flow
insid
of
a
network
work
,
how
to
implement
weight
.
how
to
initi
weight
matric
or
what
activ
function
can
be
use
.
We
will
start
with
a
simpl
neural
network
consist
of
three
layer
,
i.e
.
the
input
layer
,
a
hidden
layer
and
an
output
layer
.
A
simpl
artifici
neural
network
structurey
can
see
a
simpl
neural
network
structur
in
the
follow
diagram
.
We
have
an
input
layer
with
three
node
$
i_1
,
i_2
,
i_3
$
these
node
get
the
correspond
input
valu
$
x_1
,
x_2
,
x_3
$
.
the
middl
or
hidden
layer
ha
four
node
$
h_1
,
h_2
,
h_3
,
h_4
$
.
the
input
of
thi
layer
stem
from
the
input
layer
.
We
will
discuss
the
mechan
soon
.
final
,
our
output
layer
consist
of
the
two
node
$
o_1
,
o_2
$
We
have
to
note
that
some
would
call
thi
a
two
layer
network
,
becaus
they
do
n't
count
the
input
as
a
layer
.
the
input
layer
consist
of
the
node
$
i_1
$
,
$
i_2
$
and
$
i_3
$
.
In
principl
the
input
is
a
one-dimension
vector
,
like
(
2
,
4
,
11
)
.
A
one-dimension
vector
is
repres
in
numpi
like
thi
:
import
numpi
as
np
input_vector
=
np.array
(
[
2
,
4
,
11
]
)
print
(
input_vector
)
[
2
4
11
]
In
the
algorithm
,
which
we
will
write
later
,
we
will
have
to
transpos
it
into
a
column
vector
,
i.e
.
a
two-dimension
array
with
just
one
column
:
import
numpi
as
np
input_vector
=
np.array
(
[
2
,
4
,
11
]
)
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
print
(
input_vector
,
input_vector.shap
)
[
[
2
]
[
4
]
[
11
]
]
(
3
,
1
)
weight
and
matriceseach
of
the
arrow
in
our
network
diagram
ha
an
associ
weight
valu
.
We
will
onli
look
at
the
arrow
between
the
input
and
the
output
layer
now
.
the
valu
$
x_1
$
go
into
the
node
$
i_1
$
will
be
distribut
accord
to
the
valu
of
the
weight
.
In
the
follow
diagram
we
have
ad
some
exampl
valu
.
use
these
valu
,
the
input
valu
(
$
ih_1
,
ih_2
,
ih_3
,
ih_4
$
into
the
node
(
$
h_1
,
h_2
,
h_3
,
h_4
$
)
of
the
hidden
layer
can
be
calcul
like
thi
:
$
ih_1
=
0.81
*
0.5
+
0.12
*
1
+
0.92
*
0.8
$
$
ih_2
=
0.33
*
0.5
+
0.44
*
1
+
0.72
*
0.8
$
$
ih_3
=
0.29
*
0.5
+
0.22
*
1
+
0.53
*
0.8
$
$
ih_4
=
0.37
*
0.5
+
0.12
*
1
+
0.27
*
0.8
$
those
familiar
with
matric
and
matrix
multipl
will
see
where
it
is
boil
down
to
.
We
will
redraw
our
network
and
denot
the
weight
with
$
w_
{
ij
}
$
:
In
order
to
effici
execut
all
the
necessari
calacul
,
we
will
arrang
the
weight
into
a
weight
matrix
.
the
weight
in
our
diagram
abov
build
an
array
,
which
we
will
call
'weights_in_hidden
'
in
our
neural
network
class
.
the
name
should
indic
that
the
weight
are
connect
the
input
and
the
hidden
node
,
i.e
.
they
are
between
the
input
and
the
hidden
layer
.
We
will
also
abbrevi
the
name
as
'wih
'
.
the
weight
matrix
between
the
hidden
and
the
output
layer
will
be
denot
as
``
who
''
.
:
now
that
we
have
defin
our
weight
matric
,
we
have
to
take
the
next
step
.
We
have
to
multipli
the
matrix
wih
the
input
vector
.
btw
.
thi
is
exactli
what
we
have
manual
done
in
our
previou
exampl
.
$
$
\left
(
\begin
{
array
}
{
cc
}
y_1\\y_2\\y_3\\y_4\end
{
array
}
\right
)
=\left
(
\begin
{
array
}
{
cc
}
w_
{
11
}
&
w_
{
12
}
&
w_
{
13
}
\\w_
{
21
}
&
w_
{
22
}
&
w_
{
23
}
\\w_
{
31
}
&
w_
{
32
}
&
w_
{
33
}
\\w_
{
41
}
&
w_
{
42
}
&
w_
{
43
}
\end
{
array
}
\right
)
\left
(
\begin
{
array
}
{
cc
}
x_1\\x_2\\x_3\end
{
array
}
\right
)
=\left
(
\begin
{
array
}
{
cc
}
w_
{
11
}
\cdot
x_1
+
w_
{
12
}
\cdot
x_2
+
w_
{
13
}
\cdot
x_3\\w_
{
21
}
\cdot
x_1
+
w_
{
22
}
\cdot
x_2
+
w_
{
23
}
\cdot
x_3\\w_
{
31
}
\cdot
x_1
+
w_
{
32
}
\cdot
x_2
+
w_
{
33
}
\cdot
x_3\\w_
{
41
}
\cdot
x_1
+
w_
{
42
}
\cdot
x_2
+
w_
{
43
}
\cdot
x_3\end
{
array
}
\right
)
$
$
We
have
a
similar
situat
for
the
'who
'
matrix
between
hidden
and
output
layer
.
So
the
output
$
z_1
$
and
$
z_2
$
from
the
node
$
o_1
$
and
$
o_2
$
can
also
be
calcul
with
matrix
multipl
:
$
$
\left
(
\begin
{
array
}
{
cc
}
z_1\\z_2\end
{
array
}
\right
)
=\left
(
\begin
{
array
}
{
cc
}
wh_
{
11
}
&
wh_
{
12
}
&
wh_
{
13
}
&
wh_
{
14
}
\\wh_
{
21
}
&
wh_
{
22
}
&
wh_
{
23
}
&
wh_
{
24
}
\end
{
array
}
\right
)
\left
(
\begin
{
array
}
{
cc
}
y_1\\y_2\\y_3\\y_4\end
{
array
}
\right
)
=\left
(
\begin
{
array
}
{
cc
}
wh_
{
11
}
\cdot
y_1
+
wh_
{
12
}
\cdot
y_2
+
wh_
{
13
}
\cdot
y_3
+
wh_
{
14
}
\cdot
y_4\\wh_
{
21
}
\cdot
y_1
+
wh_
{
22
}
\cdot
y_2
+
wh_
{
23
}
\cdot
y_3
+
wh_
{
24
}
\cdot
y_4\end
{
array
}
\right
)
$
$
you
might
have
notic
that
someth
is
miss
in
our
previou
calcul
.
We
show
in
our
introductori
chapter
neural
network
from
scratch
in
python
that
we
have
to
appli
an
activ
or
step
function
$
\phi
$
on
each
of
these
sum
.
the
follow
pictur
depict
the
whole
flow
of
calcul
,
i.e
.
the
matrix
multipl
and
the
succeed
multipl
.
the
matrix
multipl
between
the
matrix
wih
and
the
matrix
of
the
valu
of
the
input
node
$
x_1
,
x_2
,
x_3
$
calcul
the
output
which
will
be
pass
to
the
activ
function
.
the
final
output
$
y_1
,
y_2
,
y_3
,
y_4
$
is
the
input
of
the
weight
matrix
who
:
even
though
treatment
is
complet
analogu
,
we
will
also
have
a
detail
look
at
what
is
go
on
between
our
hidden
layer
and
the
output
layer
:
initi
the
weight
matriceson
of
the
import
choic
which
have
to
be
made
befor
train
a
neural
network
consist
in
initi
the
weight
matric
.
We
do
n't
know
anyth
about
the
possibl
weight
,
when
we
start
.
So
,
we
could
start
with
arbitrari
valu
?
As
we
have
seen
the
input
to
all
the
node
except
the
input
node
is
calcul
by
appli
the
activ
function
to
the
follow
sum
:
$
$
y_j
=
\sum_
{
i=1
}
^
{
n
}
w_
{
ji
}
\cdot
x_i
$
$
(
with
n
be
the
number
of
node
in
the
previou
layer
and
$
y_j
$
is
the
input
to
a
node
of
the
next
layer
)
We
can
easili
see
that
it
would
not
be
a
good
idea
to
set
all
the
weight
valu
to
0
,
becaus
in
thi
case
the
result
of
thi
summat
will
alway
be
zero
.
thi
mean
that
our
network
will
be
incap
of
learn
.
thi
is
the
worst
choic
,
but
initi
a
weight
matrix
to
one
is
also
a
bad
choic
.
the
valu
for
the
weight
matric
should
be
chosen
randomli
and
not
arbitrarili
.
By
choos
a
random
normal
distribut
we
have
broken
possibl
symmetr
situat
,
which
are
bad
for
the
learn
process
.
there
are
variou
way
to
initi
the
weight
matric
randomli
.
the
first
one
we
will
introduc
is
the
uniti
function
from
numpy.random
.
It
creat
sampl
which
are
uniformli
distribut
over
the
half-open
interv
[
low
,
high
)
,
which
mean
that
low
is
includ
and
high
is
exclud
.
each
valu
within
the
given
interv
is
equal
like
to
be
drawn
by
'uniform
'
.
import
numpi
as
np
number_of_sampl
=
1200
low
=
-1
high
=
0
s
=
np.random.uniform
(
low
,
high
,
number_of_sampl
)
#
all
valu
of
s
are
within
the
half
open
interv
[
-1
,
0
)
:
print
(
np.all
(
s
>
=
-1
)
and
np.all
(
s
<
0
)
)
true
the
histogram
of
the
sampl
,
creat
with
the
uniform
function
in
our
previou
exampl
,
look
like
thi
:
import
matplotlib.pyplot
as
plt
plt.hist
(
s
)
plt.show
(
)
the
next
function
we
will
look
at
is
'binomi
'
from
numpy.binomi
:
binomi
(
n
,
p
,
size=non
)
It
draw
sampl
from
a
binomi
distribut
with
specifi
paramet
,
n
trial
and
p
probabl
of
success
where
n
is
an
integ
>
=
0
and
p
is
a
float
in
the
interv
[
0,1
]
.
(
n
may
be
input
as
a
float
,
but
it
is
truncat
to
an
integ
in
use
)
s
=
np.random.binomi
(
100
,
0.5
,
1200
)
plt.hist
(
s
)
plt.show
(
)
We
like
to
creat
random
number
with
a
normal
distribut
,
but
the
number
have
to
be
bound
.
thi
is
not
the
case
with
np.random.norm
(
)
,
becaus
it
doe
n't
offer
ani
bound
paramet
.
We
can
use
truncnorm
from
scipy.stat
for
thi
purpos
.
the
standard
form
of
thi
distribut
is
a
standard
normal
truncat
to
the
rang
[
a
,
b
]
—
notic
that
a
and
b
are
defin
over
the
domain
of
the
standard
normal
.
To
convert
clip
valu
for
a
specif
mean
and
standard
deviat
,
use
:
a
,
b
=
(
myclip_a
-
my_mean
)
/
my_std
,
(
myclip_b
-
my_mean
)
/
my_std
from
scipy.stat
import
truncnorm
s
=
truncnorm
(
a=-2/3.
,
b=2/3.
,
scale=1
,
loc=0
)
.rv
(
size=1000
)
plt.hist
(
s
)
plt.show
(
)
the
function
'truncnorm
'
is
difficult
to
use
.
To
make
life
easier
,
we
defin
a
function
'truncated_norm
'
in
the
follow
to
fascilit
thi
task
:
def
truncated_norm
(
mean=0
,
sd=1
,
low=0
,
upp=10
)
:
return
truncnorm
(
(
low
-
mean
)
/
sd
,
(
upp
-
mean
)
/
sd
,
loc=mean
,
scale=sd
)
X
=
truncated_norm
(
mean=0
,
sd=0.4
,
low=-0.5
,
upp=0.5
)
s
=
x.rv
(
10000
)
plt.hist
(
s
)
plt.show
(
)
further
exampl
:
X1
=
truncated_norm
(
mean=2
,
sd=1
,
low=1
,
upp=10
)
X2
=
truncated_norm
(
mean=5.5
,
sd=1
,
low=1
,
upp=10
)
X3
=
truncated_norm
(
mean=8
,
sd=1
,
low=1
,
upp=10
)
import
matplotlib.pyplot
as
plt
fig
,
ax
=
plt.subplot
(
3
,
sharex=tru
)
ax
[
0
]
.hist
(
x1.rv
(
10000
)
,
normed=tru
)
ax
[
1
]
.hist
(
x2.rv
(
10000
)
,
normed=tru
)
ax
[
2
]
.hist
(
x3.rv
(
10000
)
,
normed=tru
)
plt.show
(
)
We
will
creat
the
link
weight
matrix
now
.
'truncated_norm
'
is
ideal
for
thi
purpos
.
It
is
a
good
idea
to
choos
random
valu
from
within
the
interv
$
$
(
-\frac
{
1
}
{
\sqrt
{
n
}
}
,
\frac
{
1
}
{
\sqrt
{
n
}
}
)
$
$
where
n
denot
the
number
of
input
node
.
So
we
can
creat
our
``
wih
''
matrix
with
:
no_of_input_nod
=
3
no_of_hidden_nod
=
4
rad
=
1
/
np.sqrt
(
no_of_input_nod
)
X
=
truncated_norm
(
mean=2
,
sd=1
,
low=-rad
,
upp=rad
)
wih
=
x.rv
(
(
no_of_hidden_nod
,
no_of_input_nod
)
)
wih
the
abov
code
return
the
follow
:
array
(
[
[
-0.356241
,
0.46875865
,
0.41897957
]
,
[
0.43267439
,
-0.10009341
,
0.35524547
]
,
[
0.45234311
,
0.39339294
,
0.365379
]
,
[
0.49457071
,
-0.44498887
,
0.47409918
]
]
)
similarli
,
we
can
now
defin
the
``
who
''
weight
matrix
:
no_of_hidden_nod
=
4
no_of_output_nod
=
2
rad
=
1
/
np.sqrt
(
no_of_hidden_nod
)
#
thi
is
the
input
in
thi
layer
!
X
=
truncated_norm
(
mean=2
,
sd=1
,
low=-rad
,
upp=rad
)
who
=
x.rv
(
(
no_of_output_nod
,
no_of_hidden_nod
)
)
who
thi
get
us
the
follow
output
:
array
(
[
[
0.03743593
,
0.34516431
,
0.11852342
,
-0.10899819
]
,
[
0.11039838
,
0.41685055
,
-0.39363526
,
0.07941089
]
]
)
A
neural
network
class
We
are
readi
now
to
start
with
the
implement
of
our
neural
network
in
python
.
We
will
need
to
defin
the
train
and
run
method
later
.
instead
of
defin
the
weight
matric
within
the
__init__
method
of
our
python
class
,
we
defin
them
in
a
sparat
method
for
reason
of
clariti
:
import
numpi
as
np
class
neuralnetwork
:
def
__init__
(
self
,
no_of_in_nod
,
no_of_out_nod
,
no_of_hidden_nod
,
learning_r
)
:
self.no_of_in_nod
=
no_of_in_nod
self.no_of_out_nod
=
no_of_out_nod
self.no_of_hidden_nod
=
no_of_hidden_nod
self.learning_r
=
learning_r
self.create_weight_matric
(
)
def
create_weight_matric
(
self
)
:
rad
=
1
/
np.sqrt
(
self.no_of_in_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.weights_in_hidden
=
x.rv
(
(
self.no_of_hidden_nod
,
self.no_of_in_nod
)
)
rad
=
1
/
np.sqrt
(
self.no_of_hidden_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.weights_hidden_out
=
x.rv
(
(
self.no_of_out_nod
,
self.no_of_hidden_nod
)
)
def
train
(
self
)
:
pass
def
run
(
self
)
:
pass
if
__name__
==
``
__main__
''
:
simple_network
=
neuralnetwork
(
no_of_in_nod
=
3
,
no_of_out_nod
=
2
,
no_of_hidden_nod
=
4
,
learning_r
=
0.1
)
print
(
simple_network.weights_in_hidden
)
print
(
simple_network.weights_hidden_out
)
[
[
0.10607641
-0.05716482
0.55752363
]
[
0.33701589
0.05461437
0.5521666
]
[
0.11990863
-0.29320233
-0.43600856
]
[
-0.18218775
-0.20794852
-0.39419628
]
]
[
[
4.82634085e-04
-4.97611184e-01
-3.25708215e-01
-2.61086173e-01
]
[
-2.04995922e-01
-7.08439635e-02
2.66347839e-01
4.87601670e-01
]
]
activ
function
,
sigmoid
and
relurun
our
neural
network
on
some
input
mean
that
we
will
have
a
matrix
multipl
of
the
weight
vector
and
the
input
.
We
have
to
appli
an
activ
function
on
the
output
valu
.
there
are
lot
of
differ
activ
function
use
in
neural
network
.
the
sigmoid
function
belong
to
the
most
often
use
activ
function
.
It
is
defin
as
$
$
\sigma
(
x
)
=
\frac
{
1
}
{
1+e^
{
-x
}
}
$
$
let
us
have
a
look
at
the
graph
of
the
sigmoid
function
.
We
use
matplotlib
to
plot
the
sigmoid
function
:
import
numpi
as
np
import
matplotlib.pyplot
as
plt
def
sigma
(
x
)
:
return
1
/
(
1
+
np.exp
(
-x
)
)
X
=
np.linspac
(
-5
,
5
,
100
)
plt.plot
(
X
,
sigma
(
X
)
,
'
b
'
)
plt.xlabel
(
'
X
axi
'
)
plt.ylabel
(
'
Y
axi
'
)
plt.titl
(
'sigmoid
function
'
)
plt.grid
(
)
plt.text
(
4
,
0.8
,
r
'
$
\sigma
(
x
)
=\frac
{
1
}
{
1+e^
{
-x
}
}
$
'
,
fontsize=16
)
plt.show
(
)
instead
of
defin
the
sigmoid
function
ourselv
,
we
can
use
the
expit
function
from
scipy.speci
,
which
is
an
implement
of
the
sigmoid
function
.
It
can
be
appli
on
variou
data
class
like
int
,
float
,
list
,
numpi
,
ndarray
and
so
on
.
the
result
is
an
ndarray
of
the
same
shape
as
the
input
data
x.
from
scipy.speci
import
expit
print
(
expit
(
3.4
)
)
print
(
expit
(
[
3
,
4
,
1
]
)
)
print
(
expit
(
np.array
(
[
0.8
,
2.3
,
8
]
)
)
)
0.967704535302
[
0.95257413
0.98201379
0.73105858
]
[
0.68997448
0.90887704
0.99966465
]
ad
a
run
methodw
can
use
thi
as
the
activ
function
of
our
neural
network
.
As
you
most
probabl
know
,
we
can
directli
assign
a
new
name
,
when
we
import
the
function
:
from
scipy.speci
import
expit
as
activation_funct
import
numpi
as
np
from
scipy.speci
import
expit
as
activation_funct
from
scipy.stat
import
truncnorm
def
truncated_norm
(
mean=0
,
sd=1
,
low=0
,
upp=10
)
:
return
truncnorm
(
(
low
-
mean
)
/
sd
,
(
upp
-
mean
)
/
sd
,
loc=mean
,
scale=sd
)
class
neuralnetwork
:
def
__init__
(
self
,
no_of_in_nod
,
no_of_out_nod
,
no_of_hidden_nod
,
learning_r
)
:
self.no_of_in_nod
=
no_of_in_nod
self.no_of_out_nod
=
no_of_out_nod
self.no_of_hidden_nod
=
no_of_hidden_nod
self.learning_r
=
learning_r
self.create_weight_matric
(
)
def
create_weight_matric
(
self
)
:
``
''
''
A
method
to
initi
the
weight
matric
of
the
neural
network
''
''
''
rad
=
1
/
np.sqrt
(
self.no_of_in_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.weights_in_hidden
=
x.rv
(
(
self.no_of_hidden_nod
,
self.no_of_in_nod
)
)
rad
=
1
/
np.sqrt
(
self.no_of_hidden_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.weights_hidden_out
=
x.rv
(
(
self.no_of_out_nod
,
self.no_of_hidden_nod
)
)
def
train
(
self
,
input_vector
,
target_vector
)
:
pass
def
run
(
self
,
input_vector
)
:
``
''
''
run
the
network
with
an
input
vector
input_vector
.
input_vector
can
be
tupl
,
list
or
ndarray
``
''
''
#
turn
the
input
vector
into
a
column
vector
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
output_vector
=
np.dot
(
self.weights_in_hidden
,
input_vector
)
output_vector
=
activation_funct
(
output_vector
)
output_vector
=
np.dot
(
self.weights_hidden_out
,
output_vector
)
output_vector
=
activation_funct
(
output_vector
)
return
output_vector
there
is
still
a
train
method
miss
.
We
can
instanti
and
run
thi
network
,
but
the
result
will
not
make
sens
.
they
are
base
on
the
random
weight
matric
:
simple_network
=
neuralnetwork
(
no_of_in_nodes=2
,
no_of_out_nodes=2
,
no_of_hidden_nodes=10
,
learning_rate=0.6
)
simple_network.run
(
[
(
3
,
4
)
]
)
the
abov
code
return
the
follow
output
:
array
(
[
[
0.66413143
]
,
[
0.45385657
]
]
)
We
can
also
defin
our
own
sigmoid
function
with
the
decor
vector
from
numpi
:
@
np.vector
def
sigmoid
(
x
)
:
return
1
/
(
1
+
np.e
**
-x
)
#
sigmoid
=
np.vector
(
sigmoid
)
sigmoid
(
[
3
,
4
,
5
]
)
We
receiv
the
follow
result
:
array
(
[
0.95257413
,
0.98201379
,
0.99330715
]
)
We
add
train
support
in
our
next
class
definit
,
i.e
.
we
defin
the
method
'train
'
:
import
numpi
as
np
@
np.vector
def
sigmoid
(
x
)
:
return
1
/
(
1
+
np.e
**
-x
)
activation_funct
=
sigmoid
from
scipy.stat
import
truncnorm
def
truncated_norm
(
mean=0
,
sd=1
,
low=0
,
upp=10
)
:
return
truncnorm
(
(
low
-
mean
)
/
sd
,
(
upp
-
mean
)
/
sd
,
loc=mean
,
scale=sd
)
class
neuralnetwork
:
def
__init__
(
self
,
no_of_in_nod
,
no_of_out_nod
,
no_of_hidden_nod
,
learning_r
)
:
self.no_of_in_nod
=
no_of_in_nod
self.no_of_out_nod
=
no_of_out_nod
self.no_of_hidden_nod
=
no_of_hidden_nod
self.learning_r
=
learning_r
self.create_weight_matric
(
)
def
create_weight_matric
(
self
)
:
``
''
''
A
method
to
initi
the
weight
matric
of
the
neural
network
''
''
''
rad
=
1
/
np.sqrt
(
self.no_of_in_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.weights_in_hidden
=
x.rv
(
(
self.no_of_hidden_nod
,
self.no_of_in_nod
)
)
rad
=
1
/
np.sqrt
(
self.no_of_hidden_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.weights_hidden_out
=
x.rv
(
(
self.no_of_out_nod
,
self.no_of_hidden_nod
)
)
def
train
(
self
,
input_vector
,
target_vector
)
:
#
input_vector
and
target_vector
can
be
tupl
,
list
or
ndarray
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
target_vector
=
np.array
(
target_vector
,
ndmin=2
)
.T
output_vector1
=
np.dot
(
self.weights_in_hidden
,
input_vector
)
output_vector_hidden
=
activation_funct
(
output_vector1
)
output_vector2
=
np.dot
(
self.weights_hidden_out
,
output_vector_hidden
)
output_vector_network
=
activation_funct
(
output_vector2
)
output_error
=
target_vector
-
output_vector_network
#
updat
the
weight
:
tmp
=
output_error
*
output_vector_network
*
(
1.0
-
output_vector_network
)
tmp
=
self.learning_r
*
np.dot
(
tmp
,
output_vector_hidden.t
)
self.weights_hidden_out
+=
tmp
#
calcul
hidden
error
:
hidden_error
=
np.dot
(
self.weights_hidden_out.t
,
output_error
)
#
updat
the
weight
:
tmp
=
hidden_error
*
output_vector_hidden
*
(
1.0
-
output_vector_hidden
)
self.weights_in_hidden
+=
self.learning_r
*
np.dot
(
tmp
,
input_vector.t
)
def
run
(
self
,
input_vector
)
:
#
input_vector
can
be
tupl
,
list
or
ndarray
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
output_vector
=
np.dot
(
self.weights_in_hidden
,
input_vector
)
output_vector
=
activation_funct
(
output_vector
)
output_vector
=
np.dot
(
self.weights_hidden_out
,
output_vector
)
output_vector
=
activation_funct
(
output_vector
)
return
output_vector
We
will
test
our
network
with
the
same
exampl
,
we
creat
in
the
chapter
[
neural
network
from
scratch
]
(
neural_networks.php
)
:
import
numpi
as
np
from
matplotlib
import
pyplot
as
plt
data1
=
[
(
(
3
,
4
)
,
(
0.99
,
0.01
)
)
,
(
(
4.2
,
5.3
)
,
(
0.99
,
0.01
)
)
,
(
(
4
,
3
)
,
(
0.99
,
0.01
)
)
,
(
(
6
,
5
)
,
(
0.99
,
0.01
)
)
,
(
(
4
,
6
)
,
(
0.99
,
0.01
)
)
,
(
(
3.7
,
5.8
)
,
(
0.99
,
0.01
)
)
,
(
(
3.2
,
4.6
)
,
(
0.99
,
0.01
)
)
,
(
(
5.2
,
5.9
)
,
(
0.99
,
0.01
)
)
,
(
(
5
,
4
)
,
(
0.99
,
0.01
)
)
,
(
(
7
,
4
)
,
(
0.99
,
0.01
)
)
,
(
(
3
,
7
)
,
(
0.99
,
0.01
)
)
,
(
(
4.3
,
4.3
)
,
(
0.99
,
0.01
)
)
]
data2
=
[
(
(
-3
,
-4
)
,
(
0.01
,
0.99
)
)
,
(
(
-2
,
-3.5
)
,
(
0.01
,
0.99
)
)
,
(
(
-1
,
-6
)
,
(
0.01
,
0.99
)
)
,
(
(
-3
,
-4.3
)
,
(
0.01
,
0.99
)
)
,
(
(
-4
,
-5.6
)
,
(
0.01
,
0.99
)
)
,
(
(
-3.2
,
-4.8
)
,
(
0.01
,
0.99
)
)
,
(
(
-2.3
,
-4.3
)
,
(
0.01
,
0.99
)
)
,
(
(
-2.7
,
-2.6
)
,
(
0.01
,
0.99
)
)
,
(
(
-1.5
,
-3.6
)
,
(
0.01
,
0.99
)
)
,
(
(
-3.6
,
-5.6
)
,
(
0.01
,
0.99
)
)
,
(
(
-4.5
,
-4.6
)
,
(
0.01
,
0.99
)
)
,
(
(
-3.7
,
-5.8
)
,
(
0.01
,
0.99
)
)
]
data
=
data1
+
data2
np.random.shuffl
(
data
)
points1
,
labels1
=
zip
(
*data1
)
X
,
Y
=
zip
(
*points1
)
plt.scatter
(
X
,
Y
,
c=
''
r
''
)
points2
,
labels2
=
zip
(
*data2
)
X
,
Y
=
zip
(
*points2
)
plt.scatter
(
X
,
Y
,
c=
''
b
''
)
plt.show
(
)
simple_network
=
neuralnetwork
(
no_of_in_nodes=2
,
no_of_out_nodes=2
,
no_of_hidden_nodes=2
,
learning_rate=0.6
)
size_of_learn_sampl
=
int
(
len
(
data
)
*0.9
)
learn_data
=
data
[
:
size_of_learn_sampl
]
test_data
=
data
[
-size_of_learn_sampl
:
]
print
(
)
for
i
in
rang
(
size_of_learn_sampl
)
:
point
,
label
=
learn_data
[
i
]
[
0
]
,
learn_data
[
i
]
[
1
]
simple_network.train
(
point
,
label
)
for
i
in
rang
(
size_of_learn_sampl
)
:
point
,
label
=
learn_data
[
i
]
[
0
]
,
learn_data
[
i
]
[
1
]
cls1
,
cls2
=simple_network.run
(
point
)
print
(
point
,
cls1
,
cls2
,
end=
''
:
``
)
if
cls1
>
cls2
:
if
label
==
(
0.99
,
0.01
)
:
print
(
``
class1
correct
''
,
label
)
els
:
print
(
``
class2
incorrect
''
,
label
)
els
:
if
label
==
(
0.01
,
0.99
)
:
print
(
``
class1
correct
''
,
label
)
els
:
print
(
``
class2
incorrect
''
,
label
)
the
abov
code
return
the
follow
:
(
4.2
,
5.3
)
[
0.69567493
]
[
0.36574485
]
:
class1
correct
(
0.99
,
0.01
)
(
4
,
6
)
[
0.69599417
]
[
0.3655189
]
:
class1
correct
(
0.99
,
0.01
)
(
4.3
,
4.3
)
[
0.69465373
]
[
0.36646922
]
:
class1
correct
(
0.99
,
0.01
)
(
3.2
,
4.6
)
[
0.69434421
]
[
0.36667755
]
:
class1
correct
(
0.99
,
0.01
)
(
3
,
7
)
[
0.69614915
]
[
0.36540844
]
:
class1
correct
(
0.99
,
0.01
)
(
4
,
3
)
[
0.69015391
]
[
0.36965891
]
:
class1
correct
(
0.99
,
0.01
)
(
5.2
,
5.9
)
[
0.69614659
]
[
0.36541353
]
:
class1
correct
(
0.99
,
0.01
)
(
-2.3
,
-4.3
)
[
0.2887322
]
[
0.63701291
]
:
class1
correct
(
0.01
,
0.99
)
(
-3.6
,
-5.6
)
[
0.28571677
]
[
0.63918581
]
:
class1
correct
(
0.01
,
0.99
)
(
3
,
4
)
[
0.69265701
]
[
0.36786409
]
:
class1
correct
(
0.99
,
0.01
)
(
6
,
5
)
[
0.69593054
]
[
0.365569
]
:
class1
correct
(
0.99
,
0.01
)
(
-1.5
,
-3.6
)
[
0.29421745
]
[
0.6330841
]
:
class1
correct
(
0.01
,
0.99
)
(
-3.7
,
-5.8
)
[
0.2855751
]
[
0.63928833
]
:
class1
correct
(
0.01
,
0.99
)
(
-2
,
-3.5
)
[
0.29319957
]
[
0.63379548
]
:
class1
correct
(
0.01
,
0.99
)
(
3.7
,
5.8
)
[
0.69583411
]
[
0.36563081
]
:
class1
correct
(
0.99
,
0.01
)
(
5
,
4
)
[
0.69461572
]
[
0.36650241
]
:
class1
correct
(
0.99
,
0.01
)
(
-2.7
,
-2.6
)
[
0.29847545
]
[
0.62995199
]
:
class1
correct
(
0.01
,
0.99
)
(
7
,
4
)
[
0.69548262
]
[
0.36589335
]
:
class1
correct
(
0.99
,
0.01
)
(
-3.2
,
-4.8
)
[
0.2866943
]
[
0.63847813
]
:
class1
correct
(
0.01
,
0.99
)
(
-3
,
-4.3
)
[
0.28781573
]
[
0.63766572
]
:
class1
correct
(
0.01
,
0.99
)
(
-3
,
-4
)
[
0.28863857
]
[
0.63706791
]
:
class1
correct
(
0.01
,
0.99
)
someth
to
be
done
in
a
futur
releas
:
We
will
defin
at
a
later
point
also
differ
activ
function
like
the
relu
:
#
altern
activ
function
def
relu
(
x
)
:
return
np.maximum
(
0.0
,
x
)
#
deriv
of
relu
def
relu_deriv
(
x
)
:
if
x
<
=
0
:
return
0
els
:
return
1
import
numpi
as
np
import
matplotlib.pyplot
as
plt
X
=
np.linspac
(
-5
,
5
,
100
)
plt.plot
(
X
,
relu
(
X
)
,
'
b
'
)
plt.xlabel
(
'
X
axi
'
)
plt.ylabel
(
'
Y
axi
'
)
plt.titl
(
'relu
function
'
)
plt.grid
(
)
plt.text
(
3
,
0.8
,
r
'
$
relu
(
x
)
=max
(
0.0
,
x
)
$
'
,
fontsize=16
)
plt.show
(
)
neural
network
with
bia
nodesa
bia
node
is
a
node
that
is
alway
return
the
same
output
.
In
other
word
:
It
is
a
node
which
is
not
depend
on
some
input
and
it
doe
not
have
ani
input
.
the
valu
of
a
bia
node
is
often
set
to
one
,
but
it
can
be
other
valu
as
well
.
except
0
which
doe
n't
make
sens
.
If
a
neural
network
doe
not
have
a
bia
node
in
a
given
layer
,
it
will
not
be
abl
to
produc
output
in
the
next
layer
that
differ
from
0
when
the
featur
valu
are
0
.
gener
speak
,
we
can
say
that
bia
node
are
use
to
increas
the
flexibl
of
the
network
to
fit
the
data
.
usual
,
there
will
be
not
more
than
one
bia
node
per
layer
.
the
onli
except
is
the
output
layer
,
becaus
it
make
no
sens
to
add
a
bia
node
to
thi
layer
.
We
can
see
from
thi
diagram
that
our
weight
matrix
will
have
one
more
column
and
the
bia
valu
is
ad
to
the
input
vector
:
again
,
the
situat
for
the
weight
matrix
between
the
hidden
and
the
outputlay
is
similar
:
the
same
is
true
for
the
correspond
matrix
:
the
follow
is
a
complet
python
class
implement
our
network
with
bia
node
:
import
numpi
as
np
@
np.vector
def
sigmoid
(
x
)
:
return
1
/
(
1
+
np.e
**
-x
)
activation_funct
=
sigmoid
from
scipy.stat
import
truncnorm
def
truncated_norm
(
mean=0
,
sd=1
,
low=0
,
upp=10
)
:
return
truncnorm
(
(
low
-
mean
)
/
sd
,
(
upp
-
mean
)
/
sd
,
loc=mean
,
scale=sd
)
class
neuralnetwork
:
def
__init__
(
self
,
no_of_in_nod
,
no_of_out_nod
,
no_of_hidden_nod
,
learning_r
,
bias=non
)
:
self.no_of_in_nod
=
no_of_in_nod
self.no_of_out_nod
=
no_of_out_nod
self.no_of_hidden_nod
=
no_of_hidden_nod
self.learning_r
=
learning_r
self.bia
=
bia
self.create_weight_matric
(
)
def
create_weight_matric
(
self
)
:
``
''
''
A
method
to
initi
the
weight
matric
of
the
neural
network
with
option
bia
node
''
''
''
bias_nod
=
1
if
self.bia
els
0
rad
=
1
/
np.sqrt
(
self.no_of_in_nod
+
bias_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.weights_in_hidden
=
x.rv
(
(
self.no_of_hidden_nod
,
self.no_of_in_nod
+
bias_nod
)
)
rad
=
1
/
np.sqrt
(
self.no_of_hidden_nod
+
bias_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.weights_hidden_out
=
x.rv
(
(
self.no_of_out_nod
,
self.no_of_hidden_nod
+
bias_nod
)
)
def
train
(
self
,
input_vector
,
target_vector
)
:
#
input_vector
and
target_vector
can
be
tupl
,
list
or
ndarray
bias_nod
=
1
if
self.bia
els
0
if
self.bia
:
#
ad
bia
node
to
the
end
of
the
inpuy_vector
input_vector
=
np.concaten
(
(
input_vector
,
[
self.bia
]
)
)
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
target_vector
=
np.array
(
target_vector
,
ndmin=2
)
.T
output_vector1
=
np.dot
(
self.weights_in_hidden
,
input_vector
)
output_vector_hidden
=
activation_funct
(
output_vector1
)
if
self.bia
:
output_vector_hidden
=
np.concaten
(
(
output_vector_hidden
,
[
[
self.bia
]
]
)
)
output_vector2
=
np.dot
(
self.weights_hidden_out
,
output_vector_hidden
)
output_vector_network
=
activation_funct
(
output_vector2
)
output_error
=
target_vector
-
output_vector_network
#
updat
the
weight
:
tmp
=
output_error
*
output_vector_network
*
(
1.0
-
output_vector_network
)
tmp
=
self.learning_r
*
np.dot
(
tmp
,
output_vector_hidden.t
)
self.weights_hidden_out
+=
tmp
#
calcul
hidden
error
:
hidden_error
=
np.dot
(
self.weights_hidden_out.t
,
output_error
)
#
updat
the
weight
:
tmp
=
hidden_error
*
output_vector_hidden
*
(
1.0
-
output_vector_hidden
)
if
self.bia
:
x
=
np.dot
(
tmp
,
input_vector.t
)
[
:
-1
,
:
]
#
?
?
?
?
last
element
cut
off
,
?
?
?
els
:
x
=
np.dot
(
tmp
,
input_vector.t
)
self.weights_in_hidden
+=
self.learning_r
*
x
def
run
(
self
,
input_vector
)
:
#
input_vector
can
be
tupl
,
list
or
ndarray
if
self.bia
:
#
ad
bia
node
to
the
end
of
the
inpuy_vector
input_vector
=
np.concaten
(
(
input_vector
,
[
1
]
)
)
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
output_vector
=
np.dot
(
self.weights_in_hidden
,
input_vector
)
output_vector
=
activation_funct
(
output_vector
)
if
self.bia
:
output_vector
=
np.concaten
(
(
output_vector
,
[
[
1
]
]
)
)
output_vector
=
np.dot
(
self.weights_hidden_out
,
output_vector
)
output_vector
=
activation_funct
(
output_vector
)
return
output_vector
class1
=
[
(
3
,
4
)
,
(
4.2
,
5.3
)
,
(
4
,
3
)
,
(
6
,
5
)
,
(
4
,
6
)
,
(
3.7
,
5.8
)
,
(
3.2
,
4.6
)
,
(
5.2
,
5.9
)
,
(
5
,
4
)
,
(
7
,
4
)
,
(
3
,
7
)
,
(
4.3
,
4.3
)
]
class2
=
[
(
-3
,
-4
)
,
(
-2
,
-3.5
)
,
(
-1
,
-6
)
,
(
-3
,
-4.3
)
,
(
-4
,
-5.6
)
,
(
-3.2
,
-4.8
)
,
(
-2.3
,
-4.3
)
,
(
-2.7
,
-2.6
)
,
(
-1.5
,
-3.6
)
,
(
-3.6
,
-5.6
)
,
(
-4.5
,
-4.6
)
,
(
-3.7
,
-5.8
)
]
labeled_data
=
[
]
for
el
in
class1
:
labeled_data.append
(
[
el
,
[
1
,
0
]
]
)
for
el
in
class2
:
labeled_data.append
(
[
el
,
[
0
,
1
]
]
)
np.random.shuffl
(
labeled_data
)
print
(
labeled_data
[
:10
]
)
data
,
label
=
zip
(
*labeled_data
)
label
=
np.array
(
label
)
data
=
np.array
(
data
)
[
[
(
-1
,
-6
)
,
[
0
,
1
]
]
,
[
(
-2.3
,
-4.3
)
,
[
0
,
1
]
]
,
[
(
-3
,
-4
)
,
[
0
,
1
]
]
,
[
(
-2
,
-3.5
)
,
[
0
,
1
]
]
,
[
(
3.2
,
4.6
)
,
[
1
,
0
]
]
,
[
(
-3.7
,
-5.8
)
,
[
0
,
1
]
]
,
[
(
4
,
3
)
,
[
1
,
0
]
]
,
[
(
4
,
6
)
,
[
1
,
0
]
]
,
[
(
3.7
,
5.8
)
,
[
1
,
0
]
]
,
[
(
5.2
,
5.9
)
,
[
1
,
0
]
]
]
simple_network
=
neuralnetwork
(
no_of_in_nodes=2
,
no_of_out_nodes=2
,
no_of_hidden_nodes=10
,
learning_rate=0.1
,
bias=non
)
for
_
in
rang
(
20
)
:
for
i
in
rang
(
len
(
data
)
)
:
simple_network.train
(
data
[
i
]
,
label
[
i
]
)
for
i
in
rang
(
len
(
data
)
)
:
print
(
label
[
i
]
)
print
(
simple_network.run
(
data
[
i
]
)
)
[
0
1
]
[
[
0.06857234
]
[
0.93333256
]
]
[
0
1
]
[
[
0.0694426
]
[
0.93263667
]
]
[
0
1
]
[
[
0.06890567
]
[
0.93314354
]
]
[
0
1
]
[
[
0.07398586
]
[
0.92826171
]
]
[
1
0
]
[
[
0.91353761
]
[
0.08620027
]
]
[
0
1
]
[
[
0.06598966
]
[
0.93595685
]
]
[
1
0
]
[
[
0.90963169
]
[
0.09022392
]
]
[
1
0
]
[
[
0.9155282
]
[
0.08423438
]
]
[
1
0
]
[
[
0.91531178
]
[
0.08444738
]
]
[
1
0
]
[
[
0.91575254
]
[
0.08401871
]
]
[
1
0
]
[
[
0.91164767
]
[
0.08807266
]
]
[
0
1
]
[
[
0.06818507
]
[
0.93384242
]
]
[
0
1
]
[
[
0.07609557
]
[
0.92620649
]
]
[
0
1
]
[
[
0.06651258
]
[
0.93543384
]
]
[
1
0
]
[
[
0.91411049
]
[
0.08570024
]
]
[
1
0
]
[
[
0.91409934
]
[
0.08567811
]
]
[
0
1
]
[
[
0.06711438
]
[
0.93487441
]
]
[
1
0
]
[
[
0.91517701
]
[
0.08458689
]
]
[
1
0
]
[
[
0.91550873
]
[
0.08427926
]
]
[
1
0
]
[
[
0.91562321
]
[
0.08414424
]
]
[
0
1
]
[
[
0.06613625
]
[
0.93581576
]
]
[
0
1
]
[
[
0.0659944
]
[
0.9359505
]
]
[
0
1
]
[
[
0.07744433
]
[
0.92481335
]
]
[
1
0
]
[
[
0.91498511
]
[
0.08485322
]
]
previou
chapter
:
neural
network
from
scratch
in
python
next
chapter
:
backpropag
in
neural
network
©
2011
-
2018
,
bernd
klein
,
bodenseo
;
design
by
denis
mitchinson
adapt
for
python-course.eu
by
bernd
klein
