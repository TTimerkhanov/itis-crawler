machin
learn
with
python
:
backpropag
in
neural
network
python
machin
learn
tutori
machin
learn
machin
learn
terminologyk-nearest
neighbor
classifierneur
network
from
scratch
in
pythonneur
network
in
python
use
numypybackpropag
in
neural
networksconfus
matrixtrain
and
test
with
mnistdropout
neural
networksneur
network
with
scikitmachin
learn
with
scikit
and
pythonintroduct
naiv
bay
classifierna
bay
classifi
with
scikitintroduct
into
text
classif
use
naiv
bayespython
implement
of
text
classificationdecis
treesregress
treesrandom
forestsboost
algorithmprincip
compon
analysislinear
discrimin
analysisexpect
maxim
and
gaussian
mixtur
modelintroduct
into
tensorflow
quot
``
neural
comput
is
the
studi
of
cellular
network
that
have
a
natur
properti
for
store
experiment
knowledg
.
such
system
bear
a
resembl
to
the
brain
in
the
sens
that
knowledg
is
acquir
through
train
rather
than
program
and
is
retain
due
to
chang
in
node
function
.
the
knowledg
take
the
form
of
stabl
state
or
cycl
of
state
in
the
oper
of
the
et
.
A
central
properti
of
such
net
is
to
recal
these
state
or
cycl
in
respons
to
the
present
of
cue
.
''
(
aleksand
&
morton
,
1989
,
``
neural
comput
architectur
:
the
design
of
brain-lik
machin
.
``
,
p.2
as
cite
in
:
m.a
.
lovel
et
al
.
(
1997
)
develop
in
petrophys
.
p.169
``
cognit
neurosci
is
enter
an
excit
era
in
which
new
technolog
and
idea
are
make
it
possibl
to
studi
the
neural
basi
of
cognit
,
percept
,
memori
and
emot
at
the
level
of
network
of
interact
neuron
,
the
level
at
which
we
believ
mani
of
the
import
oper
of
the
brain
take
place
.
We
know
a
consider
amount
about
how
individu
neuron
work
and
how
two
cell
can
commun
with
each
other
but
the
way
in
which
entir
network
of
hundr
and
thousand
of
neuron
cooper
,
interact
with
each
other
,
and
are
orchestr
to
creat
our
idea
and
concept
is
an
underexplor
area
of
neuroscience.
``
(
john
o'keef
,
from
hi
speech
at
the
nobel
banquet
,
10
decemb
2014
)
thi
websit
is
creat
by
:
bernd
klein
,
python
train
cours
On
site
train
in
europ
,
canada
and
the
US
.
thi
websit
is
free
of
annoy
ad
.
We
want
to
keep
it
like
thi
.
you
can
help
with
your
donat
:
the
need
for
donat
bernd
klein
on
facebook
search
thi
websit
:
classroom
train
cours
thi
websit
contain
a
free
and
extens
onlin
tutori
by
bernd
klein
,
use
materi
from
hi
classroom
python
train
cours
.
If
you
are
interest
in
an
instructor-l
classroom
train
cours
,
you
may
have
a
look
at
the
python
class
by
bernd
klein
at
bodenseo
.
Â©
kabliczech
-
fotolia.com
quot
of
the
day
:
''
I
think
the
special
thing
about
python
is
that
it
's
a
writer
'
commun
.
the
writer
are
in
charg
.
the
writer
decid
what
the
materi
is
.
''
(
eric
idl
)
If
you
have
the
choic
work
with
python
2
or
python
3
,
we
recomend
to
switch
to
python
3
!
you
can
read
our
python
tutori
to
see
what
the
differ
are
.
data
protect
declar
data
protect
declar
previou
chapter
:
neural
network
in
python
use
numypi
next
chapter
:
confus
matrix
backpropag
in
neural
network
introduct
We
have
alreadi
written
neural
network
in
python
in
the
previou
chapter
of
our
tutori
.
We
could
train
these
network
,
but
we
did
n't
explain
the
mechan
use
for
train
.
We
use
backpropag
without
say
so
.
backpropag
is
a
commonli
use
method
for
train
artifici
neural
network
,
especi
deep
neural
network
.
backpropag
is
need
to
calcul
the
gradient
,
which
we
need
to
adapt
the
weight
of
the
weight
matric
.
the
weight
of
the
neuron
(
node
)
of
our
network
are
adjust
by
calcul
the
gradient
of
the
loss
function
.
for
thi
purpos
a
gradient
descent
optim
algorithm
is
use
.
It
is
also
call
backward
propag
of
error
.
quit
often
peopl
are
frighten
away
by
the
mathemat
use
in
it
.
We
tri
to
explain
it
in
simpl
term
.
explain
gradient
descent
start
in
mani
articl
or
tutori
with
mountain
.
imagin
you
are
put
on
a
mountain
,
not
necessarili
the
top
,
by
a
helicopt
at
night
or
heavi
fog
.
let
's
further
imagin
that
thi
mountain
is
on
an
island
and
you
want
to
reach
sea
level
.
you
have
to
go
down
,
but
you
hardli
see
anyth
,
mayb
just
a
few
metr
.
your
task
is
to
find
your
way
down
,
but
you
can
not
see
the
path
.
you
can
use
the
method
of
gradient
descent
.
thi
mean
that
you
are
examin
the
steep
at
your
current
posit
.
you
will
proceed
in
the
direct
with
the
steepest
descent
.
you
take
onli
a
few
step
and
then
you
stop
again
to
reorient
yourself
.
thi
mean
you
are
appli
again
the
previous
describ
procedur
,
i.e
.
you
are
look
for
the
steepest
descend
.
thi
procedur
is
depict
in
the
follow
diagram
in
a
two-dimension
space
.
go
on
like
thi
you
will
arriv
at
a
posit
,
where
there
is
no
further
descend
.
each
direct
goe
upward
.
you
may
have
reach
the
deepest
level
-
the
global
minimum
-
,
but
you
might
as
well
be
stuck
in
a
basin
.
If
you
start
at
the
posit
on
the
right
side
of
our
imag
,
everyth
work
out
fine
,
but
from
the
leftsid
,
you
will
be
stuck
in
a
local
minimum
.
If
you
imagin
now
,
-
not
veri
realist
-
you
are
drop
mani
time
at
random
place
on
thi
island
,
you
will
find
way
downward
to
sea
level
.
thi
is
what
we
actual
do
,
when
we
train
a
neural
network
.
backpropag
in
detailnow
,
we
have
to
go
into
the
detail
,
i.e
.
the
mathemat
.
We
will
start
with
the
simpler
case
.
We
look
at
a
linear
network
.
linear
neural
network
are
network
where
the
output
signal
is
creat
by
sum
up
all
the
weight
input
signal
.
No
activ
function
will
be
appli
to
thi
sum
,
which
is
the
reason
for
the
linear
.
the
will
use
the
follow
simpl
network
.
We
have
label
,
i.e
.
target
or
desir
valu
$
t_i
$
for
each
output
valu
$
o_i
$
.
principi
,
the
error
is
the
differ
between
the
target
and
the
actual
output
:
$
$
e_i
=
t_i
-
o_i
$
$
We
will
later
use
a
squar
error
function
,
becaus
it
ha
better
characterist
for
the
algorithm
:
$
$
e_i
=
\frac
{
1
}
{
2
}
(
t_i
-
o_i
)
^
2
$
$
We
will
have
a
look
at
the
output
valu
$
o_1
$
,
which
is
depend
on
the
valu
$
w_
{
11
}
$
,
$
w_
{
21
}
$
,
$
w_
{
31
}
$
and
$
w_
{
41
}
$
.
let
's
assum
the
calcul
valu
(
$
o_1
$
)
is
0.92
and
the
desir
valu
(
$
t_1
$
)
is
1
.
In
thi
case
the
error
is
$
$
e_1
=
t_1
-
o_1
=
1
-
0.92
=
0.08
$
$
depend
on
thi
error
,
we
have
to
chang
the
weight
from
the
incom
valu
accordingli
.
We
have
four
weight
,
so
we
could
spread
the
error
evenli
.
yet
,
it
make
more
sens
to
to
do
it
proport
,
accord
to
the
weight
valu
.
thi
mean
that
we
can
calcul
the
fraction
of
the
error
$
e_1
$
in
$
w_
{
11
}
$
as
:
$
$
e_1
\cdot
\frac
{
w_
{
11
}
}
{
\sum_
{
i=1
}
^
{
4
}
w_
{
i1
}
}
$
$
thi
mean
in
our
exampl
:
$
$
0.08
\cdot
\frac
{
0.6
}
{
0.6
+
0.4
+
0.1
+
0.2
}
=
0.037
$
$
the
total
error
in
our
weight
matrix
between
the
hidden
and
the
output
layer
-
we
call
it
in
our
previou
chapter
'who
'
-
look
like
thi
$
$
e_
{
who
}
=
\begin
{
bmatrix
}
\frac
{
w_
{
11
}
}
{
\sum_
{
i=1
}
^
{
4
}
w_
{
i1
}
}
&
\frac
{
w_
{
12
}
}
{
\sum_
{
i=1
}
^
{
4
}
w_
{
i1
}
}
\\
\frac
{
w_
{
21
}
}
{
\sum_
{
i=1
}
^
{
4
}
w_
{
i1
}
}
&
\frac
{
w_
{
22
}
}
{
\sum_
{
i=1
}
^
{
4
}
w_
{
i1
}
}
\\
\frac
{
w_
{
31
}
}
{
\sum_
{
i=1
}
^
{
4
}
w_
{
i1
}
}
&
\frac
{
w_
{
32
}
}
{
\sum_
{
i=1
}
^
{
4
}
w_
{
i1
}
}
\\
\frac
{
w_
{
41
}
}
{
\sum_
{
i=1
}
^
{
4
}
w_
{
i1
}
}
&
\frac
{
w_
{
22
}
}
{
\sum_
{
i=1
}
^
{
4
}
w_
{
i1
}
}
\\
\end
{
bmatrix
}
\cdot
\begin
{
bmatrix
}
e_1
\\
e_2
\end
{
bmatrix
}
$
$
you
can
see
that
the
denomin
in
the
left
matrix
is
alway
the
same
.
It
is
a
scale
factor
.
We
can
drop
it
so
that
the
calcul
get
a
lot
simpler
:
$
$
e_
{
who
}
=
\begin
{
bmatrix
}
w_
{
11
}
&
w_
{
12
}
\\
w_
{
21
}
&
w_
{
22
}
\\
w_
{
31
}
&
w_
{
32
}
\\
w_
{
41
}
&
w_
{
22
}
\\
\end
{
bmatrix
}
\cdot
\begin
{
bmatrix
}
e_1
\\
e_2
\end
{
bmatrix
}
$
$
If
you
compar
the
matrix
on
the
right
side
with
the
'who
'
matrix
of
our
chapter
neuron
network
use
python
and
numpi
,
you
will
notic
that
it
is
the
transpos
of
'who
'
.
$
$
e_
{
who
}
=
who.t
\cdot
e
$
$
So
,
thi
ha
been
the
easi
part
for
linear
neural
network
.
We
want
to
calcul
the
error
in
a
network
with
an
activ
function
,
i.e
.
a
non-linear
network
.
the
deriv
of
the
error
function
describ
the
slope
.
As
we
mention
in
the
begin
of
the
thi
chapter
,
we
want
to
descend
.
the
deriv
describ
how
the
error
$
E
$
chang
as
the
weight
$
w_
{
ij
}
$
chang
:
$
$
\frac
{
\partial
E
}
{
\partial
w_
{
ij
}
}
$
$
the
error
function
E
over
all
the
output
node
$
o_j
$
(
$
j
=
1
,
...
n
$
)
where
$
n
$
is
the
number
of
output
node
is
:
$
$
E
=
\sum_
{
j=1
}
^
{
n
}
\frac
{
1
}
{
2
}
(
t_j
-
o_j
)
^2
$
$
now
,
we
can
insert
thi
in
our
deriv
:
$
$
\frac
{
\partial
E
}
{
\partial
w_
{
ij
}
}
=
\frac
{
\partial
}
{
\partial
w_
{
ij
}
}
\frac
{
1
}
{
2
}
\sum_
{
j=1
}
^
{
n
}
(
t_j
-
o_j
)
^2
$
$
If
you
have
a
look
at
our
exampl
network
,
you
will
see
that
an
output
node
$
o_k
$
onli
depend
on
the
input
signal
creat
with
the
weight
$
w_
{
ij
}
$
with
$
i
=
1
,
\ldot
m
$
and
$
m
$
the
number
of
hidden
node
.
thi
mean
that
we
can
calcul
the
error
for
everi
output
node
independ
of
each
other
and
we
get
rid
of
the
sum
.
thi
is
the
error
for
a
node
j
for
exampl
:
$
$
\frac
{
\partial
E
}
{
\partial
w_
{
ij
}
}
=
\frac
{
\partial
}
{
\partial
w_
{
ij
}
}
\frac
{
1
}
{
2
}
(
t_j
-
o_j
)
^2
$
$
the
valu
$
t_j
$
is
a
constant
,
becaus
it
is
not
depend
on
ani
input
signal
or
weight
.
We
can
appli
the
chain
rule
for
the
differenti
of
the
previou
term
to
simplifi
thing
:
$
$
\frac
{
\partial
E
}
{
\partial
w_
{
ij
}
}
=
\frac
{
\partial
E
}
{
\partial
o_
{
j
}
}
\cdot
\frac
{
\partial
o_j
}
{
\partial
w_
{
ij
}
}
$
$
In
the
previou
chapter
of
our
tutori
,
we
use
the
sigmoid
function
as
the
activ
function
:
$
$
\sigma
(
x
)
=
\frac
{
1
}
{
1+e^
{
-x
}
}
$
$
the
output
node
$
o_j
$
is
calcul
by
appli
the
sigmoid
function
to
the
sum
of
the
weight
input
signal
.
thi
mean
that
we
can
further
simplifi
our
differenti
term
by
replac
$
o_j
$
by
thi
function
:
$
$
\frac
{
\partial
E
}
{
\partial
w_
{
ij
}
}
=
(
t_j
-
o_j
)
\cdot
\frac
{
\partial
}
{
\partial
w_
{
ij
}
}
\sigma
(
\sum_
{
i=1
}
^
{
m
}
w_
{
ij
}
h_i
)
$
$
where
$
m
$
is
the
number
of
hidden
node
.
the
sigmoid
function
is
easi
to
differenti
:
$
$
\frac
{
\partial
\sigma
(
x
)
}
{
\partial
x
}
=
\sigma
(
x
)
\cdot
(
1
-
\sigma
(
x
)
)
$
$
the
complet
differenti
look
like
thi
now
:
$
$
\frac
{
\partial
E
}
{
\partial
w_
{
ij
}
}
=
(
t_j
-
o_j
)
\cdot
\sigma
(
\sum_
{
i=1
}
^
{
m
}
w_
{
ij
}
h_i
)
\cdot
(
1
-
\sigma
(
\sum_
{
i=1
}
^
{
m
}
w_
{
ij
}
h_i
)
)
\frac
{
\partial
}
{
\partial
w_
{
ij
}
}
\sum_
{
i=1
}
^
{
m
}
w_
{
ij
}
h_i
$
$
the
last
part
ha
to
be
differenti
with
respect
to
$
w_
{
ij
}
$
.
thi
mean
that
all
the
summand
disappear
and
onli
$
o_j
$
is
left
:
$
$
\frac
{
\partial
E
}
{
\partial
w_
{
ij
}
}
=
(
t_j
-
o_j
)
\cdot
\sigma
(
\sum_
{
i=1
}
^
{
m
}
w_
{
ij
}
h_i
)
\cdot
(
1
-
\sigma
(
\sum_
{
i=1
}
^
{
m
}
w_
{
ij
}
h_i
)
)
\cdot
o_j
$
$
thi
is
what
we
had
use
in
our
method
'train
'
of
our
neuralnetwork
class
in
the
previou
chapter
.
previou
chapter
:
neural
network
in
python
use
numypi
next
chapter
:
confus
matrix
Â©
2011
-
2018
,
bernd
klein
,
bodenseo
;
design
by
denis
mitchinson
adapt
for
python-course.eu
by
bernd
klein
