machin
learn
with
python
:
neural
network
from
scratch
in
python
python
machin
learn
tutori
machin
learn
machin
learn
terminologyk-nearest
neighbor
classifierneur
network
from
scratch
in
pythonneur
network
in
python
use
numypybackpropag
in
neural
networksconfus
matrixtrain
and
test
with
mnistdropout
neural
networksneur
network
with
scikitmachin
learn
with
scikit
and
pythonintroduct
naiv
bay
classifierna
bay
classifi
with
scikitintroduct
into
text
classif
use
naiv
bayespython
implement
of
text
classificationdecis
treesregress
treesrandom
forestsboost
algorithmprincip
compon
analysislinear
discrimin
analysisexpect
maxim
and
gaussian
mixtur
modelintroduct
into
tensorflow
quot
``
neural
comput
is
the
studi
of
cellular
network
that
have
a
natur
properti
for
store
experiment
knowledg
.
such
system
bear
a
resembl
to
the
brain
in
the
sens
that
knowledg
is
acquir
through
train
rather
than
program
and
is
retain
due
to
chang
in
node
function
.
the
knowledg
take
the
form
of
stabl
state
or
cycl
of
state
in
the
oper
of
the
et
.
A
central
properti
of
such
net
is
to
recal
these
state
or
cycl
in
respons
to
the
present
of
cue
.
''
(
aleksand
&
morton
,
1989
,
``
neural
comput
architectur
:
the
design
of
brain-lik
machin
.
``
,
p.2
as
cite
in
:
m.a
.
lovel
et
al
.
(
1997
)
develop
in
petrophys
.
p.169
``
cognit
neurosci
is
enter
an
excit
era
in
which
new
technolog
and
idea
are
make
it
possibl
to
studi
the
neural
basi
of
cognit
,
percept
,
memori
and
emot
at
the
level
of
network
of
interact
neuron
,
the
level
at
which
we
believ
mani
of
the
import
oper
of
the
brain
take
place
.
We
know
a
consider
amount
about
how
individu
neuron
work
and
how
two
cell
can
commun
with
each
other
but
the
way
in
which
entir
network
of
hundr
and
thousand
of
neuron
cooper
,
interact
with
each
other
,
and
are
orchestr
to
creat
our
idea
and
concept
is
an
underexplor
area
of
neuroscience.
``
(
john
o'keef
,
from
hi
speech
at
the
nobel
banquet
,
10
decemb
2014
)
thi
websit
is
creat
by
:
bernd
klein
,
python
train
cours
On
site
train
in
europ
,
canada
and
the
US
.
thi
websit
is
free
of
annoy
ad
.
We
want
to
keep
it
like
thi
.
you
can
help
with
your
donat
:
the
need
for
donat
bernd
klein
on
facebook
search
thi
websit
:
classroom
train
cours
thi
websit
contain
a
free
and
extens
onlin
tutori
by
bernd
klein
,
use
materi
from
hi
classroom
python
train
cours
.
If
you
are
interest
in
an
instructor-l
classroom
train
cours
,
you
may
have
a
look
at
the
python
class
by
bernd
klein
at
bodenseo
.
©
kabliczech
-
fotolia.com
quot
of
the
day
:
man
is
the
best
comput
we
can
put
aboard
a
spacecraft
...
and
the
onli
one
that
can
be
mass
produc
with
unskil
labor
.
(
wernher
von
braun
)
If
you
have
the
choic
work
with
python
2
or
python
3
,
we
recomend
to
switch
to
python
3
!
you
can
read
our
python
tutori
to
see
what
the
differ
are
.
data
protect
declar
data
protect
declar
previou
chapter
:
k-nearest
neighbor
classifi
next
chapter
:
neural
network
in
python
use
numypi
neural
networksintroduct
when
we
say
``
neural
network
''
,
we
mean
artifici
neural
network
(
ann
)
.
the
idea
of
ann
is
base
on
biolog
neural
network
like
the
brain
.
the
basic
structur
of
a
neural
network
is
the
neuron
.
A
neuron
in
biolog
consist
of
three
major
part
:
the
soma
(
cell
bodi
)
,
the
dendrit
,
and
the
axon
.
the
dendrit
branch
of
from
the
soma
in
a
tree-lik
way
and
get
thinner
with
everi
branch
.
they
receiv
signal
(
impuls
)
from
other
neuron
at
synaps
.
the
axon
-
there
is
alway
onli
one
-
also
leav
the
soma
and
usual
tend
to
extend
for
longer
distanc
than
the
dentrit
.
the
axon
is
use
for
send
the
output
of
the
neuron
to
other
neuron
or
better
to
the
synapsi
of
other
neuron
.
the
follow
imag
by
quasar
jarosz
,
courtesi
of
wikipedia
,
illustr
thi
:
even
though
the
abov
imag
is
alreadi
an
abstract
for
a
biologist
,
we
can
further
abstract
it
:
A
perceptron
of
artifici
neural
network
is
simul
a
biolog
neuron
.
It
is
amazingli
simpl
,
what
is
go
on
insid
the
bodi
of
a
perceptron
or
neuron
.
the
input
signal
get
multipli
by
weight
valu
,
i.e
.
each
input
ha
it
correspond
weight
.
thi
way
the
input
can
be
adjust
individu
for
everi
$
x_i
$
.
We
can
see
all
the
input
as
an
input
vector
and
the
correspond
weight
as
the
weight
vector
.
when
a
signal
come
in
,
it
get
multipli
by
a
weight
valu
that
is
assign
to
thi
particular
input
.
that
is
,
if
a
neuron
ha
three
input
,
then
it
ha
three
weight
that
can
be
adjust
individu
.
the
weight
usual
get
adjust
dure
the
learn
phase
.
after
thi
the
modifi
input
signal
are
sum
up
.
It
is
also
possibl
to
add
addit
a
so-cal
bia
b
to
thi
sum
.
the
bia
is
a
valu
which
can
also
be
adjust
dure
the
learn
phase
.
final
,
the
actual
output
ha
to
be
determin
.
for
thi
purpos
an
activ
or
step
function
Φ
is
appli
to
weight
sum
of
the
input
valu
.
the
simplest
form
of
an
activ
function
is
a
binari
function
.
If
the
result
of
the
summat
is
greater
than
some
threshold
s
,
the
result
of
$
\phi
$
will
be
1
,
otherwis
0
.
$
$
\phi
(
x
)
=
\left\
{
\begin
{
array
}
{
rl
}
1
&
\mbox
{
wx
+
b
>
s
}
\\
0
&
\mbox
{
otherwis
}
\end
{
array
}
\right
.
$
$
A
simpl
neural
networkth
follow
imag
show
the
gener
build
principl
of
a
simpl
artifici
neural
network
:
We
will
write
a
veri
simpl
neural
network
implement
the
logic
``
and
''
and
``
Or
''
function
.
let
's
start
with
the
``
and
''
function
.
It
is
defin
for
two
input
:
input1
input2
output
0
0
0
0
1
0
1
0
0
1
1
1
import
numpi
as
np
class
perceptron
:
def
__init__
(
self
,
input_length
,
weights=non
)
:
if
weight
is
none
:
self.weight
=
np.one
(
input_length
)
*
0.5
els
:
self.weight
=
weight
@
staticmethod
def
unit_step_funct
(
x
)
:
if
x
>
0.5
:
return
1
return
0
def
__call__
(
self
,
in_data
)
:
weighted_input
=
self.weight
*
in_data
weighted_sum
=
weighted_input.sum
(
)
return
perceptron.unit_step_funct
(
weighted_sum
)
p
=
perceptron
(
2
,
np.array
(
[
0.5
,
0.5
]
)
)
for
x
in
[
np.array
(
[
0
,
0
]
)
,
np.array
(
[
0
,
1
]
)
,
np.array
(
[
1
,
0
]
)
,
np.array
(
[
1
,
1
]
)
]
:
y
=
p
(
np.array
(
x
)
)
print
(
x
,
y
)
[
0
0
]
0
[
0
1
]
0
[
1
0
]
0
[
1
1
]
1
line
separationin
the
follow
program
,
we
train
a
neural
network
to
classifi
two
cluster
in
a
2-dimension
space
.
We
show
thi
in
the
follow
diagram
with
the
two
class
class1
and
class2
.
We
will
creat
those
point
randomli
with
the
help
of
a
line
,
the
point
of
class2
will
be
abov
the
line
and
the
point
of
class1
will
be
below
the
line
.
We
will
see
that
the
neural
network
will
find
a
line
that
separ
the
two
class
.
thi
line
should
not
be
mistaken
for
the
line
,
which
we
use
to
creat
the
point
.
thi
line
is
call
a
decis
boundari
.
import
numpi
as
np
from
collect
import
counter
class
perceptron
:
def
__init__
(
self
,
input_length
,
weights=non
)
:
if
weights==non
:
self.weight
=
np.random.random
(
(
input_length
)
)
*
2
-
1
self.learning_r
=
0.1
@
staticmethod
def
unit_step_funct
(
x
)
:
if
x
<
0
:
return
0
return
1
def
__call__
(
self
,
in_data
)
:
weighted_input
=
self.weight
*
in_data
weighted_sum
=
weighted_input.sum
(
)
return
perceptron.unit_step_funct
(
weighted_sum
)
def
adjust
(
self
,
target_result
,
calculated_result
,
in_data
)
:
error
=
target_result
-
calculated_result
for
i
in
rang
(
len
(
in_data
)
)
:
correct
=
error
*
in_data
[
i
]
*self.learning_r
self.weight
[
i
]
+=
correct
def
above_lin
(
point
,
line_func
)
:
x
,
y
=
point
if
y
>
line_func
(
x
)
:
return
1
els
:
return
0
point
=
np.random.randint
(
1
,
100
,
(
100
,
2
)
)
p
=
perceptron
(
2
)
def
lin1
(
x
)
:
return
x
+
4
for
point
in
point
:
p.adjust
(
above_lin
(
point
,
lin1
)
,
p
(
point
)
,
point
)
evalu
=
counter
(
)
for
point
in
point
:
if
p
(
point
)
==
above_lin
(
point
,
lin1
)
:
evalu
[
``
correct
''
]
+=
1
els
:
evalu
[
``
wrong
''
]
+=
1
print
(
evaluation.most_common
(
)
)
[
(
'correct
'
,
100
)
]
the
decis
boundari
of
our
previou
network
can
be
calcul
by
look
at
the
follow
condit
$
$
x_1
w_1
+
x_2w_2
=
0
$
$
We
can
chang
the
equat
into
$
$
x_2
=
-\frac
{
w_1
}
{
w_2
}
x_1
$
$
when
we
look
at
the
gener
form
of
a
straight
line
$
y
=
mx
+
b
$
,
we
can
easili
see
that
our
equat
correspond
to
the
definit
of
a
line
and
the
slope
(
aka
gradient
)
$
m
$
is
$
-\frac
{
w_1
}
{
w_2
}
$
and
$
b
$
is
equal
to
0
.
singl
layer
with
biasa
the
constant
term
$
b
$
determin
the
point
at
which
a
line
cross
the
y-axi
,
i.e
.
the
y-intercept
,
we
can
see
that
our
network
can
onli
calcul
line
which
pass
through
the
origin
,
i.e
.
the
point
(
0
,
0
)
.
We
will
need
a
bia
to
get
other
line
as
well
,
i.e
.
line
which
do
n't
go
through
the
origin
.
A
neural
network
with
bia
node
can
look
like
thi
:
now
,
the
linear
equat
for
a
perceptron
contain
a
bia
:
$
$
b
+
\sum_
{
i=1
}
^
{
n
}
x_i
\cdot
w_i
=
0
$
$
We
add
now
some
code
to
print
the
point
and
the
divid
line
accord
to
the
previou
equat
:
#
the
follow
line
is
onli
need
,
#
if
you
use
``
ipython
notebook
''
:
%
matplotlib
inlin
from
matplotlib
import
pyplot
as
plt
cl
=
[
[
]
,
[
]
]
for
point
in
point
:
cl
[
above_lin
(
point
,
lin1
)
]
.append
(
tupl
(
point
)
)
colour
=
(
``
r
''
,
``
b
''
)
for
i
in
rang
(
2
)
:
X
,
Y
=
zip
(
*cl
[
i
]
)
plt.scatter
(
X
,
Y
,
c=colour
[
i
]
)
X
=
np.arang
(
-3
,
120
)
m
=
-p.weight
[
0
]
/
p.weight
[
1
]
print
(
m
)
plt.plot
(
X
,
m*x
,
label=
''
ann
line
''
)
plt.plot
(
X
,
lin1
(
X
)
,
label=
''
line1
''
)
plt.legend
(
)
plt.show
(
)
1.11082111934
We
creat
a
new
dataset
for
our
next
experi
:
from
matplotlib
import
pyplot
as
plt
class1
=
[
(
3
,
4
)
,
(
4.2
,
5.3
)
,
(
4
,
3
)
,
(
6
,
5
)
,
(
4
,
6
)
,
(
3.7
,
5.8
)
,
(
3.2
,
4.6
)
,
(
5.2
,
5.9
)
,
(
5
,
4
)
,
(
7
,
4
)
,
(
3
,
7
)
,
(
4.3
,
4.3
)
]
class2
=
[
(
-3
,
-4
)
,
(
-2
,
-3.5
)
,
(
-1
,
-6
)
,
(
-3
,
-4.3
)
,
(
-4
,
-5.6
)
,
(
-3.2
,
-4.8
)
,
(
-2.3
,
-4.3
)
,
(
-2.7
,
-2.6
)
,
(
-1.5
,
-3.6
)
,
(
-3.6
,
-5.6
)
,
(
-4.5
,
-4.6
)
,
(
-3.7
,
-5.8
)
]
X
,
Y
=
zip
(
*class1
)
plt.scatter
(
X
,
Y
,
c=
''
r
''
)
X
,
Y
=
zip
(
*class2
)
plt.scatter
(
X
,
Y
,
c=
''
b
''
)
plt.show
(
)
from
itertool
import
chain
p
=
perceptron
(
2
)
def
lin1
(
x
)
:
return
x
+
4
for
point
in
class1
:
p.adjust
(
1
,
p
(
point
)
,
point
)
for
point
in
class2
:
p.adjust
(
0
,
p
(
point
)
,
point
)
evalu
=
counter
(
)
for
point
in
chain
(
class1
,
class2
)
:
if
p
(
point
)
==
1
:
evalu
[
``
correct
''
]
+=
1
els
:
evalu
[
``
wrong
''
]
+=
1
testpoint
=
[
(
3.9
,
6.9
)
,
(
-2.9
,
-5.9
)
]
for
point
in
testpoint
:
print
(
p
(
point
)
)
print
(
evaluation.most_common
(
)
)
1
0
[
(
'correct
'
,
12
)
,
(
'wrong
'
,
12
)
]
from
matplotlib
import
pyplot
as
plt
X
,
Y
=
zip
(
*class1
)
plt.scatter
(
X
,
Y
,
c=
''
r
''
)
X
,
Y
=
zip
(
*class2
)
plt.scatter
(
X
,
Y
,
c=
''
b
''
)
x
=
np.arang
(
-7
,
10
)
y
=
5*x
+
10
m
=
-p.weight
[
0
]
/
p.weight
[
1
]
plt.plot
(
x
,
m*x
)
plt.show
(
)
from
matplotlib
import
pyplot
as
plt
class1
=
[
(
3
,
4
,
3
)
,
(
4.2
,
5.3
,
2.5
)
,
(
4
,
3
,
3.8
)
,
(
6
,
5
,
2.7
)
,
(
4
,
6
,
2.9
)
,
(
3.7
,
5.8
,
4.2
)
,
(
3.2
,
4.6
,
1.9
)
,
(
5.2
,
5.9
,
2.7
)
,
(
5
,
4
,
3.5
)
,
(
7
,
4
,
2.7
)
,
(
3
,
7
,
3.1
)
,
(
4.3
,
4.3
,
3.8
)
]
class2
=
[
(
-3
,
-4
,
7.6
)
,
(
-2
,
-3.5
,
6.9
)
,
(
-1
,
-6
,
8.6
)
,
(
-3
,
-4.3
,
7.4
)
,
(
-4
,
-5.6
,
7.9
)
,
(
-3.2
,
-4.8
,
5.3
)
,
(
-2.3
,
-4.3
,
8.1
)
,
(
-2.7
,
-2.6
,
7.3
)
,
(
-1.5
,
-3.6
,
7.8
)
,
(
-3.6
,
-5.6
,
6.8
)
,
(
-4.5
,
-4.6
,
8.3
)
,
(
-3.7
,
-5.8
,
8.7
)
]
X
,
Y
,
Z
=
zip
(
*class1
)
plt.scatter
(
X
,
Y
,
Z
,
c=
''
r
''
)
X
,
Y
,
Z
=
zip
(
*class2
)
plt.scatter
(
X
,
Y
,
Z
,
c=
''
b
''
)
plt.show
(
)
linearli
separ
and
insepar
neural
networksif
two
data
cluster
(
class
)
can
be
separ
by
a
decis
boundari
in
the
form
of
a
linear
equat
$
$
\sum_
{
i=1
}
^
{
n
}
x_i
\cdot
w_i
=
0
$
$
they
are
call
linearli
separ
.
otherwis
,
i.e
.
if
such
a
decis
boundari
doe
not
exist
,
the
two
class
are
call
linearli
insepar
.
In
thi
case
,
we
can
not
use
a
simpl
neural
network
.
In
the
follow
section
,
we
will
introduc
the
xor
problem
for
neural
network
.
It
is
the
simplest
exampl
of
a
non
linearli
separ
neural
network
.
I
can
be
solv
with
an
addit
layer
of
neuron
,
which
is
call
a
hidden
layer
.
the
xor
problem
for
neural
networksth
xor
(
exclus
or
)
function
is
defin
by
the
follow
truth
tabl
:
input1
input2
xor
output
0
0
0
0
1
1
1
0
1
1
1
0
thi
problem
ca
n't
be
solv
with
a
simpl
neural
network
.
We
need
to
introduc
a
new
type
of
neural
network
,
a
network
with
so-cal
hidden
layer
.
A
hidden
layer
allow
the
network
to
reorgan
or
rearrang
the
input
data
.
We
will
need
onli
one
hidden
layer
with
two
neuron
.
one
work
like
an
and
gate
and
the
other
one
like
an
OR
gate
.
the
output
will
``
fire
''
,
when
the
OR
gate
fire
and
the
and
gate
doe
n't
.
ann
with
hidden
layer
:
the
task
is
to
find
a
line
which
separ
the
orang
point
from
the
blue
point
.
but
they
can
be
separ
by
two
line
,
e.g
.
L1
and
L2
in
the
follow
diagram
:
To
solv
thi
problem
,
we
need
a
network
of
the
follow
kind
,
i.e
with
a
hidden
layer
N1
and
N2
the
neuron
N1
will
determin
one
line
,
e.g
.
L1
and
the
neuron
N2
will
determin
the
other
line
L2
.
N3
will
final
solv
our
problem
:
neural
network
with
bia
valuesw
will
come
back
now
to
our
initi
exampl
with
the
random
point
abov
and
below
a
line
.
We
will
rewrit
the
code
use
a
bia
valu
.
first
we
will
creat
two
class
with
random
point
,
which
are
not
separ
by
a
line
cross
the
origin
.
We
will
add
a
bia
b
to
our
neural
network
.
thi
lead
us
to
the
follow
condit
$
$
x_1
w_1
+
x_2w_2
+
b
w_3=
0
$
$
We
can
chang
the
equat
into
$
$
x_2
=
-\frac
{
w_1
}
{
w_2
}
x_1
-\frac
{
w_3
}
{
w_2
}
b
$
$
import
numpi
as
np
from
matplotlib
import
pyplot
as
plt
npoint
=
50
X
,
Y
=
[
]
,
[
]
#
class
0
x.append
(
np.random.uniform
(
low=-2.5
,
high=2.3
,
size=
(
npoint
,
)
)
)
y.append
(
np.random.uniform
(
low=-1.7
,
high=2.8
,
size=
(
npoint
,
)
)
)
#
class
1
x.append
(
np.random.uniform
(
low=-7.2
,
high=-4.4
,
size=
(
npoint
,
)
)
)
y.append
(
np.random.uniform
(
low=3
,
high=6.5
,
size=
(
npoint
,
)
)
)
learnset
=
[
]
for
i
in
rang
(
2
)
:
#
ad
point
of
class
i
to
learnset
point
=
zip
(
X
[
i
]
,
Y
[
i
]
)
for
p
in
point
:
learnset.append
(
(
p
,
i
)
)
colour
=
[
``
b
''
,
``
r
''
]
for
i
in
rang
(
2
)
:
plt.scatter
(
X
[
i
]
,
Y
[
i
]
,
c=colour
[
i
]
)
import
numpi
as
np
from
collect
import
counter
class
perceptron
:
def
__init__
(
self
,
input_length
,
weights=non
)
:
if
weights==non
:
self.weight
=
np.random.random
(
(
input_length
)
)
*
2
-
1
self.learning_r
=
0.1
@
staticmethod
def
unit_step_funct
(
x
)
:
if
x
<
0
:
return
0
return
1
def
__call__
(
self
,
in_data
)
:
weighted_input
=
self.weight
*
in_data
weighted_sum
=
weighted_input.sum
(
)
return
perceptron.unit_step_funct
(
weighted_sum
)
def
adjust
(
self
,
target_result
,
calculated_result
,
in_data
)
:
error
=
target_result
-
calculated_result
for
i
in
rang
(
len
(
in_data
)
)
:
correct
=
error
*
in_data
[
i
]
*self.learning_r
self.weight
[
i
]
+=
correct
p
=
perceptron
(
2
)
for
point
,
label
in
learnset
:
p.adjust
(
label
,
p
(
point
)
,
point
)
evalu
=
counter
(
)
for
point
,
label
in
learnset
:
if
p
(
point
)
==
label
:
evalu
[
``
correct
''
]
+=
1
els
:
evalu
[
``
wrong
''
]
+=
1
print
(
evaluation.most_common
(
)
)
colour
=
[
``
b
''
,
``
r
''
]
for
i
in
rang
(
2
)
:
plt.scatter
(
X
[
i
]
,
Y
[
i
]
,
c=colour
[
i
]
)
XR
=
np.arang
(
-8
,
4
)
m
=
-p.weight
[
0
]
/
p.weight
[
1
]
print
(
m
)
plt.plot
(
XR
,
m*xr
,
label=
''
decis
boundari
''
)
plt.legend
(
)
plt.show
(
)
[
(
'correct
'
,
77
)
,
(
'wrong
'
,
23
)
]
3.10186712936
It
is
not
possibl
to
find
a
solut
with
one
neuron
and
without
a
bia
node
.
the
reason
is
that
the
class
of
the
blue
data
point
spread
around
the
origin
.
without
bia
node
we
get
onli
line
go
through
the
origin
as
we
have
mention
earlier
.
It
is
easi
to
see
that
no
line
go
through
the
origin
can
separ
the
blue
from
the
red
data
.
the
follow
class
use
bia
node
and
solv
thi
problem
:
import
numpi
as
np
from
collect
import
counter
class
perceptron
:
def
__init__
(
self
,
input_length
,
weights=non
)
:
if
weights==non
:
#
input_length
+
1
becaus
bia
need
a
weight
as
well
self.weight
=
np.random.random
(
(
input_length
+
1
)
)
*
2
-
1
self.learning_r
=
0.05
self.bia
=
1
@
staticmethod
def
sigmoid_funct
(
x
)
:
re
=
1
/
(
1
+
np.power
(
np.e
,
-x
)
)
return
0
if
re
<
0.5
els
1
def
__call__
(
self
,
in_data
)
:
weighted_input
=
self.weight
[
:
-1
]
*
in_data
weighted_sum
=
weighted_input.sum
(
)
+
self.bia
*self.weight
[
-1
]
return
perceptron.sigmoid_funct
(
weighted_sum
)
def
adjust
(
self
,
target_result
,
calculated_result
,
in_data
)
:
error
=
target_result
-
calculated_result
for
i
in
rang
(
len
(
in_data
)
)
:
correct
=
error
*
in_data
[
i
]
*self.learning_r
#
print
(
``
weight
:
``
,
self.weight
)
#
print
(
target_result
,
calculated_result
,
in_data
,
error
,
correct
)
self.weight
[
i
]
+=
correct
#
correct
the
bia
:
correct
=
error
*
self.bia
*
self.learning_r
self.weight
[
-1
]
+=
correct
p
=
perceptron
(
2
)
for
point
,
label
in
learnset
:
p.adjust
(
label
,
p
(
point
)
,
point
)
evalu
=
counter
(
)
for
point
,
label
in
learnset
:
if
p
(
point
)
==
label
:
evalu
[
``
correct
''
]
+=
1
els
:
evalu
[
``
wrong
''
]
+=
1
print
(
evaluation.most_common
(
)
)
colour
=
[
``
b
''
,
``
r
''
]
for
i
in
rang
(
2
)
:
plt.scatter
(
X
[
i
]
,
Y
[
i
]
,
c=colour
[
i
]
)
XR
=
np.arang
(
-8
,
4
)
m
=
-p.weight
[
0
]
/
p.weight
[
1
]
b
=
-p.weight
[
-1
]
/p.weight
[
1
]
print
(
m
,
b
)
plt.plot
(
XR
,
m*xr
+
b
,
label=
''
decis
boundari
''
)
plt.legend
(
)
plt.show
(
)
[
(
'correct
'
,
90
)
,
(
'wrong
'
,
10
)
]
-5.07932788718
-6.08697420041
previou
chapter
:
k-nearest
neighbor
classifi
next
chapter
:
neural
network
in
python
use
numypi
©
2011
-
2018
,
bernd
klein
,
bodenseo
;
design
by
denis
mitchinson
adapt
for
python-course.eu
by
bernd
klein
