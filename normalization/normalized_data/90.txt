machin
learn
with
python
:
introduct
into
tensorflow
python
machin
learn
tutori
machin
learn
machin
learn
terminologyk-nearest
neighbor
classifierneur
network
from
scratch
in
pythonneur
network
in
python
use
numypybackpropag
in
neural
networksconfus
matrixtrain
and
test
with
mnistdropout
neural
networksneur
network
with
scikitmachin
learn
with
scikit
and
pythonintroduct
naiv
bay
classifierna
bay
classifi
with
scikitintroduct
into
text
classif
use
naiv
bayespython
implement
of
text
classificationdecis
treesregress
treesrandom
forestsboost
algorithmprincip
compon
analysislinear
discrimin
analysisexpect
maxim
and
gaussian
mixtur
modelintroduct
into
tensorflow
what
is
tensorflow
the
offici
tensoflow
websit
tensorflow.org
write
:
``
tensorflow™
is
an
open
sourc
softwar
librari
for
high
perform
numer
comput
.
it
flexibl
architectur
allow
easi
deploy
of
comput
across
a
varieti
of
platform
(
cpu
,
gpu
,
tpu
)
,
and
from
desktop
to
cluster
of
server
to
mobil
and
edg
devic
.
origin
develop
by
research
and
engin
from
the
googl
brain
team
within
googl
’
s
AI
organ
,
it
come
with
strong
support
for
machin
learn
and
deep
learn
and
the
flexibl
numer
comput
core
is
use
across
mani
other
scientif
domain
.
''
thi
websit
is
creat
by
:
python
train
cours
thi
websit
is
free
of
annoy
ad
.
We
want
to
keep
it
like
thi
.
you
can
help
with
your
donat
:
the
need
for
donat
bernd
klein
on
facebook
search
thi
websit
:
classroom
train
cours
thi
websit
contain
a
free
and
extens
onlin
tutori
by
bernd
klein
,
use
materi
from
hi
classroom
python
train
cours
.
If
you
are
interest
in
an
instructor-l
classroom
train
cours
,
you
may
have
a
look
at
the
python
class
by
bernd
klein
at
bodenseo
.
©
kabliczech
-
fotolia.com
quot
of
the
day
:
man
is
the
best
comput
we
can
put
aboard
a
spacecraft
...
and
the
onli
one
that
can
be
mass
produc
with
unskil
labor
.
(
wernher
von
braun
)
If
you
have
the
choic
work
with
python
2
or
python
3
,
we
recomend
to
switch
to
python
3
!
you
can
read
our
python
tutori
to
see
what
the
differ
are
.
data
protect
declar
data
protect
declar
previou
chapter
:
expect
maxim
and
gaussian
mixtur
model
tensorflowtensorflow
is
an
open-sours
softwar
librari
for
machin
learn
across
a
rang
of
task
.
It
is
a
symbol
math
librari
,
and
also
use
as
a
system
for
build
and
train
neural
network
to
detect
and
deciph
pattern
and
correl
,
analog
to
human
learn
and
reason
.
It
is
use
for
both
research
and
product
at
googl
often
replac
it
closed-sourc
predecessor
,
distbelief
.
tensorflow
wa
develop
by
the
googl
brain
team
for
intern
googl
use
.
It
wa
releas
under
the
apach
2.0
open
sourc
licens
on
9
novemb
2015
.
tensorflow
provid
a
python
api
as
well
as
c++
,
haskel
,
java
,
Go
and
rust
api
.
A
tensor
can
be
repres
as
a
multidimension
array
of
number
.
A
tensor
ha
it
rank
and
shape
,
rank
is
it
number
of
dimens
and
shape
is
the
size
of
each
dimens
.
#
a
rank
0
tensor
,
i.e
.
a
scalar
with
shape
(
)
:
42
#
a
rank
1
tensor
,
i.e
.
a
vector
with
shape
(
3
,
)
:
[
1
,
2
,
3
]
#
a
rank
2
tensor
,
i.e
.
a
matrix
with
shape
(
2
,
3
)
:
[
[
1
,
2
,
3
]
,
[
3
,
2
,
1
]
]
#
a
rank
3
tensor
with
shape
(
2
,
2
,
2
)
:
[
[
[
3
,
4
]
,
[
1
,
2
]
]
,
[
[
3
,
5
]
,
[
8
,
9
]
]
]
#
the
previou
python
code
return
the
follow
result
:
[
[
[
3
,
4
]
,
[
1
,
2
]
]
,
[
[
3
,
5
]
,
[
8
,
9
]
]
]
all
data
of
tensorflow
is
repres
as
tensor
.
It
is
the
sole
data
structur
:
tf.float32
,
tf.float64
,
tf.int8
,
tf.int16
,
…
,
tf.int64
,
tf.uint8
,
...
structur
of
tensorflow
program
tensorflow
program
consist
of
two
discret
section
:
A
graph
is
creat
in
the
construct
phase
.
the
comput
graph
is
run
in
the
execut
phase
,
which
is
a
session
.
exampl
import
tensorflow
as
tf
#
comput
graph
:
c1
=
tf.constant
(
0.034
)
c2
=
tf.constant
(
1000.0
)
x
=
tf.multipli
(
c1
,
c1
)
y
=
tf.multipli
(
c1
,
c2
)
final_nod
=
tf.add
(
x
,
y
)
#
run
the
session
:
with
tf.session
(
)
as
sess
:
result
=
sess.run
(
final_nod
)
print
(
result
,
type
(
result
)
)
34.0012
<
class
'numpy.float32
'
>
import
tensorflow
as
tf
#
comput
graph
:
c1
=
tf.constant
(
0.034
,
dtype=tf.float64
)
c2
=
tf.constant
(
1000.0
,
dtype=tf.float64
)
x
=
tf.multipli
(
c1
,
c1
)
y
=
tf.multipli
(
c1
,
c2
)
final_nod
=
tf.add
(
x
,
y
)
#
run
the
session
:
with
tf.session
(
)
as
sess
:
result
=
sess.run
(
final_nod
)
print
(
result
,
type
(
result
)
)
34.001156
<
class
'numpy.float64
'
>
import
tensorflow
as
tf
#
comput
graph
:
c1
=
tf.constant
(
[
3.4
,
9.1
,
-1.2
,
9
]
,
dtype=tf.float64
)
c2
=
tf.constant
(
[
3.4
,
9.1
,
-1.2
,
9
]
,
dtype=tf.float64
)
x
=
tf.multipli
(
c1
,
c1
)
y
=
tf.multipli
(
c1
,
c2
)
final_nod
=
tf.add
(
x
,
y
)
#
run
the
session
:
with
tf.session
(
)
as
sess
:
result
=
sess.run
(
final_nod
)
print
(
result
,
type
(
result
)
)
[
23.12
165.62
2.88
162
.
]
<
class
'numpy.ndarray
'
>
A
comput
graph
is
a
seri
of
tensorflow
oper
arrang
into
a
graph
of
node
.
let
's
build
a
simpl
comput
graph
.
each
node
take
zero
or
more
tensor
as
input
and
produc
a
tensor
as
an
output
.
constant
node
take
no
input
.
print
the
node
doe
not
output
a
numer
valu
.
We
have
defin
a
comput
graph
but
no
numer
evalu
ha
taken
place
!
c1
=
tf.constant
(
[
3.4
,
9.1
,
-1.2
,
9
]
,
dtype=tf.float64
)
c2
=
tf.constant
(
[
3.4
,
9.1
,
-1.2
,
9
]
,
dtype=tf.float64
)
x
=
tf.multipli
(
c1
,
c1
)
y
=
tf.multipli
(
c1
,
c2
)
final_nod
=
tf.add
(
x
,
y
)
print
(
c1
)
print
(
x
)
print
(
final_nod
)
tensor
(
``
const_6:0
''
,
shape=
(
4
,
)
,
dtype=float64
)
tensor
(
``
mul_6:0
''
,
shape=
(
4
,
)
,
dtype=float64
)
tensor
(
``
add_3:0
''
,
shape=
(
4
,
)
,
dtype=float64
)
To
evalu
the
node
,
we
have
to
run
the
comput
graph
within
a
session
.
A
session
encapsul
the
control
and
state
of
the
tensorflow
runtim
.
the
follow
code
creat
a
session
object
and
then
invok
it
run
method
to
run
enough
of
the
comput
graph
to
evalu
node1
and
node2
.
By
run
the
comput
graph
in
a
session
as
follow
.
We
have
to
creat
a
session
object
:
session
=
tf.session
(
)
now
,
we
can
evalu
the
comput
graph
by
start
the
run
method
of
the
session
object
:
result
=
session.run
(
final_nod
)
print
(
result
)
print
(
type
(
result
)
)
[
23.12
165.62
2.88
162
.
]
<
class
'numpy.ndarray
'
>
Of
cours
,
we
will
have
to
close
the
session
,
when
we
are
finish
:
session.clos
(
)
It
is
usual
a
better
idea
to
work
with
the
with
statement
,
as
we
did
in
the
introductori
exampl
!
similar
to
numpyw
will
rewrit
the
follow
program
with
numpi
.
import
tensorflow
as
tf
session
=
tf.session
(
)
x
=
tf.rang
(
12
)
print
(
session.run
(
x
)
)
x2
=
tf.reshap
(
tensor=x
,
shape=
(
3
,
4
)
)
x2
=
tf.reduce_sum
(
x2
,
reduction_indices=
[
0
]
)
re
=
session.run
(
x2
)
print
(
re
)
x3
=
tf.ey
(
5
,
5
)
re
=
session.run
(
x3
)
print
(
re
)
[
0
1
2
3
4
5
6
7
8
9
10
11
]
[
12
15
18
21
]
[
[
1
.
0
.
0
.
0
.
0
.
]
[
0
.
1
.
0
.
0
.
0
.
]
[
0
.
0
.
1
.
0
.
0
.
]
[
0
.
0
.
0
.
1
.
0
.
]
[
0
.
0
.
0
.
0
.
1
.
]
]
now
a
similar
numpi
version
:
import
numpi
as
np
x
=
np.arang
(
12
)
print
(
x
)
x2
=
x.reshap
(
(
3
,
4
)
)
re
=
x2.sum
(
axis=0
)
print
(
re
)
x3
=
np.ey
(
5
,
5
)
print
(
x3
)
[
0
1
2
3
4
5
6
7
8
9
10
11
]
[
12
15
18
21
]
[
[
1
.
0
.
0
.
0
.
0
.
]
[
0
.
1
.
0
.
0
.
0
.
]
[
0
.
0
.
1
.
0
.
0
.
]
[
0
.
0
.
0
.
1
.
0
.
]
[
0
.
0
.
0
.
0
.
1
.
]
]
tensorboard
tensorflow
provid
function
to
debug
and
optim
program
with
the
help
of
a
visual
tool
call
tensorboard
.
tensorflow
creat
the
necessari
data
dure
it
execut
.
the
data
are
store
in
trace
file
.
tensorboard
can
be
view
from
a
browser
use
http
:
//localhost:6006/
We
can
run
the
follow
exampl
program
,
and
it
will
creat
the
directori
``
output
''
We
can
run
now
tensorboard
:
tensorboard
--
logdir
output
which
will
creat
a
webserv
:
tensorboard
0.1.8
at
http
:
//marvin:6006
(
press
ctrl+c
to
quit
)
import
tensorflow
as
tf
p
=
tf.constant
(
0.034
)
c
=
tf.constant
(
1000.0
)
x
=
tf.add
(
c
,
tf.multipli
(
p
,
c
)
)
x
=
tf.add
(
x
,
tf.multipli
(
p
,
x
)
)
with
tf.session
(
)
as
sess
:
writer
=
tf.summary.filewrit
(
``
output
''
,
sess.graph
)
print
(
sess.run
(
x
)
)
writer.clos
(
)
1069.16
the
comput
graph
is
includ
in
the
tensorboard
:
placeholdersa
comput
graph
can
be
parameter
to
accept
extern
input
,
known
as
placehold
.
the
valu
for
placehold
are
provid
when
the
graph
is
run
in
a
session
.
import
tensorflow
as
tf
c1
=
tf.placehold
(
tf.float32
)
c2
=
tf.placehold
(
tf.float32
)
x
=
tf.multipli
(
c1
,
c1
)
y
=
tf.multipli
(
c1
,
c2
)
final_nod
=
tf.add
(
x
,
y
)
with
tf.session
(
)
as
sess
:
result
=
final_node.ev
(
{
c1
:
3.8
,
c2
:
47.11
}
)
print
(
result
)
result
=
final_node.ev
(
{
c1
:
[
3
,
5
]
,
c2
:
[
1
,
3
]
}
)
print
(
result
)
193.458
[
12
.
40
.
]
anoth
exampl
:
import
tensorflow
as
tf
import
numpi
as
np
v1
=
np.array
(
[
3
,
4
,
5
]
)
v2
=
np.array
(
[
4
,
1
,
1
]
)
c1
=
tf.placehold
(
tf.float32
,
shape=
(
3
,
)
)
c2
=
tf.placehold
(
tf.float32
,
shape=
(
3
,
)
)
x
=
tf.multipli
(
c1
,
c1
)
y
=
tf.multipli
(
c1
,
c2
)
final_nod
=
tf.add
(
x
,
y
)
with
tf.session
(
)
as
sess
:
result
=
final_node.ev
(
{
c1
:
v1
,
c2
:
v2
}
)
print
(
result
)
[
21
.
20
.
30
.
]
placehold
(
dtype
,
shape=non
,
name=non
)
insert
a
placehold
for
a
tensor
that
will
be
alway
fed
.
It
return
a
tensor
that
may
be
use
as
a
handl
for
feed
a
valu
,
but
not
evalu
directli
.
import
:
thi
tensor
will
produc
an
error
if
evalu
.
it
valu
must
be
fed
use
the
feed_dict
option
argument
to
session.run
(
)
tensor.ev
(
)
operation.run
(
)
arg
:
paramet
descript
dtype
:
the
type
of
element
in
the
tensor
to
be
fed
.
shape
:
the
shape
of
the
tensor
to
be
fed
(
option
)
.
If
the
shape
is
not
specifi
,
you
can
feed
a
tensor
of
ani
shape
.
name
:
A
name
for
the
oper
(
option
)
.
variablesvari
are
use
to
add
trainabl
paramet
to
a
graph
.
they
are
construct
with
a
type
and
initi
valu
.
variabl
are
not
initi
when
you
call
tf.variabl
.
To
initi
the
variabl
of
a
tensorflow
graph
,
we
have
to
call
global_variables_initi
:
import
tensorflow
as
tf
W
=
tf.variabl
(
[
.5
]
,
dtype=tf.float32
)
b
=
tf.variabl
(
[
-1
]
,
dtype=tf.float32
)
x
=
tf.placehold
(
tf.float32
)
model
=
W
*
x
+
b
with
tf.session
(
)
as
sess
:
init
=
tf.global_variables_initi
(
)
sess.run
(
init
)
print
(
sess.run
(
model
,
{
x
:
[
1
,
2
,
3
,
4
]
}
)
)
[
-0.5
0
.
0.5
1
.
]
differ
between
variabl
and
placeholdersth
differ
between
tf.variabl
and
tf.placehold
consist
in
the
time
when
the
valu
are
pass
.
If
you
use
tf.variabl
,
you
have
to
provid
an
initi
valu
when
you
declar
it
.
with
tf.placehold
you
do
n't
have
to
provid
an
initi
valu
.
the
valu
can
be
specifi
at
run
time
with
the
feed_dict
argument
insid
session.run
A
placehold
is
use
for
feed
extern
data
into
a
tensorflow
comput
,
i.e
.
from
outsid
of
the
graph
!
If
you
are
train
a
learn
algorithm
,
a
placehold
is
use
for
feed
in
your
train
data
.
thi
mean
that
the
train
data
is
not
part
of
the
comput
graph
.
the
placehold
behav
similar
to
the
python
``
input
''
statement
.
On
the
other
hand
a
tensorflow
variabl
behav
more
or
less
like
a
python
variabl
!
exampl
:
calcul
the
loss
:
import
tensorflow
as
tf
W
=
tf.variabl
(
[
.5
]
,
dtype=tf.float32
)
b
=
tf.variabl
(
[
-1
]
,
dtype=tf.float32
)
x
=
tf.placehold
(
tf.float32
)
y
=
tf.placehold
(
tf.float32
)
model
=
W
*
x
+
b
delta
=
tf.squar
(
model
-
y
)
loss
=
tf.reduce_sum
(
delta
)
with
tf.session
(
)
as
sess
:
init
=
tf.global_variables_initi
(
)
sess.run
(
init
)
print
(
sess.run
(
loss
,
{
x
:
[
1
,
2
,
3
,
4
]
,
y
:
[
1
,
1
,
1
,
1
]
}
)
)
3.5
reassign
valu
to
variabl
import
tensorflow
as
tf
W
=
tf.variabl
(
[
.5
]
,
dtype=tf.float32
)
b
=
tf.variabl
(
[
-1
]
,
dtype=tf.float32
)
x
=
tf.placehold
(
tf.float32
)
y
=
tf.placehold
(
tf.float32
)
model
=
W
*
x
+
b
delta
=
tf.squar
(
model
-
y
)
loss
=
tf.reduce_sum
(
delta
)
with
tf.session
(
)
as
sess
:
init
=
tf.global_variables_initi
(
)
sess.run
(
init
)
print
(
sess.run
(
loss
,
{
x
:
[
1
,
2
,
3
,
4
]
,
y
:
[
1
,
1
,
1
,
1
]
}
)
)
w_a
=
tf.assign
(
W
,
[
0
.
]
)
b_a
=
tf.assign
(
b
,
[
1
.
]
)
sess.run
(
w_a
)
sess.run
(
b_a
)
#
sess.run
(
[
w_a
,
b_a
]
)
#
altern
in
one
'run
'
print
(
sess.run
(
loss
,
{
x
:
[
1
,
2
,
3
,
4
]
,
y
:
[
1
,
1
,
1
,
1
]
}
)
)
3.5
0.0
import
tensorflow
as
tf
W
=
tf.variabl
(
[
.5
]
,
dtype=tf.float32
)
b
=
tf.variabl
(
[
-1
]
,
dtype=tf.float32
)
x
=
tf.placehold
(
tf.float32
)
y
=
tf.placehold
(
tf.float32
)
model
=
W
*
x
+
b
delta
=
tf.squar
(
model
-
y
)
loss
=
tf.reduce_sum
(
delta
)
optim
=
tf.train.gradientdescentoptim
(
0.01
)
train
=
optimizer.minim
(
loss
)
with
tf.session
(
)
as
sess
:
init
=
tf.global_variables_initi
(
)
sess.run
(
init
)
for
_
in
rang
(
1000
)
:
sess.run
(
train
,
{
x
:
[
1
,
2
,
3
,
4
]
,
y
:
[
1
,
1
,
1
,
1
]
}
)
writer
=
tf.summary.filewrit
(
``
optim
''
,
sess.graph
)
print
(
sess.run
(
[
W
,
b
]
)
)
writer.clos
(
)
[
array
(
[
3.91378126e-06
]
,
dtype=float32
)
,
array
(
[
0.99998844
]
,
dtype=float32
)
]
creat
data
setsw
will
creat
data
set
for
a
larger
exampl
for
the
gradientdescentoptim
.
import
numpi
as
np
import
matplotlib.pyplot
as
plt
for
quantiti
,
suffix
in
[
(
1000
,
``
train
''
)
,
(
200
,
``
test
''
)
]
:
sampl
=
np.random.multivariate_norm
(
[
-2
,
-2
]
,
[
[
1
,
0
]
,
[
0
,
1
]
]
,
quantiti
)
plt.plot
(
sampl
[
:
,
0
]
,
sampl
[
:
,
1
]
,
'
.
'
,
label=
''
bad
one
``
+
suffix
)
bad_on
=
np.column_stack
(
(
np.zero
(
quantiti
)
,
sampl
)
)
sampl
=
np.random.multivariate_norm
(
[
1
,
1
]
,
[
[
1
,
0.5
]
,
[
0.5
,
1
]
]
,
quantiti
)
plt.plot
(
sampl
[
:
,
0
]
,
sampl
[
:
,
1
]
,
'
.
'
,
label=
''
good
one
``
+
suffix
)
good_on
=
np.column_stack
(
(
np.one
(
quantiti
)
,
sampl
)
)
sampl
=
np.row_stack
(
(
bad_on
,
good_on
)
)
np.savetxt
(
``
data/the_good_and_the_bad_ones_
''
+
suffix
+
``
.txt
''
,
sampl
,
fmt=
''
%
1d
%
4.2f
%
4.2f
''
)
plt.legend
(
)
plt.show
(
)
import
os
os.environ
[
'tf_cpp_min_log_level
'
]
=
'
2
'
import
numpi
as
np
import
tensorflow
as
tf
from
matplotlib
import
pyplot
as
plt
number_of_samples_per_training_step
=
100
num_of_epoch
=
1
num_label
=
2
#
should
be
automat
determin
def
evaluation_func
(
X
)
:
return
predicted_class.ev
(
feed_dict=
{
x
:
X
}
)
def
plot_boundari
(
X
,
Y
,
pred_func
)
:
#
determin
canva
border
min
=
np.amin
(
X
,
0
)
#
array
with
column
minimum
min
=
min
-
0.1*np.ab
(
min
)
max
=
np.amax
(
X
,
0
)
#
array
with
column
maximum
max
=
max
+
0.1*max
xs
,
ys
=
np.meshgrid
(
np.linspac
(
min
[
0
]
,
max
[
0
]
,
300
)
,
np.linspac
(
min
[
1
]
,
max
[
1
]
,
300
)
)
#
evalu
model
use
the
dens
grid
#
c_
creat
one
array
with
``
point
''
from
meshgrid
:
Z
=
pred_func
(
np.c_
[
xs.flatten
(
)
,
ys.flatten
(
)
]
)
#
Z
is
one-dimension
and
will
be
reshap
into
300
x
300
:
Z
=
z.reshap
(
xs.shape
)
#
plot
the
contour
and
train
exampl
plt.contourf
(
xs
,
ys
,
Z
,
colors=
(
'
c
'
,
'
g
'
,
'
y
'
,
'
b
'
)
)
Xn
=
X
[
Y
[
:
,1
]
==1
]
plt.plot
(
Xn
[
:
,
0
]
,
Xn
[
:
,
1
]
,
``
bo
''
)
Xn
=
X
[
Y
[
:
,1
]
==0
]
plt.plot
(
Xn
[
:
,
0
]
,
Xn
[
:
,
1
]
,
``
go
''
)
plt.show
(
)
def
get_data
(
fname
)
:
data
=
np.loadtxt
(
fname
)
label
=
data
[
:
,
:1
]
#
array
(
[
[
0
.
]
,
[
0
.
]
,
[
1
.
]
,
...
]
]
)
labels_one_hot
=
(
np.arang
(
num_label
)
==
label
)
.astyp
(
np.float32
)
data
=
data
[
:
,
1
:
]
.astyp
(
np.float32
)
return
data
,
labels_one_hot
data_train
=
``
data/the_good_and_the_bad_ones_train.txt
''
data_test
=
``
data/the_good_and_the_bad_ones_test.txt
''
train_data
,
train_label
=
get_data
(
data_train
)
test_data
,
test_label
=
get_data
(
data_test
)
train_siz
,
num_featur
=
train_data.shap
x
=
tf.placehold
(
``
float
''
,
shape=
[
none
,
num_featur
]
)
y_
=
tf.placehold
(
``
float
''
,
shape=
[
none
,
num_label
]
)
weight
=
tf.variabl
(
tf.zero
(
[
num_featur
,
num_label
]
)
)
b
=
tf.variabl
(
tf.zero
(
[
num_label
]
)
)
y
=
tf.nn.softmax
(
tf.matmul
(
x
,
weight
)
+
b
)
#
optim
.
cross_entropi
=
-tf.reduce_sum
(
y_*tf.log
(
y
)
)
train_step
=
tf.train.gradientdescentoptim
(
0.01
)
.minim
(
cross_entropi
)
#
for
the
test
data
,
hold
the
entir
dataset
in
one
constant
node
.
test_data_nod
=
tf.constant
(
test_data
)
#
evalu
.
predicted_class
=
tf.argmax
(
y
,
1
)
correct_predict
=
tf.equal
(
tf.argmax
(
y,1
)
,
tf.argmax
(
y_,1
)
)
accuraci
=
tf.reduce_mean
(
tf.cast
(
correct_predict
,
``
float
''
)
)
with
tf.session
(
)
as
sess
:
#
run
all
the
initi
to
prepar
the
trainabl
paramet
.
init
=
tf.global_variables_initi
(
)
sess.run
(
init
)
#
iter
and
train
.
for
step
in
rang
(
num_of_epoch
*
train_siz
//
number_of_samples_per_training_step
)
:
offset
=
(
step
*
number_of_samples_per_training_step
)
%
train_siz
#
get
a
batch
of
data
batch_data
=
train_data
[
offset
:
(
offset
+
number_of_samples_per_training_step
)
,
:
]
batch_label
=
train_label
[
offset
:
(
offset
+
number_of_samples_per_training_step
)
]
#
feed
data
into
the
model
train_step.run
(
feed_dict=
{
x
:
batch_data
,
y_
:
batch_label
}
)
print
(
'\nbia
vector
:
'
,
sess.run
(
b
)
)
print
(
'weight
matrix
:
\n
'
,
sess.run
(
weight
)
)
print
(
``
\nappli
model
to
first
data
set
:
''
)
first
=
test_data
[
:1
]
print
(
first
)
print
(
``
\nwx
+
b
:
``
,
sess.run
(
tf.matmul
(
first
,
weight
)
+
b
)
)
#
the
softmax
function
,
or
normal
exponenti
function
,
is
a
gener
of
the
#
logist
function
that
``
squash
''
a
k-dimension
vector
z
of
arbitrari
real
valu
#
to
a
k-dimension
vector
σ
(
z
)
of
real
valu
in
the
rang
[
0
,
1
]
that
add
up
to
1.
print
(
``
softmax
(
Wx
+
b
)
:
``
,
sess.run
(
tf.nn.softmax
(
tf.matmul
(
first
,
weight
)
+
b
)
)
)
print
(
``
accuraci
on
test
data
:
``
,
accuracy.ev
(
feed_dict=
{
x
:
test_data
,
y_
:
test_label
}
)
)
print
(
``
accuraci
on
train
data
:
``
,
accuracy.ev
(
feed_dict=
{
x
:
train_data
,
y_
:
train_label
}
)
)
#
classifi
some
valu
:
print
(
evaluation_func
(
[
[
-3
,
7.3
]
,
[
-1,8
]
,
[
0
,
0
]
,
[
1
,
0.0
]
,
[
-1
,
0
]
]
)
)
plot_boundari
(
test_data
,
test_label
,
evaluation_func
)
the
abov
code
return
the
follow
:
bia
vector
:
[
-0.78089082
0.78089082
]
weight
matrix
:
[
[
-0.80193734
0.8019374
]
[
-0.831303
0.831303
]
]
appli
model
to
first
data
set
:
[
[
-1.05999994
-1.55999994
]
]
Wx
+
b
:
[
[
1.36599553
-1.36599553
]
]
softmax
(
Wx
+
b
)
:
[
[
0.93888813
0.06111182
]
]
accuraci
on
test
data
:
0.97
accuraci
on
train
data
:
0.9725
[
1
1
1
1
0
]
previou
chapter
:
expect
maxim
and
gaussian
mixtur
model
©
2011
-
2018
,
bernd
klein
,
bodenseo
;
design
by
denis
mitchinson
adapt
for
python-course.eu
by
bernd
klein
