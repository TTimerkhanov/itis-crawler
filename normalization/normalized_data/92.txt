machin
learn
with
python
:
decis
tree
in
python
python
machin
learn
tutori
machin
learn
machin
learn
terminologyk-nearest
neighbor
classifierneur
network
from
scratch
in
pythonneur
network
in
python
use
numypybackpropag
in
neural
networksconfus
matrixtrain
and
test
with
mnistdropout
neural
networksneur
network
with
scikitmachin
learn
with
scikit
and
pythonintroduct
naiv
bay
classifierna
bay
classifi
with
scikitintroduct
into
text
classif
use
naiv
bayespython
implement
of
text
classificationdecis
treesregress
treesrandom
forestsboost
algorithmprincip
compon
analysislinear
discrimin
analysisexpect
maxim
and
gaussian
mixtur
modelintroduct
into
tensorflow
origin
of
machin
learn
the
origin
of
machin
learn
go
back
to
the
year
1959
.
the
term
``
machin
learn
''
wa
coin
in
thi
year
by
arthur
lee
samuel
.
He
wrote
a
checkers-play
program
which
is
consid
to
be
the
first
self-learn
program
.
the
real
problem
``
peopl
worri
that
comput
will
get
too
smart
and
take
over
the
world
,
but
the
real
problem
is
that
they
're
too
stupid
and
they
've
alreadi
taken
over
the
world
.
''
(
pedro
domingo
)
pedro
domingo
is
professor
at
univers
of
washington
.
He
is
a
research
in
machin
learn
and
known
for
markov
logic
network
enabl
uncertain
infer
.
learn
``
tell
me
and
I
forget
,
teach
me
and
I
may
rememb
,
involv
me
and
I
learn
.
''
(
benjamin
franklin
)
``
the
more
I
read
,
the
more
I
acquir
,
the
more
certain
I
am
that
I
know
noth
.
''
(
voltair
)
``
you
live
and
learn
.
At
ani
rate
,
you
live
.
''
(
dougla
adam
,
mostli
harmless
)
If
learn
mean
live
,
some
comput
live
!
``
In
learn
you
will
teach
,
and
in
teach
you
will
learn
.
''
(
phil
collin
)
thi
websit
is
creat
by
:
linux
and
python
cours
and
seminar
On
site
train
in
europ
,
canada
and
the
US
.
thi
websit
is
free
of
annoy
ad
.
We
want
to
keep
it
like
thi
.
you
can
help
with
your
donat
:
the
need
for
donat
tobia
schlagenhauf
thi
chapter
wa
written
by
tobia
schlagenhauf
.
tobia
is
a
inquisit
and
motiv
machin
learn
enthusiast
.
alway
posit
,
hungri
to
learn
,
will
to
help
.
If
you
have
ani
comment
,
question
,
concern
about
the
content
of
thi
chapter
feel
free
to
get
in
contact
.
you
can
find
and
contact
tobia
schlagenhauf
at
xing
search
thi
websit
:
classroom
train
cours
thi
websit
contain
a
free
and
extens
onlin
tutori
by
bernd
klein
,
use
materi
from
hi
classroom
python
train
cours
.
If
you
are
interest
in
an
instructor-l
classroom
train
cours
,
you
may
have
a
look
at
the
python
class
by
bernd
klein
at
bodenseo
.
Â©
kabliczech
-
fotolia.com
quot
of
the
day
:
''
ani
fool
can
write
code
that
a
comput
can
understand
.
good
programm
write
code
that
human
can
understand
.
''
(
martin
fowler
)
If
you
have
the
choic
work
with
python
2
or
python
3
,
we
recomend
to
switch
to
python
3
!
you
can
read
our
python
tutori
to
see
what
the
differ
are
.
data
protect
declar
data
protect
declar
previou
chapter
:
python
implement
of
text
classif
next
chapter
:
regress
tree
what
are
decis
tree
?
decis
tree
are
supervis
learn
algorithm
use
for
both
,
classif
and
regress
task
where
we
will
concentr
on
classif
in
thi
first
part
of
our
decis
tree
tutori
.
decis
tree
are
assign
to
the
inform
base
learn
algorithm
which
use
differ
measur
of
inform
gain
for
learn
.
We
can
use
decis
tree
for
issu
where
we
have
continu
but
also
categor
input
and
target
featur
.
the
main
idea
of
decis
tree
is
to
find
those
descript
featur
which
contain
the
most
``
inform
''
regard
the
target
featur
and
then
split
the
dataset
along
the
valu
of
these
featur
such
that
the
target
featur
valu
for
the
result
sub_dataset
are
as
pure
as
possibl
--
>
the
descript
featur
which
leav
the
target
featur
most
pure
is
said
to
be
the
most
inform
one
.
thi
process
of
find
the
``
most
inform
''
featur
is
done
until
we
accomplish
a
stop
criteria
where
we
then
final
end
up
in
so
call
leaf
node
.
the
leaf
node
contain
the
predict
we
will
make
for
new
queri
instanc
present
to
our
train
model
.
thi
is
possibl
sinc
the
model
ha
kind
of
learn
the
underli
structur
of
the
train
data
and
henc
can
,
given
some
assumpt
,
make
predict
about
the
target
featur
valu
(
class
)
of
unseen
queri
instanc
.
A
decis
tree
mainli
contain
of
a
root
node
,
interior
node
,
and
leaf
node
which
are
then
connect
by
branch
.
decis
tree
are
further
subdivid
whether
the
target
featur
is
continu
scale
like
for
instanc
hous
price
or
categor
scale
like
for
instanc
anim
speci
.
In
simplifi
term
,
the
process
of
train
a
decis
tree
and
predict
the
target
featur
of
queri
instanc
is
as
follow
:
1
.
present
a
dataset
contain
of
a
number
of
train
instanc
character
by
a
number
of
descript
featur
and
a
target
featur
2
.
train
the
decis
tree
model
by
continu
split
the
target
featur
along
the
valu
of
the
descript
featur
use
a
measur
of
inform
gain
dure
the
train
process
3
.
grow
the
tree
until
we
accomplish
a
stop
criteria
--
>
creat
leaf
node
which
repres
the
predict
we
want
to
make
for
new
queri
instanc
4
.
show
queri
instanc
to
the
tree
and
run
down
the
tree
until
we
arriv
at
leaf
node
5
.
done
-
congratul
you
have
found
the
answer
to
your
question
So
what
do
we
know
until
know
?
In
princip
decis
tree
can
be
use
to
predict
the
target
featur
of
a
unknown
queri
instanc
by
build
a
model
base
on
exist
data
for
which
the
target
featur
valu
are
known
(
supervis
learn
)
.
addit
,
we
know
that
thi
model
can
make
predict
for
unknown
queri
instanc
becaus
it
model
the
relationship
between
the
known
descript
featur
and
the
know
target
featur
.
In
our
follow
exampl
,
the
tree
model
learn
``
how
a
specif
anim
speci
look
like
''
respect
the
combin
of
descript
featur
valu
distinct
for
anim
speci
.
addit
,
we
know
that
to
train
a
decis
tree
model
we
need
a
dataset
consist
of
a
number
of
train
exampl
character
by
a
number
of
descript
featur
and
a
target
featur
.
what
we
do
not
know
until
know
is
:
how
we
can
build
a
tree
model
.
To
answer
that
question
we
should
recapitul
what
we
tri
to
achiev
use
a
decis
tree
model
.
We
want
,
given
a
dataset
,
train
a
model
which
kind
of
learn
the
relationship
between
the
descript
featur
and
a
target
featur
such
that
we
can
present
the
model
a
new
,
unseen
set
of
queri
instanc
and
predict
the
target
featur
valu
for
these
queri
instanc
.
let
further
recapitul
the
gener
shape
of
a
decis
tree
.
We
know
that
we
have
at
the
bottom
of
the
tree
leaf
node
which
contain
(
in
the
optim
case
)
target
featur
valu
.
To
make
thi
more
illustr
we
use
as
a
practic
exampl
a
simplifi
version
of
the
uci
machin
learn
zoo
anim
classif
dataset
which
includ
properti
of
anim
as
descript
featur
and
the
and
the
anim
speci
as
target
featur
.
In
our
exampl
the
anim
are
classifi
as
be
mammal
or
reptil
base
on
whether
they
are
tooth
,
have
leg
and
do
breath
.
the
dataset
look
like
:
import
panda
as
pd
data
=
pd.datafram
(
{
``
tooth
''
:
[
``
true
''
,
''
true
''
,
''
true
''
,
''
fals
''
,
''
true
''
,
''
true
''
,
''
true
''
,
''
true
''
,
''
true
''
,
''
fals
''
]
,
``
hair
''
:
[
``
true
''
,
''
true
''
,
''
fals
''
,
''
true
''
,
''
true
''
,
''
true
''
,
''
fals
''
,
''
fals
''
,
''
true
''
,
''
fals
''
]
,
``
breath
''
:
[
``
true
''
,
''
true
''
,
''
true
''
,
''
true
''
,
''
true
''
,
''
true
''
,
''
fals
''
,
''
true
''
,
''
true
''
,
''
true
''
]
,
``
leg
''
:
[
``
true
''
,
''
true
''
,
''
fals
''
,
''
true
''
,
''
true
''
,
''
true
''
,
''
fals
''
,
''
fals
''
,
''
true
''
,
''
true
''
]
,
``
speci
''
:
[
``
mammal
''
,
''
mammal
''
,
''
reptil
''
,
''
mammal
''
,
''
mammal
''
,
''
mammal
''
,
''
reptil
''
,
''
reptil
''
,
''
mammal
''
,
''
reptil
''
]
}
,
columns=
[
``
tooth
''
,
''
hair
''
,
''
breath
''
,
''
leg
''
,
''
speci
''
]
)
featur
=
data
[
[
``
tooth
''
,
''
hair
''
,
''
breath
''
,
''
leg
''
]
]
target
=
data
[
``
speci
''
]
data
after
have
execut
the
python
code
abov
we
receiv
the
follow
output
:
tooth
hair
breath
leg
speci
0
true
true
true
true
mammal
1
true
true
true
true
mammal
2
true
fals
true
fals
reptil
3
fals
true
true
true
mammal
4
true
true
true
true
mammal
5
true
true
true
true
mammal
6
true
fals
fals
fals
reptil
7
true
fals
true
fals
reptil
8
true
true
true
true
mammal
9
fals
fals
true
true
reptil
henc
,
to
come
back
to
our
initi
question
,
each
leaf
node
should
(
in
the
best
case
)
onli
contain
``
mammal
''
or
``
reptil
''
.
the
task
for
us
is
now
to
find
the
best
``
way
''
to
split
the
dataset
such
that
thi
can
be
achiev
.
what
do
I
mean
when
I
say
split
?
well
consid
the
dataset
abov
and
think
about
what
must
be
done
to
split
the
dataset
into
a
dataset
1
contain
as
target
featur
valu
(
speci
)
onli
mammal
and
a
dataset
2
,
contain
onli
reptil
.
To
achiev
that
,
in
thi
simplifi
exampl
,
we
onli
need
the
descript
featur
hair
sinc
if
hair
is
true
,
the
associ
speci
is
alway
a
mammal
.
henc
in
thi
case
our
tree
model
would
look
like
:
that
is
,
we
have
split
our
dataset
by
ask
the
question
if
the
anim
ha
hair
or
not
.
and
exactli
thi
ask
and
therewith
split
is
the
key
to
the
decis
tree
model
.
now
in
that
case
the
split
ha
been
veri
easi
becaus
we
onli
have
a
small
number
of
descript
featur
and
the
dataset
is
complet
separ
along
the
valu
of
onli
one
descript
featur
.
howev
,
most
of
the
time
dataset
are
not
that
easili
separ
and
we
must
split
the
dataset
more
than
one
time
(
``
ask
more
than
one
question
''
)
.
here
,
the
next
question
directli
aris
:
given
that
we
have
to
split
the
dataset
more
than
one
time
,
that
is
,
ask
more
then
one
question
to
separ
the
dataset
,
which
is
the
descript
featur
we
should
start
with
(
root
node
)
and
in
which
order
should
we
ask
question
(
build
the
interior
node
)
that
is
,
use
descript
featur
to
split
the
dataset
on
?
well
,
we
have
seen
that
use
the
hair
descript
featur
seem
to
occupi
the
most
inform
about
the
target
featur
sinc
we
onli
need
thi
featur
to
perfectli
split
the
dataset
.
henc
it
would
be
use
to
measur
the
``
inform
''
of
the
featur
and
use
the
featur
with
the
most
``
inform
''
as
the
featur
which
should
be
use
to
split
the
data
on
.
from
now
on
,
we
use
the
term
inform
gain
as
a
measur
of
``
inform
''
of
a
featur
.
In
the
follow
section
we
will
introduc
some
mathemat
term
and
deriv
how
the
inform
gain
is
calcul
as
well
as
how
we
can
build
a
tree
model
base
on
that
.
the
math
behind
decis
tree
In
the
preced
section
we
have
introduc
the
inform
gain
as
a
measur
of
how
good
a
descript
featur
is
suit
to
split
a
dataset
on
.
To
be
abl
to
calcul
the
inform
gain
,
we
have
to
first
introduc
the
term
entropi
of
a
dataset
.
the
entropi
of
a
dataset
is
use
to
measur
the
impur
of
a
dataset
and
we
will
use
thi
kind
of
inform
measur
in
our
calcul
.
there
are
also
other
type
of
measur
which
can
be
use
to
calcul
the
inform
gain
.
the
most
promin
one
are
the
:
gini
index
,
chi-squar
,
inform
gain
ratio
,
varianc
.
the
term
entropi
(
in
inform
theori
)
goe
back
to
claud
E.
shannon
.
the
idea
behind
the
entropi
is
,
in
simplifi
term
,
the
follow
:
imagin
you
have
a
lotteri
wheel
which
includ
100
green
ball
.
the
set
of
ball
within
the
lotteri
wheel
can
be
said
to
be
total
pure
becaus
onli
green
ball
are
includ
.
To
express
thi
in
the
terminolog
of
entropi
,
thi
set
of
ball
ha
a
entropi
of
0
(
we
can
also
say
zero
impur
)
.
consid
now
,
30
of
these
ball
are
replac
by
red
and
20
by
blue
ball
.
If
you
now
draw
anoth
ball
from
the
lotteri
wheel
,
the
probabl
of
receiv
a
green
ball
ha
drop
from
1.0
to
0.5
.
sinc
the
impur
increas
,
the
puriti
decreas
,
henc
also
the
entropi
increas
.
henc
we
can
say
,
the
more
``
impur
''
a
dataset
,
the
higher
the
entropi
and
the
less
``
impur
''
a
dataset
,
the
lower
the
entropi
.
shannon
's
entropi
model
use
the
logarithm
function
(
$
log_
{
2
}
(
P
(
x
)
)
$
)
to
measur
the
entropi
and
therewith
the
impur
of
a
dataset
sinc
the
higher
the
probabl
of
get
a
specif
result
==
P
(
x
)
(
randomli
draw
a
green
ball
)
,
the
closer
approach
the
binari
logarithm
1.
import
numpi
as
np
import
matplotlib.pyplot
as
plt
fig=plt.figur
(
)
ax=fig.add_subplot
(
111
)
ax.plot
(
np.linspac
(
0.01,1
)
,
np.log2
(
np.linspac
(
0.01,1
)
)
)
ax.set_xlabel
(
``
P
(
x
)
''
)
ax.set_ylabel
(
``
log2
(
P
(
x
)
)
''
)
plt.show
(
)
onc
a
dataset
contain
more
than
one
``
type
''
of
element
specif
more
than
one
target
featur
valu
,
the
impur
will
be
greater
than
zero
.
therewith
also
the
entropi
of
the
dataset
will
be
greater
than
zero
.
henc
it
is
use
to
sum
up
the
entropi
of
each
possibl
target
featur
valu
and
weight
it
by
the
probabl
that
we
achiev
these
valu
assum
we
would
randomli
draw
valu
from
the
target
featur
valu
space
(
what
is
the
probabl
to
draw
a
green
ball
just
by
chanc
?
exactli
,
0.5
and
therewith
we
have
to
weight
the
entropi
calcul
for
the
green
ball
with
0.5
)
.
thi
final
lead
to
the
formal
definit
of
shannon
's
entropi
which
serv
as
the
baselin
for
the
inform
gain
calcul
:
$
$
H
(
x
)
=
-\sum_
{
for
\
k
\
\in
target
}
(
P
(
x=k
)
*log_2
(
P
(
x=k
)
)
)
$
$
where
we
say
that
P
(
x=k
)
is
the
probabl
,
that
the
target
featur
take
a
specif
valu
k.
henc
appli
thi
formula
to
our
exampl
with
the
three
color
ball
we
get
:
green
ball
:
$
H
(
x=green
)
=
0.5*log_2
(
0.5
)
=
-0.5
$
blue
ball
:
$
H
(
x=blue
)
=
0.2*log_2
(
0.2
)
=
-0.464
$
red
ball
:
$
H
(
x=red
)
=
0.3*log_2
(
0.3
)
=
-0.521
$
H
(
x
)
:
$
H
(
x
)
=
-
(
(
-0.5
)
+
(
-0.464
)
+
(
-0.521
)
)
=
1.485
$
let
appli
thi
approach
to
our
origin
dataset
where
we
want
to
predict
the
anim
speci
.
our
dataset
ha
two
target
featur
valu
in
it
target
featur
valu
space
{
mammal
,
reptil
}
.
where
$
P
(
x=mammal
)
=
0.6
$
and
$
P
(
x=reptil
)
=
0.4
$
henc
the
entropi
of
our
dataset
regard
the
target
featur
is
calcul
with
:
$
H
(
x
)
=
-
(
(
0.6*log_2
(
0.6
)
)
+
(
0.4*log_2
(
0.4
)
)
)
=
0.971
$
So
where
are
we
now
on
our
way
toward
creat
a
tree
model
?
We
have
now
determin
the
total
impurity/pur
(
$
\approx
$
entropi
)
of
our
dataset
which
equal
to
approxim
$
0.971
$
.
now
our
task
is
to
find
the
best
featur
in
term
of
inform
gain
(
rememb
that
we
want
to
find
the
featur
which
split
the
data
most
accur
along
the
target
featur
valu
)
which
we
should
use
to
first
split
our
data
on
(
which
serv
as
root
node
)
.
rememb
that
the
hair
featur
is
no
longer
part
of
our
featur
set
.
follow
thi
,
how
can
we
check
which
of
the
descript
featur
most
accur
split
the
dataset
,
that
is
,
remain
the
dataset
with
the
lowest
impur
$
\approx
$
entropi
or
in
other
word
best
classifi
the
target
featur
by
it
own
?
well
,
we
use
each
descript
featur
and
split
the
dataset
along
the
valu
of
these
descript
featur
and
then
calcul
the
entropi
of
the
dataset
onc
we
have
split
the
data
along
the
featur
valu
.
thi
give
us
the
remain
entropi
after
we
have
split
the
dataset
along
the
featur
valu
.
next
,
we
subtract
thi
valu
from
the
origin
calcul
entropi
of
the
dataset
to
see
how
much
thi
featur
split
reduc
the
origin
entropi
.
the
inform
gain
of
a
featur
is
calcul
with
:
$
$
infogain
(
feature_
{
d
}
)
=
entropi
(
D
)
-entropi
(
feature_
{
d
}
)
$
$
So
the
onli
thing
we
have
to
do
is
to
split
the
dataset
along
the
valu
of
each
featur
and
then
treat
these
sub
set
as
if
they
were
our
``
origin
''
dataset
in
term
of
entropi
calcul
.
the
formula
for
the
inform
gain
calcul
per
featur
is
:
$
$
inforgain
(
feature_
{
d
}
,
D
)
=
entropi
(
D
)
-\sum_
{
t
\
\in
\
featur
}
(
\frac
{
|feature_
{
d
}
=
t|
}
{
|d|
}
*H
(
feature_
{
d
}
=
t
)
)
$
$
$
$
=
$
$
$
$
entropi
(
D
)
-\sum_
{
t
\
\in
\
featur
}
(
\frac
{
|feature_
{
d
}
=
t|
}
{
|d|
}
*
(
-\sum_
{
k
\
\in
\
target
}
(
P
(
target=k
,
feature_
{
d
}
=
t
)
*log_
{
2
}
(
P
(
target
=
k
,
feature_
{
d
}
=
t
)
)
)
)
$
$
summar
,
for
each
descript
featur
,
we
sum
up
the
result
entropi
for
split
the
dataset
along
the
featur
valu
and
addit
weight
the
featur
valu
entropi
by
their
occurr
probabl
.
now
we
will
calcuat
the
inform
gain
for
each
descript
featur
:
tooth
:
$
=
$
0.963547
$
infogain
(
tooth
)
=
0.971-0.963547
=
$
0.00745
breath
:
$
H
(
breath
)
=
(
\frac
{
9
}
{
10
}
*-
(
(
\frac
{
6
}
{
9
}
*log_2
(
\frac
{
6
}
{
9
}
)
)
+
(
\frac
{
3
}
{
9
}
*log_2
(
\frac
{
3
}
{
9
}
)
)
)
+\frac
{
1
}
{
10
}
*-
(
(
0
)
+
(
1*log_2
(
1
)
)
)
)
$
=
0.82647
$
infogain
(
breath
)
=
0.971-0.82647
=
$
0.1445
leg
:
$
H
(
leg
)
=\frac
{
7
}
{
10
}
*-
(
(
\frac
{
6
}
{
7
}
*log_2
(
\frac
{
6
}
{
7
}
)
)
+
(
\frac
{
1
}
{
7
}
*log_2
(
\frac
{
1
}
{
7
}
)
)
)
+\frac
{
3
}
{
10
}
*-
(
(
0
)
+
(
1*log_2
(
1
)
)
)
=
$
0.41417
$
infogain
(
leg
)
=
0.971-0.41417
=
$
0.5568
henc
the
split
the
dataset
along
the
featur
leg
result
in
the
largest
inform
gain
and
we
should
use
thi
featur
for
our
root
node
.
henc
for
the
time
be
the
decis
tree
model
look
like
:
We
see
that
for
leg
==
fals
,
the
target
featur
valu
of
the
remain
dataset
are
all
reptil
and
henc
we
set
thi
as
leaf
node
becaus
we
have
a
pure
dataset
(
further
split
the
dataset
on
ani
of
the
remain
two
featur
would
not
lead
to
a
differ
or
more
accur
result
sinc
whatev
we
do
after
thi
point
,
the
predict
will
remain
reptil
)
.
addit
,
you
see
that
the
featur
leg
is
no
longer
includ
in
the
remain
dataset
.
becaus
we
alreadi
ha
use
thi
(
categor
)
featur
to
split
the
dataset
on
it
must
not
be
further
use
.
until
now
we
have
found
the
featur
for
the
root
node
as
well
as
a
leaf
node
for
leg
==
fals
.
the
same
step
for
inform
gain
calcul
must
now
be
accomplish
also
for
the
remain
dataset
for
leg
==
true
sinc
here
we
still
have
a
mixtur
of
differ
target
featur
valu
.
henc
:
inform
gain
calcul
for
the
featur
tooth
and
breath
for
the
remain
dataset
leg
==
true
:
entropi
of
the
(
new
)
sub
data
set
after
first
split
:
$
H
(
D
)
=
-
(
(
\frac
{
6
}
{
7
}
*log_2
(
\frac
{
6
}
{
7
}
)
)
+
(
\frac
{
1
}
{
7
}
*log_2
(
\frac
{
1
}
{
7
}
)
)
)
=
$
0.5917
tooth
:
$
H
(
tooth
)
=
\frac
{
5
}
{
7
}
*-
(
(
1*log_2
(
1
)
)
+
(
0
)
)
+\frac
{
2
}
{
7
}
*-
(
(
\frac
{
1
}
{
2
}
*log_2
(
\frac
{
1
}
{
2
}
)
)
+
(
\frac
{
1
}
{
2
}
*log_2
(
\frac
{
1
}
{
2
}
)
)
)
=
$
0.285
$
infogain
(
tooth
)
=
0.5917-0.285
=
$
0.3067
breath
:
$
H
(
breath
)
=
$
$
\frac
{
7
}
{
7
}
*-
(
(
\frac
{
6
}
{
7
}
*log_2
(
\frac
{
6
}
{
7
}
)
)
+
(
\frac
{
1
}
{
7
}
*log_2
(
\frac
{
1
}
{
7
}
)
)
)
+0
=
$
5917
$
infogain
(
tooth
)
=
$
$
0.5917-0.5917
=
$
0
the
dataset
for
tooth
==
fals
still
contain
a
mixtur
of
differ
target
featur
valu
whi
we
proceed
partit
on
the
last
left
featur
(
==
breath
)
henc
the
complet
grown
tree
look
like
:
mind
the
last
split
(
node
)
where
the
dataset
got
split
on
the
breath
featur
.
here
the
breath
featur
sole
contain
data
where
breath
==
true
.
henc
for
breath
==
fals
there
are
no
instanc
in
the
dataset
and
therewith
there
is
no
sub-dataset
which
can
be
built
.
In
that
case
we
return
the
most
frequent
occur
target
featur
valu
in
the
origin
dataset
which
is
mammal
.
thi
is
an
exampl
how
our
tree
model
gener
behind
the
train
data
.
If
we
consid
the
other
branch
,
that
is
breath
==
true
we
know
,
that
after
split
the
dataset
on
the
valu
of
a
specif
featur
(
breath
{
true
,
fals
}
)
in
our
case
,
the
featur
must
be
remov
.
well
,
that
lead
to
a
dataset
where
no
more
featur
are
avail
to
further
split
the
dataset
on
.
henc
we
stop
grow
the
tree
and
return
the
mode
valu
of
the
direct
parent
node
which
is
``
mammal
''
.
that
lead
us
to
the
introduct
of
the
id3
algorithm
which
is
a
popular
algorithm
to
grow
decis
tree
,
publish
by
ross
quinlan
in
1986
.
besid
the
id3
algorithm
there
are
also
other
popular
algorithm
like
the
c4.5
,
the
c5.0
and
the
cart
algorithm
which
we
will
not
further
consid
here
.
befor
we
introduc
the
id3
algorithm
let
quickli
come
back
to
the
stop
criteria
of
the
abov
grown
tree
.
We
can
defin
a
nearli
arbitrarili
larg
number
of
stop
criteria
.
assum
for
instanc
,
we
say
a
tree
is
allow
to
grow
for
onli
2
second
and
then
the
grow
process
should
stop
-
well
that
would
be
a
stop
criteria
-
nonetheless
,
there
are
mainli
three
use
case
in
which
we
stop
the
tree
from
grow
assum
we
do
not
stop
it
beforehand
by
defin
for
instanc
a
maximum
tree
depth
or
a
minimum
inform
gain
valu
.
We
stop
the
tree
from
grow
when
:
1
.
all
row
in
the
target
featur
have
the
same
valu
2
.
the
dataset
can
be
no
longer
split
sinc
there
are
no
more
featur
left
3
.
the
dataset
can
no
longer
be
split
sinc
there
are
no
more
row
left
/
there
is
no
data
left
By
definit
,
we
say
that
if
the
grow
get
stop
becaus
of
stop
criteria
two
,
the
leaf
node
should
predict
the
most
frequent
occur
target
featur
valu
of
the
superior
(
parent
)
node
.
If
the
grow
get
stop
becaus
of
the
third
stop
criteria
,
we
assign
the
leaf
node
the
mode
target
featur
valu
of
the
origin
dataset
.
attent
,
we
now
introduc
the
id3
algorithm
:
the
pseudocod
for
the
id3
algorithm
is
base
on
the
pseudocod
illust
of
(
mitchel
,
1997
)
.
id3
(
D
,
feature_attribut
,
target_attribut
)
creat
a
root
node
r
set
r
to
the
mode
target
featur
valu
in
D
If
all
target
featur
valu
are
the
same
:
return
r
els
:
pass
If
feature_attribut
is
empti
:
return
r
els
:
att
=
attribut
from
feature_attribut
with
the
largest
inform
gain
valu
r
=
att
for
valu
in
att
:
add
a
new
node
below
r
where
node_valu
=
(
att
==
valu
)
sub_d_valu
=
(
att
==
valu
)
If
sub_d_valu
==
empti
:
add
a
leaf
node
l
where
l
equal
the
mode
target
valu
in
D
els
:
add
sub_tre
with
id3
(
sub_d_valu
,
feature_attribut
=
feature_attribut
without
att
,
target_attribut
)
well
thi
pseudocod
is
probabl
a
littl
bit
confus
if
you
are
new
to
decis
tree
and
you
do
n't
have
a
mental
pictur
of
a
decis
tree
on
your
mind
.
therefor
we
will
illustr
thi
pseudocod
in
pictur
to
make
thing
a
littl
bit
more
clear
-hopefully-
.
classif
decis
tree
from
scratch
with
python
sinc
we
now
know
the
princip
step
of
the
id3
algorithm
,
we
will
start
creat
our
own
decis
tree
classif
model
from
scratch
in
python
.
therefor
we
will
use
the
whole
uci
zoo
data
set
.
thi
dataset
consist
of
101
row
and
17
categor
valu
attribut
defin
whether
an
anim
ha
a
specif
properti
or
not
(
e.g.hair
,
feather
,
..
)
.
the
first
attribut
repres
the
name
of
the
anim
and
will
be
remov
.
the
target
featur
consist
of
7
integ
valu
[
1
to
7
]
which
repres
[
1
:
mammal,2
:
bird,3
:
reptile,4
:
fish,5
:
amphibian,6
:
bug,7
:
invertebr
]
though
,
befor
we
final
start
build
the
decis
tree
,
I
want
to
note
a
few
thing
:
the
intent
of
the
follow
code
is
not
to
creat
a
highli
effici
and
robust
implement
of
a
id3
decis
tree
.
for
thi
purpos
bright
head
have
creat
the
prepackag
sklearn
decis
tree
model
which
we
will
use
in
the
next
section
.
with
the
follow
code
I
want
to
to
provid
and
show
the
basic
principl
and
step
behind
creat
a
decis
tree
from
scratch
with
the
goal
that
we
can
use
the
prepackag
modul
more
effici
becaus
we
understand
and
know
what
they
are
do
and
can
eventu
,
build
our
own
machin
learn
model
.
that
said
,
there
are
four
import
step
:
the
calcul
of
the
inform
gain
the
recurs
call
of
the
treemodel
the
build
of
the
actual
tree
structur
the
speci
predict
of
a
new
unseen
animal-inst
here
the
most
critic
aspect
are
the
recurs
call
of
the
treemodel
,
the
creation
of
the
tree
itself
(
build
the
tree
structur
)
as
well
as
the
predict
of
a
unseen
queri
instanc
(
the
process
of
wander
down
the
tree
to
predict
the
class
of
a
unseen
queri
instanc
)
.
``
''
''
make
the
import
of
python
packag
need
``
''
''
import
panda
as
pd
import
numpi
as
np
from
pprint
import
pprint
#
import
the
dataset
and
defin
the
featur
as
well
as
the
target
dataset
/
column
#
dataset
=
pd.read_csv
(
'data/zoo.csv
'
,
names=
[
'animal_nam
'
,
'hair
'
,
'feather
'
,
'egg
'
,
'milk
'
,
'airbon
'
,
'aquat
'
,
'predat
'
,
'tooth
'
,
'backbon
'
,
'breath
'
,
'venom
'
,
'fin
'
,
'leg
'
,
'tail
'
,
'domest
'
,
'catsiz
'
,
'class
'
,
]
)
#
import
all
column
omit
the
fist
which
consist
the
name
of
the
anim
#
We
drop
the
anim
name
sinc
thi
is
not
a
good
featur
to
split
the
data
on
dataset=dataset.drop
(
'animal_nam
'
,
axis=1
)
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
def
entropi
(
target_col
)
:
``
''
''
calcul
the
entropi
of
a
dataset
.
the
onli
paramet
of
thi
function
is
the
target_col
paramet
which
specifi
the
target
column
``
''
''
element
,
count
=
np.uniqu
(
target_col
,
return_count
=
true
)
entropi
=
np.sum
(
[
(
-count
[
i
]
/np.sum
(
count
)
)
*np.log2
(
count
[
i
]
/np.sum
(
count
)
)
for
i
in
rang
(
len
(
element
)
)
]
)
return
entropi
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
def
infogain
(
data
,
split_attribute_nam
,
target_name=
''
class
''
)
:
``
''
''
calcul
the
inform
gain
of
a
dataset
.
thi
function
take
three
paramet
:
1.
data
=
the
dataset
for
whose
featur
the
IG
should
be
calcul
2.
split_attribute_nam
=
the
name
of
the
featur
for
which
the
inform
gain
should
be
calcul
3.
target_nam
=
the
name
of
the
target
featur
.
the
default
for
thi
exampl
is
``
class
''
``
''
''
#
calcul
the
entropi
of
the
total
dataset
total_entropi
=
entropi
(
data
[
target_nam
]
)
#
#
calcul
the
entropi
of
the
dataset
#
calcul
the
valu
and
the
correspond
count
for
the
split
attribut
val
,
counts=
np.uniqu
(
data
[
split_attribute_nam
]
,
return_counts=tru
)
#
calcul
the
weight
entropi
weighted_entropi
=
np.sum
(
[
(
count
[
i
]
/np.sum
(
count
)
)
*entropi
(
data.wher
(
data
[
split_attribute_nam
]
==val
[
i
]
)
.dropna
(
)
[
target_nam
]
)
for
i
in
rang
(
len
(
val
)
)
]
)
#
calcul
the
inform
gain
information_gain
=
total_entropi
-
weighted_entropi
return
information_gain
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
def
id3
(
data
,
originaldata
,
featur
,
target_attribute_name=
''
class
''
,
parent_node_class
=
none
)
:
``
''
''
id3
algorithm
:
thi
function
take
five
paramt
:
1.
data
=
the
data
for
which
the
id3
algorithm
should
be
run
--
>
In
the
first
run
thi
equal
the
total
dataset
2.
originaldata
=
thi
is
the
origin
dataset
need
to
calcul
the
mode
target
featur
valu
of
the
origin
dataset
in
the
case
the
dataset
deliv
by
the
first
paramet
is
empti
3.
featur
=
the
featur
space
of
the
dataset
.
thi
is
need
for
the
recurs
call
sinc
dure
the
tree
grow
process
we
have
to
remov
featur
from
our
dataset
--
>
split
at
each
node
4.
target_attribute_nam
=
the
name
of
the
target
attribut
5.
parent_node_class
=
thi
is
the
valu
or
class
of
the
mode
target
featur
valu
of
the
parent
node
for
a
specif
node
.
thi
is
also
need
for
the
recurs
call
sinc
if
the
split
lead
to
a
situat
that
there
are
no
more
featur
left
in
the
featur
space
,
we
want
to
return
the
mode
target
featur
valu
of
the
direct
parent
node.
``
''
''
#
defin
the
stop
criteria
--
>
If
one
of
thi
is
satisfi
,
we
want
to
return
a
leaf
node
#
#
If
all
target_valu
have
the
same
valu
,
return
thi
valu
if
len
(
np.uniqu
(
data
[
target_attribute_nam
]
)
)
<
=
1
:
return
np.uniqu
(
data
[
target_attribute_nam
]
)
[
0
]
#
If
the
dataset
is
empti
,
return
the
mode
target
featur
valu
in
the
origin
dataset
elif
len
(
data
)
==0
:
return
np.uniqu
(
originaldata
[
target_attribute_nam
]
)
[
np.argmax
(
np.uniqu
(
originaldata
[
target_attribute_nam
]
,
return_counts=tru
)
[
1
]
)
]
#
If
the
featur
space
is
empti
,
return
the
mode
target
featur
valu
of
the
direct
parent
node
--
>
note
that
#
the
direct
parent
node
is
that
node
which
ha
call
the
current
run
of
the
id3
algorithm
and
henc
#
the
mode
target
featur
valu
is
store
in
the
parent_node_class
variabl
.
elif
len
(
featur
)
==0
:
return
parent_node_class
#
If
none
of
the
abov
hold
true
,
grow
the
tree
!
els
:
#
set
the
default
valu
for
thi
node
--
>
the
mode
target
featur
valu
of
the
current
node
parent_node_class
=
np.uniqu
(
data
[
target_attribute_nam
]
)
[
np.argmax
(
np.uniqu
(
data
[
target_attribute_nam
]
,
return_counts=tru
)
[
1
]
)
]
#
select
the
featur
which
best
split
the
dataset
item_valu
=
[
infogain
(
data
,
featur
,
target_attribute_nam
)
for
featur
in
featur
]
#
return
the
inform
gain
valu
for
the
featur
in
the
dataset
best_feature_index
=
np.argmax
(
item_valu
)
best_featur
=
featur
[
best_feature_index
]
#
creat
the
tree
structur
.
the
root
get
the
name
of
the
featur
(
best_featur
)
with
the
maximum
inform
#
gain
in
the
first
run
tree
=
{
best_featur
:
{
}
}
#
remov
the
featur
with
the
best
inforamt
gain
from
the
featur
space
featur
=
[
i
for
i
in
featur
if
i
!
=
best_featur
]
#
grow
a
branch
under
the
root
node
for
each
possibl
valu
of
the
root
node
featur
for
valu
in
np.uniqu
(
data
[
best_featur
]
)
:
valu
=
valu
#
split
the
dataset
along
the
valu
of
the
featur
with
the
largest
inform
gain
and
therwith
creat
sub_dataset
sub_data
=
data.wher
(
data
[
best_featur
]
==
valu
)
.dropna
(
)
#
call
the
id3
algorithm
for
each
of
those
sub_dataset
with
the
new
paramet
--
>
here
the
recurs
come
in
!
subtre
=
id3
(
sub_data
,
dataset
,
featur
,
target_attribute_nam
,
parent_node_class
)
#
add
the
sub
tree
,
grown
from
the
sub_dataset
to
the
tree
under
the
root
node
tree
[
best_featur
]
[
valu
]
=
subtre
return
(
tree
)
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
def
predict
(
queri
,
tree
,
default
=
1
)
:
``
''
''
predict
of
a
new/unseen
queri
instanc
.
thi
take
two
paramet
:
1
.
the
queri
instanc
as
a
dictionari
of
the
shape
{
``
feature_nam
''
:
feature_valu
,
...
}
2
.
the
tree
We
do
thi
also
in
a
recurs
manner
.
that
is
,
we
wander
down
the
tree
and
check
if
we
have
reach
a
leaf
or
if
we
are
still
in
a
sub
tree
.
sinc
thi
is
a
import
step
to
understand
,
the
singl
step
are
extens
comment
below
.
1.check
for
everi
featur
in
the
queri
instanc
if
thi
featur
is
exist
in
the
tree.key
(
)
for
the
first
call
,
tree.key
(
)
onli
contain
the
valu
for
the
root
node
--
>
if
thi
valu
is
not
exist
,
we
can
not
make
a
predict
and
have
to
return
the
default
valu
which
is
the
major
valu
of
the
target
featur
2
.
first
of
all
we
have
to
take
care
of
a
import
fact
:
sinc
we
train
our
model
with
a
databas
A
and
then
show
our
model
a
unseen
queri
it
may
happen
that
the
featur
valu
of
these
queri
are
not
exist
in
our
tree
model
becaus
non
of
the
train
instanc
ha
had
such
a
valu
for
thi
specif
featur
.
for
instanc
imagin
the
situat
where
your
model
ha
onli
seen
anim
with
one
to
four
leg
-
the
``
leg
''
node
in
your
model
will
onli
have
four
outgo
branch
(
from
one
to
four
)
.
If
you
now
show
your
model
a
new
instanc
(
anim
)
which
ha
for
the
leg
featur
the
vale
5
,
you
have
to
tell
your
model
what
to
do
in
such
a
situat
becaus
otherwis
there
is
no
classif
possibl
becaus
in
the
classif
step
you
tri
to
run
down
the
outgo
branch
with
the
valu
5
but
there
is
no
such
a
branch
.
henc
:
error
and
no
classif
!
We
can
address
thi
issu
with
a
classif
valu
of
for
instanc
(
999
)
which
tell
us
that
there
is
no
classif
possibl
or
we
assign
the
most
frequent
target
featur
valu
of
our
dataset
use
to
train
the
model
.
Or
,
in
for
instanc
medic
applic
we
can
return
the
most
wors
case
-
just
to
make
sure
...
We
can
also
return
the
most
frequent
valu
of
the
direct
parent
node
.
To
make
a
long
stori
short
,
we
have
to
tell
the
model
what
to
do
in
thi
situat
.
In
our
exampl
,
sinc
we
are
deal
with
anim
speci
where
a
fals
classif
is
not
that
critic
,
we
will
assign
the
valu
1
which
is
the
valu
for
the
mammal
speci
(
for
conveni
)
.
3
.
address
the
key
in
the
tree
which
fit
the
valu
for
key
--
>
note
that
key
==
the
featur
in
the
queri
.
becaus
we
want
the
tree
to
predict
the
valu
which
is
hidden
under
the
key
valu
(
imagin
you
have
a
drawn
tree
model
on
the
tabl
in
front
of
you
and
you
have
a
queri
instanc
for
which
you
want
to
predict
the
target
featur
-
what
are
you
do
?
-
correct
:
you
start
at
the
root
node
and
wander
down
the
tree
compar
your
queri
to
the
node
valu
.
henc
you
want
to
have
the
valu
which
is
hidden
under
the
current
node
.
If
thi
is
a
leaf
,
perfect
,
otherwis
you
wander
the
tree
deeper
until
you
get
to
a
leaf
node
.
though
,
you
want
to
have
thi
``
someth
''
[
either
leaf
or
sub_tre
]
which
is
hidden
under
the
current
node
and
henc
we
must
address
the
node
in
the
tree
which
==
the
key
valu
from
our
queri
instanc
.
thi
is
done
with
tree
[
key
]
.
next
you
want
to
run
down
the
branch
of
thi
node
which
is
equal
to
the
valu
given
``
behind
''
the
key
valu
of
your
queri
instanc
e.g
.
if
you
find
``
leg
''
==
to
tree.key
(
)
that
is
,
for
the
first
run
==
the
root
node
.
you
want
to
run
deeper
and
therefor
you
have
to
address
the
branch
at
your
node
whose
valu
is
==
to
the
valu
behind
key
.
thi
is
done
with
queri
[
key
]
e.g
.
queri
[
key
]
==
queri
[
'leg
'
]
==
0
--
>
therewith
we
run
down
the
branch
of
the
node
with
the
valu
0
.
summar
,
in
thi
step
we
want
to
address
the
node
which
is
hidden
behind
a
specif
branch
of
the
root
node
(
in
the
first
run
)
thi
is
done
with
:
result
=
[
key
]
[
queri
[
key
]
]
4
.
As
said
in
the
2.
step
,
we
run
down
the
tree
along
node
and
branch
until
we
get
to
a
leaf
node
.
that
is
,
if
result
=
tree
[
key
]
[
queri
[
key
]
]
return
anoth
tree
object
(
we
have
repres
thi
by
a
dict
object
--
>
that
is
if
result
is
a
dict
object
)
we
know
that
we
have
not
arriv
at
a
root
node
and
have
to
run
deeper
the
tree
.
okay
...
look
at
your
drawn
tree
in
front
of
you
...
what
are
you
do
?
...
well
,
you
run
down
the
next
branch
...
exactli
as
we
have
done
it
abov
with
the
slight
differ
that
we
alreadi
have
pass
a
node
and
therewith
have
to
run
onli
a
fraction
of
the
tree
--
>
you
clever
guy
!
that
``
fraction
of
the
tree
''
is
exactli
what
we
have
store
under
'result
'
.
So
we
simpli
call
our
predict
method
use
the
same
queri
instanc
(
we
do
not
have
to
drop
ani
featur
from
the
queri
instanc
sinc
for
instanc
the
featur
for
the
root
node
will
not
be
avail
in
ani
of
the
deeper
sub_tre
and
henc
we
will
simpli
not
find
that
featur
)
as
well
as
the
``
reduc
/
sub_tre
''
store
in
result
.
summar
:
If
we
have
a
queri
instanc
consist
of
valu
for
featur
,
we
take
thi
featur
and
check
if
the
name
of
the
root
node
is
equal
to
one
of
the
queri
featur
.
If
thi
is
true
,
we
run
down
the
root
node
outgo
branch
whose
valu
equal
the
valu
of
queri
featur
==
the
root
node
.
If
we
find
at
the
end
of
thi
branch
a
leaf
node
(
not
a
dict
object
)
we
return
thi
valu
(
thi
is
our
predict
)
.
If
we
instead
find
anoth
node
(
==
sub_tre
==
dict
objct
)
we
search
in
our
queri
for
the
featur
which
equal
the
valu
of
that
node
.
next
we
look
up
the
valu
of
our
queri
featur
and
run
down
the
branch
whose
valu
is
equal
to
the
queri
[
key
]
==
queri
featur
valu
.
and
as
you
can
see
thi
is
exactli
the
recurs
we
talk
about
with
the
import
fact
that
for
each
node
we
run
down
the
tree
,
we
check
onli
the
node
and
branch
which
are
below
thi
node
and
do
not
run
the
whole
tree
begin
at
the
root
node
--
>
thi
is
whi
we
re-cal
the
classif
function
with
'result
'
``
''
''
#
1.
for
key
in
list
(
query.key
(
)
)
:
if
key
in
list
(
tree.key
(
)
)
:
#
2.
tri
:
result
=
tree
[
key
]
[
queri
[
key
]
]
except
:
return
default
#
3.
result
=
tree
[
key
]
[
queri
[
key
]
]
#
4.
if
isinst
(
result
,
dict
)
:
return
predict
(
queri
,
result
)
els
:
return
result
``
''
''
check
the
accuraci
of
our
predict
.
the
train_test_split
function
take
the
dataset
as
paramet
which
should
be
divid
into
a
train
and
a
test
set
.
the
test
function
take
two
paramet
,
which
are
the
test
data
as
well
as
the
tree
model.
``
''
''
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
def
train_test_split
(
dataset
)
:
training_data
=
dataset.iloc
[
:80
]
.reset_index
(
drop=tru
)
#
We
drop
the
index
respect
relabel
the
index
#
start
form
0
,
becaus
we
do
not
want
to
run
into
error
regard
the
row
label
/
index
testing_data
=
dataset.iloc
[
80
:
]
.reset_index
(
drop=tru
)
return
training_data
,
testing_data
training_data
=
train_test_split
(
dataset
)
[
0
]
testing_data
=
train_test_split
(
dataset
)
[
1
]
def
test
(
data
,
tree
)
:
#
creat
new
queri
instanc
by
simpli
remov
the
target
featur
column
from
the
origin
dataset
and
#
convert
it
to
a
dictionari
queri
=
data.iloc
[
:
,
:
-1
]
.to_dict
(
orient
=
``
record
''
)
#
creat
a
empti
datafram
in
whose
column
the
predict
of
the
tree
are
store
predict
=
pd.datafram
(
columns=
[
``
predict
''
]
)
#
calcul
the
predict
accuraci
for
i
in
rang
(
len
(
data
)
)
:
predicted.loc
[
i
,
''
predict
''
]
=
predict
(
queri
[
i
]
,
tree,1.0
)
print
(
'the
predict
accuraci
is
:
'
,
(
np.sum
(
predict
[
``
predict
''
]
==
data
[
``
class
''
]
)
/len
(
data
)
)
*100
,
'
%
'
)
``
''
''
train
the
tree
,
print
the
tree
and
predict
the
accuraci
``
''
''
tree
=
id3
(
training_data
,
training_data
,
training_data.column
[
:
-1
]
)
pprint
(
tree
)
test
(
testing_data
,
tree
)
{
'leg
'
:
{
'
0
'
:
{
'fin
'
:
{
'
0
'
:
{
'tooth
'
:
{
'
0
'
:
'
7
'
,
'
1
'
:
'
3
'
}
}
,
'
1
'
:
{
'egg
'
:
{
'
0
'
:
'
1
'
,
'
1
'
:
'
4
'
}
}
}
}
,
'
2
'
:
{
'hair
'
:
{
'
0
'
:
'
2
'
,
'
1
'
:
'
1
'
}
}
,
'
4
'
:
{
'hair
'
:
{
'
0
'
:
{
'tooth
'
:
{
'
0
'
:
'
7
'
,
'
1
'
:
'
5
'
}
}
,
'
1
'
:
'
1
'
}
}
,
'
6
'
:
{
'aquat
'
:
{
'
0
'
:
'
6
'
,
'
1
'
:
'
7
'
}
}
,
'
8
'
:
'
7
'
,
'leg
'
:
'class_typ
'
}
}
the
predict
accuraci
is
:
86.36363636363636
%
As
we
can
see
,
the
predict
accuraci
for
the
zoo
dataset
is
about
86
%
which
is
actual
not
that
bad
consid
that
we
do
n't
have
done
ani
improv
like
for
instanc
defin
a
minim
split
size
or
a
minim
amount
of
instanc
per
leaf
or
bag
or
boost
,
or
prune
,
etc
.
decis
tree
use
sklearn
even
if
the
abov
code
is
suitabl
and
import
to
convey
the
concept
of
decis
tree
as
well
as
how
to
implement
a
classif
tree
model
``
from
scratch
''
,
there
is
a
veri
power
decis
tree
classif
model
implement
in
sklearn
sklearn.tree.decisiontreeclassifierÂ¶
.
thank
to
thi
model
we
can
implement
a
tree
model
faster
,
more
effici
and
also
neater
as
we
can
do
it
in
just
a
few
line
of
code
.
the
step
to
use
the
sklearn
classif
decis
tree
follow
the
princip
sklearn
api
which
are
:
choos
the
model
you
want
to
use
--
>
the
decisiontreeclassifi
set
the
model
hyperparamet
--
>
e.g
.
number
of
minimum
sampl
per
leaf
creat
a
featur
data
set
as
well
as
a
target
array
contain
the
label
for
the
instanc
fit
the
model
to
the
train
data
use
the
fit
model
on
unseen
data
.
that
it
!
As
alway
,
the
step
are
straight
forward.
``
''
''
import
the
decisiontreeclassifi
model.
``
''
''
#
import
the
decisiontreeclassifi
from
sklearn.tre
import
decisiontreeclassifi
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
``
''
''
import
the
zoo
dataset
``
''
''
#
import
the
dataset
dataset
=
pd.read_csv
(
'data/zoo.csv
'
)
#
We
drop
the
anim
name
sinc
thi
is
not
a
good
featur
to
split
the
data
on
dataset=dataset.drop
(
'animal_nam
'
,
axis=1
)
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
``
''
''
split
the
data
into
a
train
and
a
test
set
``
''
''
train_featur
=
dataset.iloc
[
:80
,
:-1
]
test_featur
=
dataset.iloc
[
80
:
,
:
-1
]
train_target
=
dataset.iloc
[
:80
,
-1
]
test_target
=
dataset.iloc
[
80
:
,-1
]
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
``
''
''
train
the
model
``
''
''
tree
=
decisiontreeclassifi
(
criterion
=
'entropi
'
)
.fit
(
train_featur
,
train_target
)
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
``
''
''
predict
the
class
of
new
,
unseen
data
``
''
''
predict
=
tree.predict
(
test_featur
)
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
``
''
''
check
the
accuraci
``
''
''
print
(
``
the
predict
accuraci
is
:
``
,
tree.scor
(
test_featur
,
test_target
)
*100
,
''
%
''
)
the
predict
accuraci
is
:
80.95238095238095
%
cool
is
n't
it
?
well
,
the
accuraci
is
not
that
mind
blow
but
thi
is
more
like
due
to
the
composit
of
the
data
itself
as
due
to
the
model
.
feel
free
to
tri
differ
model
paramet
to
improv
the
accuraci
of
the
model
.
advantg
and
disadvantag
of
decis
tree
sinc
we
now
have
seen
how
a
decis
tree
classif
model
is
program
in
python
by
hand
and
and
by
use
a
prepackag
sklearn
model
we
will
consid
the
main
advantag
and
disadvantag
of
decis
tree
in
gener
,
that
is
not
onli
of
classif
decis
tree
.
advantag
white
box
,
easi
to
interpret
model
No
featur
normal
need
tree
model
can
handl
both
continu
and
categor
data
(
classif
and
regress
tree
)
can
model
nonlinear
relationship
can
model
interact
between
the
differ
descript
featur
disadvantag
If
continu
featur
are
use
the
tree
may
becom
quit
larg
and
henc
less
interpret
decis
tree
are
prone
to
overfit
the
train
data
and
henc
do
not
well
gener
the
data
if
no
stop
criteria
or
improv
like
prune
,
boost
or
bag
are
implement
small
chang
in
the
data
may
lead
to
a
complet
differ
tree
.
thi
issu
can
be
address
by
use
ensembl
method
like
bag
,
boost
or
random
forest
unbalanc
dataset
where
some
target
featur
valu
occur
much
more
frequent
than
other
may
lead
to
bias
tree
sinc
the
frequent
occur
featur
valu
are
prefer
over
the
less
frequent
occur
one
.
facilit
:
there
are
in
gener
three
case
whi
we
want
to
grow
a
leaf
node
:
If
there
are
onli
pure
target
featur
valu
in
a
sub_set
--
>
We
return
thi
valu
;
If
the
sub_dataset
is
empti
--
>
We
return
the
mode
valu
of
the
origin
dataset
;
If
there
are
no
featur
left
in
the
sub_dataset
--
>
We
return
the
mode
valu
of
the
parent
node
.
If
we
have
now
one
target
featur
valu
whose
frequenc
top
all
other
frequenc
,
it
is
clear
whi
the
outcom
may
be
bias
toward
thi
valu
.
We
can
address
thi
by
ensur
that
the
dataset
is
rel
balanc
in
term
of
the
target
featur
valu
If
the
number
of
featur
is
rel
larg
(
high
dimension
)
and
the
number
of
instanc
is
rel
low
,
the
tree
might
overfit
the
data
featur
with
mani
level
may
be
prefer
over
featur
with
less
level
sinc
for
them
it
is
``
more
easi
''
to
split
the
dataset
such
that
the
sub_dataset
onli
contain
pure
target
featur
valu
.
thi
issu
can
be
address
by
prefer
for
instanc
the
inform
gain
ratio
as
split
criteria
over
inform
gain
when
we
illustr
the
target
featur
split
process
,
we
see
that
the
tree
model
kind
of
categor
the
target
featur
class
into
rectangular
region
.
henc
the
tree
model
assum
that
the
underli
data
can
be
split
respect
repres
by
these
rectangular
region
.
further
issu
and
variat
the
first
thing
which
ha
not
been
shown
abov
is
how
to
grow
a
tree
when
the
descript
featur
are
not
categor
but
continu
scale
.
thi
doe
not
chang
much
from
the
abov
approach
with
the
larg
differ
that
we
can
use
a
continu
scale
featur
multipl
time
dure
the
grow
of
the
tree
and
we
have
to
use
the
mean
or
mode
of
a
featur
regard
the
valu
of
the
target
featur
instead
of
the
singl
(
categor
)
featur
valu
--
>
these
can
no
loner
be
use
sinc
there
is
now
a
infinit
number
of
differ
possibl
valu
.
the
second
import
variat
is
when
we
do
no
longer
have
a
categor
scale
but
continu
scale
target
featur
.
If
thi
is
the
case
we
call
the
tree
model
a
regress
tree
model
instead
of
a
classif
tree
model
.
here
as
one
exampl
we
can
use
the
varianc
of
a
featur
regard
the
target
featur
as
split
criteria
instead
of
the
inform
gain
.
We
then
use
the
featur
with
the
lowest
weight
varianc
as
split
featur
.
We
said
abov
that
decis
tree
are
prone
to
overfit
the
train
data
.
We
also
mention
that
thi
issu
can
be
address
use
a
method
call
prune
.
and
it
is
exactli
what
it
sound
like
.
We
prune
the
tree
.
therefor
we
start
at
the
leaf
node
and
simpli
check
if
the
accuraci
grow
if
we
prune
the
leaf
and
replac
the
parent
node
of
these
leaf
by
a
leaf
node
repres
the
mode
target
featur
valu
for
thi
node
.
follow
thi
procedur
we
wander
up
the
tree
until
the
prune
will
not
lead
to
a
higher
accuraci
or
until
the
prune
doe
not
reduc
the
accuraci
.
To
make
a
long
stori
short
,
if
prune
doe
not
reduc
the
accuraci
,
prune
.
done
.
We
have
found
the
tree
which
result
in
the
maximum
accuraci
regard
our
test
data
set
.
anoth
approach
to
increas
the
accuraci
of
a
tree
model
is
to
use
an
ensembl
approach
.
with
an
ensembl
approach
we
creat
differ
model
(
in
thi
case
)
tree
from
the
origin
dataset
and
let
the
differ
model
make
a
major
vote
on
the
test
dataset
.
that
is
,
we
predict
the
target
valu
for
the
test
dataset
use
each
of
the
creat
model
and
then
return
thi
target
featur
valu
which
ha
been
predict
by
the
major
of
the
model
.
the
most
promin
approach
to
creat
decis
tree
ensembl
model
are
call
bag
and
boost
.
A
variant
of
a
boosting-bas
decis
tree
ensembl
model
is
call
random
forest
model
which
is
one
of
the
most
power
machin
learn
algorithm
.
ensembl
model
can
also
be
creat
by
use
differ
split
criteria
for
the
singl
model
such
as
the
gini
index
as
well
as
the
inform
gain
ratio
.
We
have
now
seen
a
lot
of
variat
and
differ
approach
to
decis
tree
model
.
though
,
there
is
no
gener
guidelin
on
which
approach
should
be
use
.
-there
is
no
free
lunch-
As
often
,
it
depend
on
...
and
the
onli
real
advic
which
can
be
given
is
that
you
have
to
tri
differ
model
with
differ
hyperparamet
to
find
the
best
fit
model
for
a
specif
problem
.
nevertheless
,
ensembl
model
such
as
the
random
forest
algorithm
have
proven
as
veri
power
model
.
In
the
follow
chapter
we
will
address
some
of
the
abov
mention
variat
to
get
a
deeper
understand
of
decis
tree
.
refer
:
http
:
//www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/
http
:
//nbviewer.jupyter.org/gist/jwdink/9715a1a30e8c7f50a572
john
D.
kelleh
,
brian
mac
name
,
aoif
d'arci
,
2015
.
machin
learn
for
predicti
data
analyt
.
cambridg
,
massachusett
:
the
mit
press
.
lior
rokach
,
ode
maimon
,
2015
.
data
mine
with
decis
tree
.
2nd
Ed
.
ben-gurion
,
israel
,
tel-aviv
,
israel
:
wolrd
scientif
.
tom
M.
mitchel
,
1997
.
machin
learn
.
new
york
,
NY
,
usa
:
mcgraw-hil
.
previou
chapter
:
python
implement
of
text
classif
next
chapter
:
regress
tree
Â©
2011
-
2018
,
bernd
klein
,
bodenseo
;
design
by
denis
mitchinson
adapt
for
python-course.eu
by
bernd
klein
