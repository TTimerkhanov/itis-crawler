machin
learn
with
python
:
train
and
test
the
neural
network
with
mnist
data
set
python
machin
learn
tutori
machin
learn
machin
learn
terminologyk-nearest
neighbor
classifierneur
network
from
scratch
in
pythonneur
network
in
python
use
numypybackpropag
in
neural
networksconfus
matrixtrain
and
test
with
mnistdropout
neural
networksneur
network
with
scikitmachin
learn
with
scikit
and
pythonintroduct
naiv
bay
classifierna
bay
classifi
with
scikitintroduct
into
text
classif
use
naiv
bayespython
implement
of
text
classificationdecis
treesregress
treesrandom
forestsboost
algorithmprincip
compon
analysislinear
discrimin
analysisexpect
maxim
and
gaussian
mixtur
modelintroduct
into
tensorflow
quot
“
artifici
intellig
would
be
the
ultim
version
of
googl
.
the
ultim
search
engin
that
would
understand
everyth
on
the
web
.
It
would
understand
exactli
what
you
want
,
and
it
would
give
you
the
right
thing
.
We
’
re
nowher
near
do
that
now
.
howev
,
we
can
get
increment
closer
to
that
,
and
that
is
basic
what
we
work
on.
”
(
larri
wall
)
“
machin
intellig
is
the
last
invent
that
human
will
ever
need
to
make.
”
(
nick
bostrom
)
thi
websit
is
creat
and
maintain
by
:
bernd
klein
,
On
site
train
in
europ
,
canada
and
the
US
.
thi
websit
is
free
of
annoy
ad
.
We
want
to
keep
it
like
thi
.
you
can
help
with
your
donat
:
the
need
for
donat
bernd
klein
on
facebook
search
thi
websit
:
classroom
train
cours
thi
websit
contain
a
free
and
extens
onlin
tutori
by
bernd
klein
,
use
materi
from
hi
classroom
python
train
cours
.
If
you
are
interest
in
an
instructor-l
classroom
train
cours
,
you
may
have
a
look
at
the
python
class
by
bernd
klein
at
bodenseo
.
©
kabliczech
-
fotolia.com
quot
of
the
day
:
''
some
program
languag
manag
to
absorb
chang
,
but
withstand
progress.
``
(
alan
perli
)
If
you
have
the
choic
work
with
python
2
or
python
3
,
we
recomend
to
switch
to
python
3
!
you
can
read
our
python
tutori
to
see
what
the
differ
are
.
data
protect
declar
data
protect
declar
previou
chapter
:
confus
matrix
next
chapter
:
dropout
neural
network
neural
network
test
with
mnist
the
mnist
databas
(
modifi
nation
institut
of
standard
and
technolog
databas
)
of
handwritten
digit
consist
of
a
train
set
of
60,000
exampl
,
and
a
test
set
of
10,000
exampl
.
It
is
a
subset
of
a
larger
set
avail
from
nist
.
addit
,
the
black
and
white
imag
from
nist
were
size-norm
and
center
to
fit
into
a
28x28
pixel
bound
box
and
anti-alias
,
which
introduc
grayscal
level
.
thi
databas
is
well
like
for
train
and
test
in
the
field
of
machin
learn
and
imag
process
.
It
is
a
remix
subset
of
the
origin
nist
dataset
.
one
half
of
the
60,000
train
imag
consist
of
imag
from
nist
's
test
dataset
and
the
other
half
from
nist
's
train
set
.
the
10,000
imag
from
the
test
set
are
similarli
assembl
.
the
mnist
dataset
is
use
by
research
to
test
and
compar
their
research
result
with
other
.
the
lowest
error
rate
in
literatur
are
as
low
as
0.21
percent.1
read
the
mnist
data
setth
imag
from
the
data
set
have
the
size
28
x
28
.
they
are
save
in
the
csv
data
file
mnist_train.csv
and
mnist_test.csv
.
everi
line
of
these
file
consist
of
an
imag
,
i.e
.
785
number
between
0
and
255
.
the
first
number
of
each
line
is
the
label
,
i.e
.
the
digit
which
is
depict
in
the
imag
.
the
follow
784
number
are
the
pixel
of
the
28
x
28
imag
.
%
matplotlib
inlin
import
numpi
as
np
import
matplotlib.pyplot
as
plt
image_s
=
28
#
width
and
length
no_of_different_label
=
10
#
i.e
.
0
,
1
,
2
,
3
,
...
,
9
image_pixel
=
image_s
*
image_s
data_path
=
``
data/mnist/
''
train_data
=
np.loadtxt
(
data_path
+
``
mnist_train.csv
''
,
delimiter=
''
,
''
)
test_data
=
np.loadtxt
(
data_path
+
``
mnist_test.csv
''
,
delimiter=
''
,
''
)
We
map
the
valu
of
the
imag
data
into
the
interv
[
0.01
,
0.99
]
by
divid
the
train_data
and
test_data
array
by
(
255
*
0.99
+
0.01
)
thi
way
,
we
have
input
valu
between
0
and
1
but
not
includ
0
and
1.
fac
=
255
*0.99
+
0.01
train_img
=
np.asfarray
(
train_data
[
:
,
1
:
]
)
/
fac
test_img
=
np.asfarray
(
test_data
[
:
,
1
:
]
)
/
fac
train_label
=
np.asfarray
(
train_data
[
:
,
:1
]
)
test_label
=
np.asfarray
(
test_data
[
:
,
:1
]
)
We
need
the
label
in
our
calcul
in
a
one-hot
represent
.
We
have
10
digit
from
0
to
9
,
i.e
.
lr
=
np.arang
(
10
)
.
turn
a
label
into
one-hot
represent
can
be
achiev
with
the
command
:
(
lr==label
)
.astyp
(
np.int
)
We
demonstr
thi
in
the
follow
:
import
numpi
as
np
lr
=
np.arang
(
10
)
for
label
in
rang
(
10
)
:
one_hot
=
(
lr==label
)
.astyp
(
np.int
)
print
(
``
label
:
``
,
label
,
``
in
one-hot
represent
:
``
,
one_hot
)
label
:
0
in
one-hot
represent
:
[
1
0
0
0
0
0
0
0
0
0
]
label
:
1
in
one-hot
represent
:
[
0
1
0
0
0
0
0
0
0
0
]
label
:
2
in
one-hot
represent
:
[
0
0
1
0
0
0
0
0
0
0
]
label
:
3
in
one-hot
represent
:
[
0
0
0
1
0
0
0
0
0
0
]
label
:
4
in
one-hot
represent
:
[
0
0
0
0
1
0
0
0
0
0
]
label
:
5
in
one-hot
represent
:
[
0
0
0
0
0
1
0
0
0
0
]
label
:
6
in
one-hot
represent
:
[
0
0
0
0
0
0
1
0
0
0
]
label
:
7
in
one-hot
represent
:
[
0
0
0
0
0
0
0
1
0
0
]
label
:
8
in
one-hot
represent
:
[
0
0
0
0
0
0
0
0
1
0
]
label
:
9
in
one-hot
represent
:
[
0
0
0
0
0
0
0
0
0
1
]
We
are
readi
now
to
turn
our
label
imag
into
one-hot
represent
.
instead
of
zero
and
one
,
we
creat
0.01
and
0.99
,
which
will
be
better
for
our
calcul
:
lr
=
np.arang
(
no_of_different_label
)
#
transform
label
into
one
hot
represent
train_labels_one_hot
=
(
lr==train_label
)
.astyp
(
np.float
)
test_labels_one_hot
=
(
lr==test_label
)
.astyp
(
np.float
)
#
we
do
n't
want
zero
and
one
in
the
label
neither
:
train_labels_one_hot
[
train_labels_one_hot==0
]
=
0.01
train_labels_one_hot
[
train_labels_one_hot==1
]
=
0.99
test_labels_one_hot
[
test_labels_one_hot==0
]
=
0.01
test_labels_one_hot
[
test_labels_one_hot==1
]
=
0.99
befor
we
start
use
the
mnist
data
set
with
our
neural
network
,
we
will
have
a
look
at
same
imag
:
for
i
in
rang
(
10
)
:
img
=
train_img
[
i
]
.reshap
(
(
28,28
)
)
plt.imshow
(
img
,
cmap=
''
grey
''
)
plt.show
(
)
dump
the
data
for
faster
reloady
may
have
notic
that
it
is
quit
slow
to
read
in
the
data
from
the
csv
file
.
We
will
save
the
data
in
binari
format
with
the
dump
function
from
the
pickl
modul
:
import
pickl
with
open
(
``
data/mnist/pickled_mnist.pkl
''
,
``
bw
''
)
as
fh
:
data
=
(
train_img
,
test_img
,
train_label
,
test_label
,
train_labels_one_hot
,
test_labels_one_hot
)
pickle.dump
(
data
,
fh
)
We
are
abl
now
to
read
in
the
data
by
use
pickle.load
.
thi
is
a
lot
faster
than
use
loadtxt
on
the
csv
file
:
import
pickl
with
open
(
``
data/mnist/pickled_mnist.pkl
''
,
``
br
''
)
as
fh
:
data
=
pickle.load
(
fh
)
train_img
=
data
[
0
]
test_img
=
data
[
1
]
train_label
=
data
[
2
]
test_label
=
data
[
3
]
train_labels_one_hot
=
data
[
4
]
test_labels_one_hot
=
data
[
5
]
image_s
=
28
#
width
and
length
no_of_different_label
=
10
#
i.e
.
0
,
1
,
2
,
3
,
...
,
9
image_pixel
=
image_s
*
image_s
classifi
the
dataw
will
use
the
follow
neuron
network
class
for
our
first
classif
:
import
numpi
as
np
@
np.vector
def
sigmoid
(
x
)
:
return
1
/
(
1
+
np.e
**
-x
)
activation_funct
=
sigmoid
from
scipy.stat
import
truncnorm
def
truncated_norm
(
mean=0
,
sd=1
,
low=0
,
upp=10
)
:
return
truncnorm
(
(
low
-
mean
)
/
sd
,
(
upp
-
mean
)
/
sd
,
loc=mean
,
scale=sd
)
class
neuralnetwork
:
def
__init__
(
self
,
no_of_in_nod
,
no_of_out_nod
,
no_of_hidden_nod
,
learning_r
)
:
self.no_of_in_nod
=
no_of_in_nod
self.no_of_out_nod
=
no_of_out_nod
self.no_of_hidden_nod
=
no_of_hidden_nod
self.learning_r
=
learning_r
self.create_weight_matric
(
)
def
create_weight_matric
(
self
)
:
``
''
''
A
method
to
initi
the
weight
matric
of
the
neural
network
``
''
''
rad
=
1
/
np.sqrt
(
self.no_of_in_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.wih
=
x.rv
(
(
self.no_of_hidden_nod
,
self.no_of_in_nod
)
)
rad
=
1
/
np.sqrt
(
self.no_of_hidden_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.who
=
x.rv
(
(
self.no_of_out_nod
,
self.no_of_hidden_nod
)
)
def
train
(
self
,
input_vector
,
target_vector
)
:
``
''
''
input_vector
and
target_vector
can
be
tupl
,
list
or
ndarray
``
''
''
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
target_vector
=
np.array
(
target_vector
,
ndmin=2
)
.T
output_vector1
=
np.dot
(
self.wih
,
input_vector
)
output_hidden
=
activation_funct
(
output_vector1
)
output_vector2
=
np.dot
(
self.who
,
output_hidden
)
output_network
=
activation_funct
(
output_vector2
)
output_error
=
target_vector
-
output_network
#
updat
the
weight
:
tmp
=
output_error
*
output_network
\
*
(
1.0
-
output_network
)
tmp
=
self.learning_r
*
np.dot
(
tmp
,
output_hidden.t
)
self.who
+=
tmp
#
calcul
hidden
error
:
hidden_error
=
np.dot
(
self.who.t
,
output_error
)
#
updat
the
weight
:
tmp
=
hidden_error
*
output_hidden
*
\
(
1.0
-
output_hidden
)
self.wih
+=
self.learning_r
\
*
np.dot
(
tmp
,
input_vector.t
)
def
run
(
self
,
input_vector
)
:
#
input_vector
can
be
tupl
,
list
or
ndarray
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
output_vector
=
np.dot
(
self.wih
,
input_vector
)
output_vector
=
activation_funct
(
output_vector
)
output_vector
=
np.dot
(
self.who
,
output_vector
)
output_vector
=
activation_funct
(
output_vector
)
return
output_vector
def
confusion_matrix
(
self
,
data_array
,
label
)
:
cm
=
np.zero
(
(
10
,
10
)
,
int
)
for
i
in
rang
(
len
(
data_array
)
)
:
re
=
self.run
(
data_array
[
i
]
)
res_max
=
res.argmax
(
)
target
=
label
[
i
]
[
0
]
cm
[
res_max
,
int
(
target
)
]
+=
1
return
cm
def
precis
(
self
,
label
,
confusion_matrix
)
:
col
=
confusion_matrix
[
:
,
label
]
return
confusion_matrix
[
label
,
label
]
/
col.sum
(
)
def
recal
(
self
,
label
,
confusion_matrix
)
:
row
=
confusion_matrix
[
label
,
:
]
return
confusion_matrix
[
label
,
label
]
/
row.sum
(
)
def
evalu
(
self
,
data
,
label
)
:
correct
,
wrong
=
0
,
0
for
i
in
rang
(
len
(
data
)
)
:
re
=
self.run
(
data
[
i
]
)
res_max
=
res.argmax
(
)
if
res_max
==
label
[
i
]
:
correct
+=
1
els
:
wrong
+=
1
return
correct
,
wrong
ann
=
neuralnetwork
(
no_of_in_nod
=
image_pixel
,
no_of_out_nod
=
10
,
no_of_hidden_nod
=
100
,
learning_r
=
0.1
)
for
i
in
rang
(
len
(
train_img
)
)
:
ann.train
(
train_img
[
i
]
,
train_labels_one_hot
[
i
]
)
for
i
in
rang
(
20
)
:
re
=
ann.run
(
test_img
[
i
]
)
print
(
test_label
[
i
]
,
np.argmax
(
re
)
,
np.max
(
re
)
)
[
7
.
]
7
0.994814077597
[
2
.
]
2
0.603771682531
[
1
.
]
1
0.982015463842
[
0
.
]
0
0.990292276621
[
4
.
]
4
0.937906824567
[
1
.
]
1
0.981472191404
[
4
.
]
4
0.987517056632
[
9
.
]
9
0.9842185187
[
5
.
]
6
0.420854281372
[
9
.
]
9
0.984952163258
[
0
.
]
0
0.961267648873
[
6
.
]
6
0.917602299303
[
9
.
]
9
0.987062413114
[
0
.
]
0
0.992667140202
[
1
.
]
1
0.981738961657
[
5
.
]
5
0.946351366465
[
9
.
]
9
0.988766822771
[
7
.
]
7
0.994488061024
[
3
.
]
3
0.72374375894
[
4
.
]
4
0.993429959931
correct
,
wrong
=
ann.evalu
(
train_img
,
train_label
)
print
(
``
accruraci
train
:
``
,
correct
/
(
correct
+
wrong
)
)
correct
,
wrong
=
ann.evalu
(
test_img
,
test_label
)
print
(
``
accruraci
:
test
''
,
correct
/
(
correct
+
wrong
)
)
cm
=
ann.confusion_matrix
(
train_img
,
train_label
)
print
(
cm
)
for
i
in
rang
(
10
)
:
print
(
``
digit
:
``
,
i
,
``
precis
:
``
,
ann.precis
(
i
,
cm
)
,
``
recal
:
``
,
ann.recal
(
i
,
cm
)
)
accruraci
train
:
0.94945
accruraci
:
test
0.9491
[
[
5816
1
48
18
16
57
46
19
27
34
]
[
1
6630
53
23
13
27
14
74
93
10
]
[
3
22
5507
70
14
15
4
51
21
5
]
[
5
28
99
5795
2
110
1
27
73
64
]
[
9
13
54
6
5480
41
8
39
23
66
]
[
4
1
5
52
1
4948
35
1
6
5
]
[
26
2
44
19
53
83
5782
4
27
2
]
[
0
10
38
40
4
6
0
5822
1
28
]
[
48
22
96
62
9
65
28
25
5478
26
]
[
11
13
14
46
250
69
0
203
102
5709
]
]
digit
:
0
precis
:
0.981934830322
recal
:
0.956264386715
digit
:
1
precis
:
0.983387718778
recal
:
0.955606803113
digit
:
2
precis
:
0.924303457536
recal
:
0.964110644258
digit
:
3
precis
:
0.945196542163
recal
:
0.934074790458
digit
:
4
precis
:
0.938034919548
recal
:
0.954870186444
digit
:
5
precis
:
0.912746725696
recal
:
0.978252273626
digit
:
6
precis
:
0.977019263265
recal
:
0.956967891427
digit
:
7
precis
:
0.929289704709
recal
:
0.978651874265
digit
:
8
precis
:
0.936250213639
recal
:
0.934971838198
digit
:
9
precis
:
0.959657085224
recal
:
0.889668069191
multipl
runsw
can
repeat
the
train
multipl
time
.
each
run
is
call
an
``
epoch
''
.
epoch
=
3
NN
=
neuralnetwork
(
no_of_in_nod
=
image_pixel
,
no_of_out_nod
=
10
,
no_of_hidden_nod
=
100
,
learning_r
=
0.1
)
for
epoch
in
rang
(
epoch
)
:
print
(
``
epoch
:
``
,
epoch
)
for
i
in
rang
(
len
(
train_img
)
)
:
nn.train
(
train_img
[
i
]
,
train_labels_one_hot
[
i
]
)
correct
,
wrong
=
nn.evalu
(
train_img
,
train_label
)
print
(
``
accruraci
train
:
``
,
correct
/
(
correct
+
wrong
)
)
correct
,
wrong
=
nn.evalu
(
test_img
,
test_label
)
print
(
``
accruraci
:
test
''
,
correct
/
(
correct
+
wrong
)
)
epoch
:
0
accruraci
train
:
0.9478
accruraci
:
test
0.9468
epoch
:
1
accruraci
train
:
0.9626333333333333
accruraci
:
test
0.9589
epoch
:
2
accruraci
train
:
0.96765
accruraci
:
test
0.9625
We
want
to
do
the
multipl
train
of
the
train
set
insid
of
our
network
.
To
thi
purpos
we
rewrit
the
method
train
and
add
a
method
train_singl
.
train_singl
is
more
or
less
what
we
call
'train
'
befor
.
wherea
the
new
'train
'
method
is
do
the
epoch
count
.
for
test
purpos
,
we
save
the
weight
matric
after
each
epoch
in
the
list
intermediate_weight
.
thi
list
is
return
as
the
output
of
train
:
import
numpi
as
np
@
np.vector
def
sigmoid
(
x
)
:
return
1
/
(
1
+
np.e
**
-x
)
activation_funct
=
sigmoid
from
scipy.stat
import
truncnorm
def
truncated_norm
(
mean=0
,
sd=1
,
low=0
,
upp=10
)
:
return
truncnorm
(
(
low
-
mean
)
/
sd
,
(
upp
-
mean
)
/
sd
,
loc=mean
,
scale=sd
)
class
neuralnetwork
:
def
__init__
(
self
,
no_of_in_nod
,
no_of_out_nod
,
no_of_hidden_nod
,
learning_r
)
:
self.no_of_in_nod
=
no_of_in_nod
self.no_of_out_nod
=
no_of_out_nod
self.no_of_hidden_nod
=
no_of_hidden_nod
self.learning_r
=
learning_r
self.create_weight_matric
(
)
def
create_weight_matric
(
self
)
:
``
''
''
A
method
to
initi
the
weight
matric
of
the
neural
network
''
''
''
rad
=
1
/
np.sqrt
(
self.no_of_in_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.wih
=
x.rv
(
(
self.no_of_hidden_nod
,
self.no_of_in_nod
)
)
rad
=
1
/
np.sqrt
(
self.no_of_hidden_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.who
=
x.rv
(
(
self.no_of_out_nod
,
self.no_of_hidden_nod
)
)
def
train_singl
(
self
,
input_vector
,
target_vector
)
:
``
''
''
input_vector
and
target_vector
can
be
tupl
,
list
or
ndarray
``
''
''
output_vector
=
[
]
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
target_vector
=
np.array
(
target_vector
,
ndmin=2
)
.T
output_vector1
=
np.dot
(
self.wih
,
input_vector
)
output_hidden
=
activation_funct
(
output_vector1
)
output_vector2
=
np.dot
(
self.who
,
output_hidden
)
output_network
=
activation_funct
(
output_vector2
)
output_error
=
target_vector
-
output_network
#
updat
the
weight
:
tmp
=
output_error
*
output_network
*
\
(
1.0
-
output_network
)
tmp
=
self.learning_r
*
np.dot
(
tmp
,
output_hidden.t
)
self.who
+=
tmp
#
calcul
hidden
error
:
hidden_error
=
np.dot
(
self.who.t
,
output_error
)
#
updat
the
weight
:
tmp
=
hidden_error
*
output_hidden
*
(
1.0
-
output_hidden
)
self.wih
+=
self.learning_r
*
np.dot
(
tmp
,
input_vector.t
)
def
train
(
self
,
data_array
,
labels_one_hot_array
,
epochs=1
,
intermediate_results=fals
)
:
intermediate_weight
=
[
]
for
epoch
in
rang
(
epoch
)
:
print
(
``
*
''
,
end=
''
''
)
for
i
in
rang
(
len
(
data_array
)
)
:
self.train_singl
(
data_array
[
i
]
,
labels_one_hot_array
[
i
]
)
if
intermediate_result
:
intermediate_weights.append
(
(
self.wih.copi
(
)
,
self.who.copi
(
)
)
)
return
intermediate_weight
def
confusion_matrix
(
self
,
data_array
,
label
)
:
cm
=
{
}
for
i
in
rang
(
len
(
data_array
)
)
:
re
=
self.run
(
data_array
[
i
]
)
res_max
=
res.argmax
(
)
target
=
label
[
i
]
[
0
]
if
(
target
,
res_max
)
in
cm
:
cm
[
(
target
,
res_max
)
]
+=
1
els
:
cm
[
(
target
,
res_max
)
]
=
1
return
cm
def
run
(
self
,
input_vector
)
:
``
''
''
input_vector
can
be
tupl
,
list
or
ndarray
``
''
''
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
output_vector
=
np.dot
(
self.wih
,
input_vector
)
output_vector
=
activation_funct
(
output_vector
)
output_vector
=
np.dot
(
self.who
,
output_vector
)
output_vector
=
activation_funct
(
output_vector
)
return
output_vector
def
evalu
(
self
,
data
,
label
)
:
correct
,
wrong
=
0
,
0
for
i
in
rang
(
len
(
data
)
)
:
re
=
self.run
(
data
[
i
]
)
res_max
=
res.argmax
(
)
if
res_max
==
label
[
i
]
:
correct
+=
1
els
:
wrong
+=
1
return
correct
,
wrong
epoch
=
10
ann
=
neuralnetwork
(
no_of_in_nod
=
image_pixel
,
no_of_out_nod
=
10
,
no_of_hidden_nod
=
100
,
learning_r
=
0.15
)
weight
=
ann.train
(
train_img
,
train_labels_one_hot
,
epochs=epoch
,
intermediate_results=tru
)
**********
cm
=
ann.confusion_matrix
(
train_img
,
train_label
)
print
(
ann.run
(
train_img
[
i
]
)
)
[
[
1.26008523e-02
]
[
1.36319944e-02
]
[
5.93165174e-03
]
[
5.65000996e-03
]
[
2.72212976e-03
]
[
3.26118452e-03
]
[
4.16713243e-04
]
[
1.03747031e-03
]
[
9.29099059e-01
]
[
8.70810282e-03
]
]
cm
=
list
(
cm.item
(
)
)
print
(
sort
(
cm
)
)
[
(
(
0.0
,
0
)
,
5813
)
,
(
(
0.0
,
2
)
,
5
)
,
(
(
0.0
,
3
)
,
6
)
,
(
(
0.0
,
4
)
,
7
)
,
(
(
0.0
,
5
)
,
7
)
,
(
(
0.0
,
6
)
,
14
)
,
(
(
0.0
,
8
)
,
57
)
,
(
(
0.0
,
9
)
,
14
)
,
(
(
1.0
,
1
)
,
6650
)
,
(
(
1.0
,
2
)
,
13
)
,
(
(
1.0
,
3
)
,
13
)
,
(
(
1.0
,
4
)
,
22
)
,
(
(
1.0
,
5
)
,
1
)
,
(
(
1.0
,
6
)
,
3
)
,
(
(
1.0
,
7
)
,
10
)
,
(
(
1.0
,
8
)
,
28
)
,
(
(
1.0
,
9
)
,
2
)
,
(
(
2.0
,
0
)
,
25
)
,
(
(
2.0
,
1
)
,
30
)
,
(
(
2.0
,
2
)
,
5643
)
,
(
(
2.0
,
3
)
,
77
)
,
(
(
2.0
,
4
)
,
13
)
,
(
(
2.0
,
5
)
,
10
)
,
(
(
2.0
,
6
)
,
10
)
,
(
(
2.0
,
7
)
,
34
)
,
(
(
2.0
,
8
)
,
97
)
,
(
(
2.0
,
9
)
,
19
)
,
(
(
3.0
,
0
)
,
6
)
,
(
(
3.0
,
1
)
,
4
)
,
(
(
3.0
,
2
)
,
22
)
,
(
(
3.0
,
3
)
,
5898
)
,
(
(
3.0
,
4
)
,
5
)
,
(
(
3.0
,
5
)
,
25
)
,
(
(
3.0
,
6
)
,
3
)
,
(
(
3.0
,
7
)
,
33
)
,
(
(
3.0
,
8
)
,
75
)
,
(
(
3.0
,
9
)
,
60
)
,
(
(
4.0
,
0
)
,
3
)
,
(
(
4.0
,
1
)
,
8
)
,
(
(
4.0
,
2
)
,
6
)
,
(
(
4.0
,
4
)
,
5711
)
,
(
(
4.0
,
6
)
,
13
)
,
(
(
4.0
,
7
)
,
4
)
,
(
(
4.0
,
8
)
,
6
)
,
(
(
4.0
,
9
)
,
91
)
,
(
(
5.0
,
0
)
,
13
)
,
(
(
5.0
,
1
)
,
3
)
,
(
(
5.0
,
2
)
,
4
)
,
(
(
5.0
,
3
)
,
42
)
,
(
(
5.0
,
4
)
,
13
)
,
(
(
5.0
,
5
)
,
5252
)
,
(
(
5.0
,
6
)
,
22
)
,
(
(
5.0
,
7
)
,
4
)
,
(
(
5.0
,
8
)
,
47
)
,
(
(
5.0
,
9
)
,
21
)
,
(
(
6.0
,
0
)
,
19
)
,
(
(
6.0
,
1
)
,
6
)
,
(
(
6.0
,
2
)
,
7
)
,
(
(
6.0
,
4
)
,
18
)
,
(
(
6.0
,
5
)
,
24
)
,
(
(
6.0
,
6
)
,
5784
)
,
(
(
6.0
,
8
)
,
48
)
,
(
(
6.0
,
9
)
,
12
)
,
(
(
7.0
,
0
)
,
3
)
,
(
(
7.0
,
1
)
,
18
)
,
(
(
7.0
,
2
)
,
17
)
,
(
(
7.0
,
3
)
,
6
)
,
(
(
7.0
,
4
)
,
40
)
,
(
(
7.0
,
5
)
,
3
)
,
(
(
7.0
,
6
)
,
6
)
,
(
(
7.0
,
7
)
,
5991
)
,
(
(
7.0
,
8
)
,
26
)
,
(
(
7.0
,
9
)
,
155
)
,
(
(
8.0
,
0
)
,
13
)
,
(
(
8.0
,
1
)
,
20
)
,
(
(
8.0
,
2
)
,
7
)
,
(
(
8.0
,
3
)
,
30
)
,
(
(
8.0
,
4
)
,
22
)
,
(
(
8.0
,
5
)
,
14
)
,
(
(
8.0
,
6
)
,
17
)
,
(
(
8.0
,
7
)
,
5
)
,
(
(
8.0
,
8
)
,
5661
)
,
(
(
8.0
,
9
)
,
62
)
,
(
(
9.0
,
0
)
,
19
)
,
(
(
9.0
,
1
)
,
3
)
,
(
(
9.0
,
2
)
,
3
)
,
(
(
9.0
,
3
)
,
29
)
,
(
(
9.0
,
4
)
,
30
)
,
(
(
9.0
,
5
)
,
8
)
,
(
(
9.0
,
6
)
,
1
)
,
(
(
9.0
,
7
)
,
21
)
,
(
(
9.0
,
8
)
,
19
)
,
(
(
9.0
,
9
)
,
5816
)
]
for
i
in
rang
(
epoch
)
:
print
(
``
epoch
:
``
,
i
)
ann.wih
=
weight
[
i
]
[
0
]
ann.who
=
weight
[
i
]
[
1
]
correct
,
wrong
=
ann.evalu
(
train_img
,
train_label
)
print
(
``
accruraci
train
:
``
,
correct
/
(
correct
+
wrong
)
)
correct
,
wrong
=
ann.evalu
(
test_img
,
test_label
)
print
(
``
accruraci
:
test
''
,
correct
/
(
correct
+
wrong
)
)
epoch
:
0
accruraci
train
:
0.9423333333333334
accruraci
:
test
0.9422
epoch
:
1
accruraci
train
:
0.95965
accruraci
:
test
0.9555
epoch
:
2
accruraci
train
:
0.9658
accruraci
:
test
0.957
epoch
:
3
accruraci
train
:
0.9683333333333334
accruraci
:
test
0.9621
epoch
:
4
accruraci
train
:
0.9685333333333334
accruraci
:
test
0.9598
epoch
:
5
accruraci
train
:
0.9720666666666666
accruraci
:
test
0.9621
epoch
:
6
accruraci
train
:
0.9710833333333333
accruraci
:
test
0.9609
epoch
:
7
accruraci
train
:
0.9731166666666666
accruraci
:
test
0.9628
epoch
:
8
accruraci
train
:
0.9706
accruraci
:
test
0.9591
epoch
:
9
accruraci
train
:
0.9703166666666667
accruraci
:
test
0.9579
with
bia
node
import
numpi
as
np
@
np.vector
def
sigmoid
(
x
)
:
return
1
/
(
1
+
np.e
**
-x
)
activation_funct
=
sigmoid
from
scipy.stat
import
truncnorm
def
truncated_norm
(
mean=0
,
sd=1
,
low=0
,
upp=10
)
:
return
truncnorm
(
(
low
-
mean
)
/
sd
,
(
upp
-
mean
)
/
sd
,
loc=mean
,
scale=sd
)
class
neuralnetwork
:
def
__init__
(
self
,
no_of_in_nod
,
no_of_out_nod
,
no_of_hidden_nod
,
learning_r
,
bias=non
)
:
self.no_of_in_nod
=
no_of_in_nod
self.no_of_out_nod
=
no_of_out_nod
self.no_of_hidden_nod
=
no_of_hidden_nod
self.learning_r
=
learning_r
self.bia
=
bia
self.create_weight_matric
(
)
def
create_weight_matric
(
self
)
:
``
''
''
A
method
to
initi
the
weight
matric
of
the
neural
network
with
option
bia
node
``
''
''
bias_nod
=
1
if
self.bia
els
0
rad
=
1
/
np.sqrt
(
self.no_of_in_nod
+
bias_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.wih
=
x.rv
(
(
self.no_of_hidden_nod
,
self.no_of_in_nod
+
bias_nod
)
)
rad
=
1
/
np.sqrt
(
self.no_of_hidden_nod
+
bias_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.who
=
x.rv
(
(
self.no_of_out_nod
,
self.no_of_hidden_nod
+
bias_nod
)
)
def
train
(
self
,
input_vector
,
target_vector
)
:
``
''
''
input_vector
and
target_vector
can
be
tupl
,
list
or
ndarray
``
''
''
bias_nod
=
1
if
self.bia
els
0
if
self.bia
:
#
ad
bia
node
to
the
end
of
the
inpuy_vector
input_vector
=
np.concaten
(
(
input_vector
,
[
self.bia
]
)
)
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
target_vector
=
np.array
(
target_vector
,
ndmin=2
)
.T
output_vector1
=
np.dot
(
self.wih
,
input_vector
)
output_hidden
=
activation_funct
(
output_vector1
)
if
self.bia
:
output_hidden
=
np.concaten
(
(
output_hidden
,
[
[
self.bia
]
]
)
)
output_vector2
=
np.dot
(
self.who
,
output_hidden
)
output_network
=
activation_funct
(
output_vector2
)
output_error
=
target_vector
-
output_network
#
updat
the
weight
:
tmp
=
output_error
*
output_network
*
(
1.0
-
output_network
)
tmp
=
self.learning_r
*
np.dot
(
tmp
,
output_hidden.t
)
self.who
+=
tmp
#
calcul
hidden
error
:
hidden_error
=
np.dot
(
self.who.t
,
output_error
)
#
updat
the
weight
:
tmp
=
hidden_error
*
output_hidden
*
(
1.0
-
output_hidden
)
if
self.bia
:
x
=
np.dot
(
tmp
,
input_vector.t
)
[
:
-1
,
:
]
els
:
x
=
np.dot
(
tmp
,
input_vector.t
)
self.wih
+=
self.learning_r
*
x
def
run
(
self
,
input_vector
)
:
``
''
''
input_vector
can
be
tupl
,
list
or
ndarray
``
''
''
if
self.bia
:
#
ad
bia
node
to
the
end
of
the
inpuy_vector
input_vector
=
np.concaten
(
(
input_vector
,
[
1
]
)
)
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
output_vector
=
np.dot
(
self.wih
,
input_vector
)
output_vector
=
activation_funct
(
output_vector
)
if
self.bia
:
output_vector
=
np.concaten
(
(
output_vector
,
[
[
1
]
]
)
)
output_vector
=
np.dot
(
self.who
,
output_vector
)
output_vector
=
activation_funct
(
output_vector
)
return
output_vector
def
evalu
(
self
,
data
,
label
)
:
correct
,
wrong
=
0
,
0
for
i
in
rang
(
len
(
data
)
)
:
re
=
self.run
(
data
[
i
]
)
res_max
=
res.argmax
(
)
if
res_max
==
label
[
i
]
:
correct
+=
1
els
:
wrong
+=
1
return
correct
,
wrong
ann
=
neuralnetwork
(
no_of_in_nodes=image_pixel
,
no_of_out_nodes=10
,
no_of_hidden_nodes=200
,
learning_rate=0.1
,
bias=non
)
for
i
in
rang
(
len
(
train_img
)
)
:
ann.train
(
train_img
[
i
]
,
train_labels_one_hot
[
i
]
)
for
i
in
rang
(
20
)
:
re
=
ann.run
(
test_img
[
i
]
)
print
(
test_label
[
i
]
,
np.argmax
(
re
)
,
np.max
(
re
)
)
[
7
.
]
7
0.987360189348
[
2
.
]
2
0.749410818011
[
1
.
]
1
0.990936932334
[
0
.
]
0
0.982162095094
[
4
.
]
4
0.972015930565
[
1
.
]
1
0.986745897455
[
4
.
]
4
0.979223439296
[
9
.
]
9
0.964589260844
[
5
.
]
6
0.42949267086
[
9
.
]
9
0.971961659342
[
0
.
]
0
0.979675421627
[
6
.
]
6
0.843565954906
[
9
.
]
9
0.989992702963
[
0
.
]
0
0.98947745372
[
1
.
]
1
0.988959403957
[
5
.
]
5
0.889105662605
[
9
.
]
9
0.99279904523
[
7
.
]
7
0.985973174432
[
3
.
]
3
0.711112673804
[
4
.
]
4
0.993171047267
correct
,
wrong
=
ann.evalu
(
train_img
,
train_label
)
print
(
``
accruraci
train
:
``
,
correct
/
(
correct
+
wrong
)
)
correct
,
wrong
=
ann.evalu
(
test_img
,
test_label
)
print
(
``
accruraci
:
test
''
,
correct
/
(
correct
+
wrong
)
)
accruraci
train
:
0.9504666666666667
accruraci
:
test
0.9493
version
with
bia
and
epoch
:
import
numpi
as
np
@
np.vector
def
sigmoid
(
x
)
:
return
1
/
(
1
+
np.e
**
-x
)
activation_funct
=
sigmoid
from
scipy.stat
import
truncnorm
def
truncated_norm
(
mean=0
,
sd=1
,
low=0
,
upp=10
)
:
return
truncnorm
(
(
low
-
mean
)
/
sd
,
(
upp
-
mean
)
/
sd
,
loc=mean
,
scale=sd
)
class
neuralnetwork
:
def
__init__
(
self
,
no_of_in_nod
,
no_of_out_nod
,
no_of_hidden_nod
,
learning_r
,
bias=non
)
:
self.no_of_in_nod
=
no_of_in_nod
self.no_of_out_nod
=
no_of_out_nod
self.no_of_hidden_nod
=
no_of_hidden_nod
self.learning_r
=
learning_r
self.bia
=
bia
self.create_weight_matric
(
)
def
create_weight_matric
(
self
)
:
``
''
''
A
method
to
initi
the
weight
matric
of
the
neural
network
with
option
bia
node
''
''
''
bias_nod
=
1
if
self.bia
els
0
rad
=
1
/
np.sqrt
(
self.no_of_in_nod
+
bias_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.wih
=
x.rv
(
(
self.no_of_hidden_nod
,
self.no_of_in_nod
+
bias_nod
)
)
rad
=
1
/
np.sqrt
(
self.no_of_hidden_nod
+
bias_nod
)
X
=
truncated_norm
(
mean=0
,
sd=1
,
low=-rad
,
upp=rad
)
self.who
=
x.rv
(
(
self.no_of_out_nod
,
self.no_of_hidden_nod
+
bias_nod
)
)
def
train_singl
(
self
,
input_vector
,
target_vector
)
:
``
''
''
input_vector
and
target_vector
can
be
tupl
,
list
or
ndarray
``
''
''
bias_nod
=
1
if
self.bia
els
0
if
self.bia
:
#
ad
bia
node
to
the
end
of
the
inpuy_vector
input_vector
=
np.concaten
(
(
input_vector
,
[
self.bia
]
)
)
output_vector
=
[
]
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
target_vector
=
np.array
(
target_vector
,
ndmin=2
)
.T
output_vector1
=
np.dot
(
self.wih
,
input_vector
)
output_hidden
=
activation_funct
(
output_vector1
)
if
self.bia
:
output_hidden
=
np.concaten
(
(
output_hidden
,
[
[
self.bia
]
]
)
)
output_vector2
=
np.dot
(
self.who
,
output_hidden
)
output_network
=
activation_funct
(
output_vector2
)
output_error
=
target_vector
-
output_network
#
updat
the
weight
:
tmp
=
output_error
*
output_network
*
(
1.0
-
output_network
)
tmp
=
self.learning_r
*
np.dot
(
tmp
,
output_hidden.t
)
self.who
+=
tmp
#
calcul
hidden
error
:
hidden_error
=
np.dot
(
self.who.t
,
output_error
)
#
updat
the
weight
:
tmp
=
hidden_error
*
output_hidden
*
(
1.0
-
output_hidden
)
if
self.bia
:
x
=
np.dot
(
tmp
,
input_vector.t
)
[
:
-1
,
:
]
els
:
x
=
np.dot
(
tmp
,
input_vector.t
)
self.wih
+=
self.learning_r
*
x
def
train
(
self
,
data_array
,
labels_one_hot_array
,
epochs=1
,
intermediate_results=fals
)
:
intermediate_weight
=
[
]
for
epoch
in
rang
(
epoch
)
:
for
i
in
rang
(
len
(
data_array
)
)
:
self.train_singl
(
data_array
[
i
]
,
labels_one_hot_array
[
i
]
)
if
intermediate_result
:
intermediate_weights.append
(
(
self.wih.copi
(
)
,
self.who.copi
(
)
)
)
return
intermediate_weight
def
run
(
self
,
input_vector
)
:
#
input_vector
can
be
tupl
,
list
or
ndarray
if
self.bia
:
#
ad
bia
node
to
the
end
of
the
inpuy_vector
input_vector
=
np.concaten
(
(
input_vector
,
[
self.bia
]
)
)
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
output_vector
=
np.dot
(
self.wih
,
input_vector
)
output_vector
=
activation_funct
(
output_vector
)
if
self.bia
:
output_vector
=
np.concaten
(
(
output_vector
,
[
[
self.bia
]
]
)
)
output_vector
=
np.dot
(
self.who
,
output_vector
)
output_vector
=
activation_funct
(
output_vector
)
return
output_vector
def
evalu
(
self
,
data
,
label
)
:
correct
,
wrong
=
0
,
0
for
i
in
rang
(
len
(
data
)
)
:
re
=
self.run
(
data
[
i
]
)
res_max
=
res.argmax
(
)
if
res_max
==
label
[
i
]
:
correct
+=
1
els
:
wrong
+=
1
return
correct
,
wrong
epoch
=
12
network
=
neuralnetwork
(
no_of_in_nodes=image_pixel
,
no_of_out_nodes=10
,
no_of_hidden_nodes=100
,
learning_rate=0.1
,
bias=non
)
weight
=
network.train
(
train_img
,
train_labels_one_hot
,
epochs=epoch
,
intermediate_results=tru
)
for
epoch
in
rang
(
epoch
)
:
print
(
``
epoch
:
``
,
epoch
)
network.wih
=
weight
[
epoch
]
[
0
]
network.who
=
weight
[
epoch
]
[
1
]
correct
,
wrong
=
network.evalu
(
train_img
,
train_label
)
print
(
``
accruraci
train
:
``
,
correct
/
(
correct
+
wrong
)
)
correct
,
wrong
=
network.evalu
(
test_img
,
test_label
)
print
(
``
accruraci
test
:
``
,
correct
/
(
correct
+
wrong
)
)
epoch
:
0
accruraci
train
:
0.9442166666666667
accruraci
test
:
0.9438
epoch
:
1
accruraci
train
:
0.9585333333333333
accruraci
test
:
0.9533
epoch
:
2
accruraci
train
:
0.9651833333333333
accruraci
test
:
0.9578
epoch
:
3
accruraci
train
:
0.96995
accruraci
test
:
0.9598
epoch
:
4
accruraci
train
:
0.9728333333333333
accruraci
test
:
0.9629
epoch
:
5
accruraci
train
:
0.9733833333333334
accruraci
test
:
0.9604
epoch
:
6
accruraci
train
:
0.9707333333333333
accruraci
test
:
0.9582
epoch
:
7
accruraci
train
:
0.9736333333333334
accruraci
test
:
0.9621
epoch
:
8
accruraci
train
:
0.9760333333333333
accruraci
test
:
0.9641
epoch
:
9
accruraci
train
:
0.9779
accruraci
test
:
0.9629
epoch
:
10
accruraci
train
:
0.9776333333333334
accruraci
test
:
0.9629
epoch
:
11
accruraci
train
:
0.97755
accruraci
test
:
0.9625
epoch
=
12
with
open
(
``
nist_tests.csv
''
,
``
w
''
)
as
fh_out
:
for
hidden_nod
in
[
20
,
50
,
100
,
120
,
150
]
:
for
learning_r
in
[
0.01
,
0.05
,
0.1
,
0.2
]
:
for
bia
in
[
none
,
0.5
]
:
network
=
neuralnetwork
(
no_of_in_nodes=image_pixel
,
no_of_out_nodes=10
,
no_of_hidden_nodes=hidden_nod
,
learning_rate=learning_r
,
bias=bia
)
weight
=
network.train
(
train_img
,
train_labels_one_hot
,
epochs=epoch
,
intermediate_results=tru
)
for
epoch
in
rang
(
epoch
)
:
print
(
``
*
''
,
end=
''
''
)
network.wih
=
weight
[
epoch
]
[
0
]
network.who
=
weight
[
epoch
]
[
1
]
train_correct
,
train_wrong
=
network.evalu
(
train_img
,
train_label
)
test_correct
,
test_wrong
=
network.evalu
(
test_img
,
test_label
)
outstr
=
str
(
hidden_nod
)
+
``
``
+
str
(
learning_r
)
+
``
``
+
str
(
bia
)
outstr
+=
``
``
+
str
(
epoch
)
+
``
``
outstr
+=
str
(
train_correct
/
(
train_correct
+
train_wrong
)
)
+
``
``
outstr
+=
str
(
train_wrong
/
(
train_correct
+
train_wrong
)
)
+
``
``
outstr
+=
str
(
test_correct
/
(
test_correct
+
test_wrong
)
)
+
``
``
outstr
+=
str
(
test_wrong
/
(
test_correct
+
test_wrong
)
)
fh_out.writ
(
outstr
+
``
\n
''
)
fh_out.flush
(
)
the
file
nist_tests_20_50_100_120_150.csv
contain
the
result
from
a
run
of
the
previou
program
.
network
with
multipl
hidden
layersw
will
write
a
new
neural
network
class
,
in
which
we
can
defin
an
arbitrari
number
of
hidden
layer
.
the
code
is
also
improv
,
becaus
the
weight
matric
are
now
build
insid
of
a
loop
instead
redund
code
:
import
numpi
as
np
from
scipy.speci
import
expit
as
activation_funct
from
scipy.stat
import
truncnorm
def
truncated_norm
(
mean=0
,
sd=1
,
low=0
,
upp=10
)
:
return
truncnorm
(
(
low
-
mean
)
/
sd
,
(
upp
-
mean
)
/
sd
,
loc=mean
,
scale=sd
)
class
neuralnetwork
:
def
__init__
(
self
,
network_structur
,
#
ie
.
[
input_nod
,
hidden1_nod
,
...
,
hidden_n_nod
,
output_nod
]
learning_r
,
bias=non
)
:
self.structur
=
network_structur
self.learning_r
=
learning_r
self.bia
=
bia
self.create_weight_matric
(
)
def
create_weight_matric
(
self
)
:
bias_nod
=
1
if
self.bia
els
0
self.weights_matric
=
[
]
layer_index
=
1
no_of_lay
=
len
(
self.structur
)
while
layer_index
<
no_of_lay
:
nodes_in
=
self.structur
[
layer_index-1
]
nodes_out
=
self.structur
[
layer_index
]
n
=
(
nodes_in
+
bias_nod
)
*
nodes_out
rad
=
1
/
np.sqrt
(
nodes_in
)
X
=
truncated_norm
(
mean=2
,
sd=1
,
low=-rad
,
upp=rad
)
wm
=
x.rv
(
n
)
.reshap
(
(
nodes_out
,
nodes_in
+
bias_nod
)
)
self.weights_matrices.append
(
wm
)
layer_index
+=
1
def
train
(
self
,
input_vector
,
target_vector
)
:
``
''
''
input_vector
and
target_vector
can
be
tupl
,
list
or
ndarray
``
''
''
no_of_lay
=
len
(
self.structur
)
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
layer_index
=
0
#
the
output/input
vector
of
the
variou
layer
:
res_vector
=
[
input_vector
]
while
layer_index
<
no_of_lay
-
1
:
in_vector
=
res_vector
[
-1
]
if
self.bia
:
#
ad
bia
node
to
the
end
of
the
'input'_vector
in_vector
=
np.concaten
(
(
in_vector
,
[
[
self.bia
]
]
)
)
res_vector
[
-1
]
=
in_vector
x
=
np.dot
(
self.weights_matric
[
layer_index
]
,
in_vector
)
out_vector
=
activation_funct
(
x
)
#
the
output
of
one
layer
is
the
input
of
the
next
one
:
res_vectors.append
(
out_vector
)
layer_index
+=
1
layer_index
=
no_of_lay
-
1
target_vector
=
np.array
(
target_vector
,
ndmin=2
)
.T
#
the
input
vector
to
the
variou
layer
output_error
=
target_vector
-
out_vector
while
layer_index
>
0
:
out_vector
=
res_vector
[
layer_index
]
in_vector
=
res_vector
[
layer_index-1
]
if
self.bia
and
not
layer_index==
(
no_of_layers-1
)
:
out_vector
=
out_vector
[
:
-1
,
:
]
.copi
(
)
tmp
=
output_error
*
out_vector
*
(
1.0
-
out_vector
)
tmp
=
np.dot
(
tmp
,
in_vector.t
)
#
if
self.bia
:
#
tmp
=
tmp
[
:
-1
,
:
]
self.weights_matric
[
layer_index-1
]
+=
self.learning_r
*
tmp
output_error
=
np.dot
(
self.weights_matric
[
layer_index-1
]
.T
,
output_error
)
if
self.bia
:
output_error
=
output_error
[
:
-1
,
:
]
layer_index
-=
1
def
run
(
self
,
input_vector
)
:
#
input_vector
can
be
tupl
,
list
or
ndarray
no_of_lay
=
len
(
self.structur
)
if
self.bia
:
#
ad
bia
node
to
the
end
of
the
inpuy_vector
input_vector
=
np.concaten
(
(
input_vector
,
[
self.bia
]
)
)
in_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
layer_index
=
1
#
the
input
vector
to
the
variou
layer
while
layer_index
<
no_of_lay
:
x
=
np.dot
(
self.weights_matric
[
layer_index-1
]
,
in_vector
)
out_vector
=
activation_funct
(
x
)
#
input
vector
for
next
layer
in_vector
=
out_vector
if
self.bia
:
in_vector
=
np.concaten
(
(
in_vector
,
[
[
self.bia
]
]
)
)
layer_index
+=
1
return
out_vector
def
evalu
(
self
,
data
,
label
)
:
correct
,
wrong
=
0
,
0
for
i
in
rang
(
len
(
data
)
)
:
re
=
self.run
(
data
[
i
]
)
res_max
=
res.argmax
(
)
if
res_max
==
label
[
i
]
:
correct
+=
1
els
:
wrong
+=
1
return
correct
,
wrong
ann
=
neuralnetwork
(
network_structure=
[
image_pixel
,
50
,
50
,
10
]
,
learning_rate=0.1
,
bias=non
)
for
i
in
rang
(
len
(
train_img
)
)
:
ann.train
(
train_img
[
i
]
,
train_labels_one_hot
[
i
]
)
correct
,
wrong
=
ann.evalu
(
train_img
,
train_label
)
print
(
``
accruraci
train
:
``
,
correct
/
(
correct
+
wrong
)
)
correct
,
wrong
=
ann.evalu
(
test_img
,
test_label
)
print
(
``
accruraci
:
test
''
,
correct
/
(
correct
+
wrong
)
)
accruraci
train
:
0.8851
accruraci
:
test
0.8879
network
with
multipl
hidden
layer
and
epoch
import
numpi
as
np
from
scipy.speci
import
expit
as
activation_funct
from
scipy.stat
import
truncnorm
def
truncated_norm
(
mean=0
,
sd=1
,
low=0
,
upp=10
)
:
return
truncnorm
(
(
low
-
mean
)
/
sd
,
(
upp
-
mean
)
/
sd
,
loc=mean
,
scale=sd
)
class
neuralnetwork
:
def
__init__
(
self
,
network_structur
,
#
ie
.
[
input_nod
,
hidden1_nod
,
...
,
hidden_n_nod
,
output_nod
]
learning_r
,
bias=non
)
:
self.structur
=
network_structur
self.learning_r
=
learning_r
self.bia
=
bia
self.create_weight_matric
(
)
def
create_weight_matric
(
self
)
:
X
=
truncated_norm
(
mean=2
,
sd=1
,
low=-0.5
,
upp=0.5
)
bias_nod
=
1
if
self.bia
els
0
self.weights_matric
=
[
]
layer_index
=
1
no_of_lay
=
len
(
self.structur
)
while
layer_index
<
no_of_lay
:
nodes_in
=
self.structur
[
layer_index-1
]
nodes_out
=
self.structur
[
layer_index
]
n
=
(
nodes_in
+
bias_nod
)
*
nodes_out
rad
=
1
/
np.sqrt
(
nodes_in
)
X
=
truncated_norm
(
mean=2
,
sd=1
,
low=-rad
,
upp=rad
)
wm
=
x.rv
(
n
)
.reshap
(
(
nodes_out
,
nodes_in
+
bias_nod
)
)
self.weights_matrices.append
(
wm
)
layer_index
+=
1
def
train_singl
(
self
,
input_vector
,
target_vector
)
:
#
input_vector
and
target_vector
can
be
tupl
,
list
or
ndarray
no_of_lay
=
len
(
self.structur
)
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
layer_index
=
0
#
the
output/input
vector
of
the
variou
layer
:
res_vector
=
[
input_vector
]
while
layer_index
<
no_of_lay
-
1
:
in_vector
=
res_vector
[
-1
]
if
self.bia
:
#
ad
bia
node
to
the
end
of
the
'input'_vector
in_vector
=
np.concaten
(
(
in_vector
,
[
[
self.bia
]
]
)
)
res_vector
[
-1
]
=
in_vector
x
=
np.dot
(
self.weights_matric
[
layer_index
]
,
in_vector
)
out_vector
=
activation_funct
(
x
)
res_vectors.append
(
out_vector
)
layer_index
+=
1
layer_index
=
no_of_lay
-
1
target_vector
=
np.array
(
target_vector
,
ndmin=2
)
.T
#
the
input
vector
to
the
variou
layer
output_error
=
target_vector
-
out_vector
while
layer_index
>
0
:
out_vector
=
res_vector
[
layer_index
]
in_vector
=
res_vector
[
layer_index-1
]
if
self.bia
and
not
layer_index==
(
no_of_layers-1
)
:
out_vector
=
out_vector
[
:
-1
,
:
]
.copi
(
)
tmp
=
output_error
*
out_vector
*
(
1.0
-
out_vector
)
tmp
=
np.dot
(
tmp
,
in_vector.t
)
#
if
self.bia
:
#
tmp
=
tmp
[
:
-1
,
:
]
self.weights_matric
[
layer_index-1
]
+=
self.learning_r
*
tmp
output_error
=
np.dot
(
self.weights_matric
[
layer_index-1
]
.T
,
output_error
)
if
self.bia
:
output_error
=
output_error
[
:
-1
,
:
]
layer_index
-=
1
def
train
(
self
,
data_array
,
labels_one_hot_array
,
epochs=1
,
intermediate_results=fals
)
:
intermediate_weight
=
[
]
for
epoch
in
rang
(
epoch
)
:
for
i
in
rang
(
len
(
data_array
)
)
:
self.train_singl
(
data_array
[
i
]
,
labels_one_hot_array
[
i
]
)
if
intermediate_result
:
intermediate_weights.append
(
(
self.wih.copi
(
)
,
self.who.copi
(
)
)
)
return
intermediate_weight
def
run
(
self
,
input_vector
)
:
#
input_vector
can
be
tupl
,
list
or
ndarray
no_of_lay
=
len
(
self.structur
)
if
self.bia
:
#
ad
bia
node
to
the
end
of
the
inpuy_vector
input_vector
=
np.concaten
(
(
input_vector
,
[
self.bia
]
)
)
in_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
layer_index
=
1
#
the
input
vector
to
the
variou
layer
while
layer_index
<
no_of_lay
:
x
=
np.dot
(
self.weights_matric
[
layer_index-1
]
,
in_vector
)
out_vector
=
activation_funct
(
x
)
#
input
vector
for
next
layer
in_vector
=
out_vector
if
self.bia
:
in_vector
=
np.concaten
(
(
in_vector
,
[
[
self.bia
]
]
)
)
layer_index
+=
1
return
out_vector
def
evalu
(
self
,
data
,
label
)
:
correct
,
wrong
=
0
,
0
for
i
in
rang
(
len
(
data
)
)
:
re
=
self.run
(
data
[
i
]
)
res_max
=
res.argmax
(
)
if
res_max
==
label
[
i
]
:
correct
+=
1
els
:
wrong
+=
1
return
correct
,
wrong
epoch
=
3
ann
=
neuralnetwork
(
network_structure=
[
image_pixel
,
80
,
80
,
10
]
,
learning_rate=0.01
,
bias=non
)
ann.train
(
train_img
,
train_labels_one_hot
,
epochs=epoch
)
the
previou
code
return
the
follow
:
[
]
correct
,
wrong
=
ann.evalu
(
train_img
,
train_label
)
print
(
``
accruraci
train
:
``
,
correct
/
(
correct
+
wrong
)
)
correct
,
wrong
=
ann.evalu
(
test_img
,
test_label
)
print
(
``
accruraci
:
test
''
,
correct
/
(
correct
+
wrong
)
)
accruraci
train
:
0.9564166666666667
accruraci
:
test
0.953
footnot
1
wan
,
Li
;
matthew
zeiler
;
sixin
zhang
;
yann
lecun
;
rob
fergu
(
2013
)
.
regular
of
neural
network
use
dropconnect
.
intern
confer
on
machin
learn
(
icml
)
.
previou
chapter
:
confus
matrix
next
chapter
:
dropout
neural
network
©
2011
-
2018
,
bernd
klein
,
bodenseo
;
design
by
denis
mitchinson
adapt
for
python-course.eu
by
bernd
klein
