machin
learn
with
python
:
regress
tree
in
python
python
machin
learn
tutori
machin
learn
machin
learn
terminologyk-nearest
neighbor
classifierneur
network
from
scratch
in
pythonneur
network
in
python
use
numypybackpropag
in
neural
networksconfus
matrixtrain
and
test
with
mnistdropout
neural
networksneur
network
with
scikitmachin
learn
with
scikit
and
pythonintroduct
naiv
bay
classifierna
bay
classifi
with
scikitintroduct
into
text
classif
use
naiv
bayespython
implement
of
text
classificationdecis
treesregress
treesrandom
forestsboost
algorithmprincip
compon
analysislinear
discrimin
analysisexpect
maxim
and
gaussian
mixtur
modelintroduct
into
tensorflow
python
In
greek
mytholog
,
python
is
the
name
of
a
a
huge
serpent
and
sometim
a
dragon
.
python
had
been
kill
by
the
god
apollo
at
delphi
.
python
wa
creat
out
of
the
slime
and
mud
left
after
the
great
flood
.
He
wa
appoint
by
gaia
(
mother
earth
)
to
guard
the
oracl
of
delphi
,
known
as
pytho
.
the
program
languag
python
ha
not
been
creat
out
of
slime
and
mud
but
out
of
the
program
languag
abc
.
It
ha
been
devis
by
a
dutch
programm
,
name
guido
van
rossum
,
in
amsterdam
.
origin
of
python
guido
van
rossum
wrote
the
follow
about
the
origin
of
python
in
a
foreword
for
the
book
``
program
python
''
by
mark
lutz
in
1996
:
``
over
six
year
ago
,
in
decemb
1989
,
I
wa
look
for
a
``
hobbi
''
program
project
that
would
keep
me
occupi
dure
the
week
around
christma
.
My
offic
(
a
government-run
research
lab
in
amsterdam
)
would
be
close
,
but
I
had
a
home
comput
,
and
not
much
els
on
my
hand
.
I
decid
to
write
an
interpret
for
the
new
script
languag
I
had
been
think
about
late
:
a
descend
of
abc
that
would
appeal
to
unix/c
hacker
.
I
chose
python
as
a
work
titl
for
the
project
,
be
in
a
slightli
irrever
mood
(
and
a
big
fan
of
monti
python
's
fli
circu
)
.
''
thi
websit
is
free
of
annoy
ad
.
We
want
to
keep
it
like
thi
.
you
can
help
with
your
donat
:
the
need
for
donat
tobia
schlagenhauf
thi
chapter
wa
written
by
tobia
schlagenhauf
.
tobia
is
a
inquisit
and
motiv
machin
learn
enthusiast
.
alway
posit
,
hungri
to
learn
,
will
to
help
.
If
you
have
ani
comment
,
question
,
concern
about
the
content
of
thi
chapter
feel
free
to
get
in
contact
.
you
can
find
and
contact
tobia
schlagenhauf
at
xing
search
thi
websit
:
classroom
train
cours
thi
websit
contain
a
free
and
extens
onlin
tutori
by
bernd
klein
,
use
materi
from
hi
classroom
python
train
cours
.
If
you
are
interest
in
an
instructor-l
classroom
train
cours
,
you
may
have
a
look
at
the
python
class
by
bernd
klein
at
bodenseo
.
Â©
kabliczech
-
fotolia.com
quot
of
the
day
:
''
comput
scienc
is
no
more
about
comput
than
astronomi
is
about
telescop
.
''
(
edsger
dijkstra
)
If
you
have
the
choic
work
with
python
2
or
python
3
,
we
recomend
to
switch
to
python
3
!
you
can
read
our
python
tutori
to
see
what
the
differ
are
.
data
protect
declar
data
protect
declar
previou
chapter
:
decis
tree
next
chapter
:
random
forest
regress
tree
In
the
previou
chapter
about
classif
decis
tree
we
have
introduc
the
basic
concept
underli
decis
tree
model
,
how
they
can
be
build
with
python
from
scratch
as
well
as
use
the
prepackag
sklearn
decisiontreeclassifi
method
.
We
have
also
introduc
advantag
and
disadvantag
of
decis
tree
model
as
well
as
import
extens
and
variat
.
one
disadvantag
of
classif
decis
tree
is
that
they
need
a
target
featur
which
is
categor
scale
like
for
instanc
weather
=
{
sunni
,
raini
,
overcast
,
thunderstorm
}
.
here
aris
a
problem
:
what
if
we
want
our
tree
for
instanc
to
predict
the
price
of
a
hous
given
some
target
featur
attribut
like
the
number
of
room
and
the
locat
?
here
the
valu
of
the
target
featur
(
prize
)
are
no
longer
categor
scale
but
are
continu
-
A
hous
can
have
,
theoret
,
a
infinit
number
of
differ
price
-
that
where
regress
tree
come
in
.
regress
tree
work
in
princip
in
the
same
way
as
classif
tree
with
the
larg
differ
that
the
target
featur
valu
can
now
take
on
an
infinit
number
of
continu
scale
valu
.
henc
the
task
is
now
to
predict
the
valu
of
a
continu
scale
target
featur
Y
given
the
valu
of
a
set
of
categor
(
or
continu
)
scale
descript
featur
X
.
As
state
abov
,
the
principl
of
build
a
regress
tree
follow
the
same
approach
as
the
creation
of
a
classif
tree
.
We
search
for
the
descript
featur
which
split
the
target
featur
valu
most
pure
,
divid
the
dataset
along
the
valu
of
thi
descript
featur
and
repeat
thi
process
for
each
of
the
sub
dataset
until
we
accomplish
a
stop
criteria.if
we
accomplish
a
stop
criteria
,
we
grow
a
leaf
node
.
though
,
a
few
thing
chang
.
first
of
all
,
let
us
consid
the
stop
criteria
we
have
introduc
in
the
classif
tree
chapter
to
grow
a
leaf
node
:
If
the
split
process
lead
to
a
empti
dataset
,
return
the
mode
target
featur
valu
of
the
origin
dataset
If
the
split
process
lead
to
a
dataset
where
no
featur
are
left
,
return
the
mode
target
featur
valu
of
the
direct
parent
node
If
the
split
process
lead
to
a
dataset
where
the
target
featur
valu
are
pure
,
return
thi
valu
If
we
now
consid
the
properti
of
our
new
continu
scale
target
featur
we
mention
that
the
third
stop
criteria
can
no
longer
be
use
sinc
the
target
featur
valu
can
now
take
on
an
infinit
number
of
differ
valu
.
consequ
,
it
is
most
like
that
we
will
not
find
pure
target
featur
valu
until
there
is
onli
one
instanc
left
in
the
dataset
.
To
make
a
long
stori
short
,
there
is
in
gener
noth
like
pure
target
featur
valu
.
To
address
thi
issu
,
we
will
introduc
an
earli
stop
criteria
that
return
the
averag
valu
of
the
target
featur
valu
left
in
the
dataset
if
the
number
of
instanc
in
the
dataset
is
$
\leq
5
$
.
In
gener
,
while
handl
with
regress
tree
we
will
return
the
averag
target
featur
valu
as
predict
at
a
leaf
node
.
the
second
chang
we
have
to
make
becom
appar
when
we
consid
the
split
process
itself
.
while
work
with
classif
tree
we
use
the
inform
gain
(
IG
)
of
a
featur
as
split
criteria
.
that
is
,
the
featur
with
the
largest
IG
wa
use
to
split
the
dataset
on
.
consid
the
follow
exampl
where
we
examin
onli
one
descript
featur
,
let
say
the
number
of
bedroom
,
and
the
cost
of
the
hous
as
target
featur
.
import
panda
as
pd
import
numpi
as
np
df
=
pd.datafram
(
{
'number_of_bedroom
'
:
[
2,2,4,1,3,1,4,2
]
,
'price_of_sal
'
:
[
100000,120000,250000,80000,220000,170000,500000,75000
]
}
)
df
the
abov
code
return
the
follow
:
number_of_bedroom
price_of_sal
0
2
100000
1
2
120000
2
4
250000
3
1
80000
4
3
220000
5
1
170000
6
4
500000
7
2
75000
now
how
would
we
calcul
the
entropi
of
the
*number_of_bedrooms*
featur
?
$
H
(
number
\
of
\
bedroom
)
=
\sum_
{
j
\
\in
\
number
\
of
\
bedroom
}
*
(
\frac
{
|d_
{
number
\
of
\
bedroom
=
j
}
|
}
{
|d|
}
*
(
\sum_
{
k
\
\in
\
price
\
of
\
sale
}
*
(
-P
(
k
\
|
\
j
)
*log2
(
P
(
k
\
|
\
j
)
)
)
)
)
$
If
we
calcul
the
weight
entropi
,
we
see
that
for
j
=
3
,
we
get
a
weight
entropi
of
0
.
We
get
thi
result
becaus
there
is
onli
one
hous
in
the
dataset
with
3
bedroom
.
On
the
other
hand
,
for
j
=
2
(
occur
three
time
)
we
will
get
a
weight
entropi
of
0.59436
.
To
make
a
long
stori
short
,
sinc
our
target
featur
is
continu
scale
,
the
ig
of
the
categor
scale
descript
featur
are
no
longer
appropri
split
criteria
.
well
,
we
could
instead
categor
the
target
featur
along
it
valu
where
for
instanc
hous
price
between
0
$
and
80000
$
are
categor
as
low
,
between
80001
$
and
150000
$
as
middl
and
>
150001
$
as
high
.
what
we
have
done
here
is
convert
our
regress
problem
into
kind
of
a
classif
problem
.
though
,
sinc
we
want
to
be
abl
to
make
predict
from
a
infinit
number
of
possibl
valu
(
regress
)
thi
is
not
what
we
are
look
for
.
let
come
back
to
our
initi
issu
:
We
want
to
have
a
split
criteria
which
allow
us
to
split
the
dataset
in
such
a
way
that
when
arriv
a
tree
node
,
the
predict
valu
(
we
defin
the
predict
valu
as
the
mean
target
featur
valu
of
the
instanc
at
thi
leaf
node
where
we
defin
the
minimum
number
of
5
instanc
as
earli
stop
criteria
)
is
closest
to
the
actual
valu
.
It
turn
out
that
the
varianc
is
one
of
the
most
commonli
use
split
criteria
for
regress
tree
where
we
will
use
the
varianc
as
split
criteria
.
the
explan
therefor
is
,
that
we
want
to
search
for
the
featur
attribut
which
most
exactli
point
to
the
real
target
featur
valu
when
split
the
dataset
along
the
valu
of
these
target
featur
.
therefor
,
examin
the
follow
pictur
.
what
do
you
think
which
of
those
two
layout
of
the
*number_of_bedrooms*
featur
point
more
exactli
to
the
real
sale
prize
?
well
,
obvious
that
one
with
the
smallest
varianc
!
We
will
introduc
the
math
behind
the
measur
of
varianc
in
the
next
section
.
for
the
time
be
we
start
by
illustr
these
by
arrow
where
wide
arrow
repres
a
high
varianc
and
slim
arrow
a
low
varianc
.
We
can
illustr
that
by
show
the
*variance*
of
the
target
featur
for
each
valu
of
the
descript
featur
.
As
you
can
see
,
the
featur
layout
which
minim
the
varianc
of
the
target
featur
valu
when
we
split
the
dataset
along
the
valu
of
the
descript
featur
is
the
featur
layout
which
most
exactli
point
to
the
real
valu
and
henc
should
be
use
as
split
criteria
.
dure
the
creation
of
our
regress
tree
model
we
will
use
the
measur
of
varianc
to
replac
the
inform
gain
as
split
criteria
.
the
math
behind
regress
tree
As
state
abov
,
the
task
dure
grow
a
regress
tree
is
in
principl
the
same
as
dure
the
creation
of
classif
tree
.
though
,
sinc
the
IG
turn
out
to
be
no
longer
an
appropri
split
criteria
(
neither
is
the
gini
index
)
due
to
the
continu
charact
of
the
target
featur
we
must
have
a
new
split
criteria
.
therefor
we
use
the
varianc
which
we
will
introduc
now
.
**variance**
$
var
(
x
)
=
\frac
{
\sum_
{
i
\
=
1
}
^n
(
y_i-\bar
{
y
}
)
}
{
n-1
}
$
where
$
y_i
$
are
the
singl
target
featur
valu
and
$
\bar
{
y
}
$
is
the
mean
of
these
target
featur
valu
.
take
the
exampl
from
abov
the
total
varianc
of
the
*prize_of_sale*
target
featur
is
calcul
with
:
$
var
(
price
\
of
\
sale
)
=
\frac
{
(
100000-189375
)
^2+
(
120000-189375
)
^2+
(
250000-189375
)
^2+
(
80000-189375
)
^2+
(
220000-189375
)
^2+
(
170000-189375
)
^2+
(
500000-189375
)
^2+
(
75000-189375
)
^2
}
{
7
}
$
$
=
19.903125*10^9
$
#
larg
number
;
)
though
thi
ha
no
effect
on
our
calcul
sinc
we
want
to
know
which
descript
featur
is
best
suit
to
split
the
target
featur
on
,
we
have
to
calcul
the
varianc
for
each
valu
of
the
descript
featur
with
respect
to
the
target
featur
valu
.
henc
for
the
*number_of_rooms*
descript
featur
abov
we
get
for
the
singl
number
of
room
:
$
var
(
number
\
of
\
room
\
=
\
1
)
=
\frac
{
(
80000-125000
)
^2+
(
170000-125000
)
^2
}
{
1
}
=4050000000
$
$
var
(
number
\
of
\
room
\
=
\
2
)
=
\frac
{
(
100000-98333.3
)
^2+
(
120000-98333.3
)
^2+
(
75000-98333.3
)
^2
}
{
2
}
=
508333333.3
$
$
var
(
number
\
of
\
room
\
=
\
3
)
=
(
220000-220000
)
^2
=
0
$
$
var
(
number
\
of
\
room
\
=
\
4
)
=
\frac
{
(
250000-375000
)
^2+
(
500000-375000
)
^2
}
{
1
}
=
31250000000
$
sinc
we
now
want
to
also
address
the
issu
that
there
are
featur
valu
which
occur
rel
rare
but
have
a
high
varianc
(
thi
could
lead
to
a
veri
high
varianc
for
the
whole
featur
just
becaus
of
one
outlin
featur
valu
even
though
the
varianc
of
all
other
featur
valu
may
be
small
)
we
address
thi
by
calcul
the
weight
varianc
for
each
featur
valu
with
:
$
weightvar
(
number
\
of
\
room
\
=
\
1
)
=
\frac
{
2
}
{
8
}
*4050000000
=1012500000
$
$
weightvar
(
number
\
of
\
room
\
=
\
2
)
=
\frac
{
2
}
{
8
}
*508333333.3
=
190625000
$
$
weightvar
(
number
\
of
\
room
\
=
\
3
)
=
\frac
{
2
}
{
8
}
*0
=0
$
$
weightvar
(
number
\
of
\
room
\
=
\
4
)
=
\frac
{
2
}
{
8
}
*31250000000
=
7812500000
$
final
,
we
sum
up
these
weight
varianc
to
make
an
assess
about
the
featur
as
a
whole
:
$
sumvar
(
featur
)
=
\sum_
{
valu
\
\in
\
featur
}
weightvar
(
feature_
{
valu
}
)
$
which
is
in
our
case
:
$
1012500000+190625000+0+7812500000=9015625000
$
put
all
thi
togeth
final
lead
to
the
formula
for
the
weight
featur
varianc
which
we
will
use
at
each
node
in
the
split
process
to
determin
which
featur
we
should
choos
to
split
our
dataset
on
next
.
$
featur
[
choos
]
\
=
\underset
{
f
\
\in
\
featur
}
{
\operatornam
{
argmin
}
}
\
\sum_
{
l
\
\in
\
level
(
f
)
}
\frac
{
|f
=
l|
}
{
|f|
}
*var
(
t
,
f=l
)
$
$
=
\underset
{
f
\
\in
\
featur
}
{
\operatornam
{
argmin
}
}
\
\sum_
{
l
\
\in
\
level
(
f
)
}
\frac
{
|f
=
l|
}
{
|f|
}
*\frac
{
\sum_
{
i
\
=
\
1
}
^n
(
t_i-\bar
{
t
}
)
^2
}
{
n-1
}
$
here
*f*
denot
a
singl
featur
,
*l*
denot
the
valu
of
a
featur
(
e.g
price
==
medium
)
,
*t*
denot
the
valu
of
the
target
featur
in
the
subset
where
*f=l*
.
follow
thi
calcul
specif
we
find
the
featur
at
each
node
to
split
our
dataset
on
.
To
illustr
the
process
of
split
the
dataset
along
the
featur
valu
of
the
lowest
varianc
featur
,
we
take
a
simplifi
exampl
of
the
[
uci
bike
share
dataset
]
(
http
:
//archive.ics.uci.edu/ml/datasets/bike+sharing+dataset
)
which
we
will
use
later
on
in
the
*regress
tree
from
scratch
with
python*
part
of
thi
chapter
and
calcul
the
varianc
for
each
featur
to
find
the
featur
we
should
use
as
root
node
.
import
panda
as
pd
df
=
pd.read_csv
(
``
data/day.csv
''
,
usecols=
[
'season
'
,
'holiday
'
,
'weekday
'
,
'weathersit
'
,
'cnt
'
]
)
df_exampl
=
df.sampl
(
frac=0.012
)
**season**
$
weightvar
(
season
)
=
\frac
{
1
}
{
9
}
*
(
79-79
)
^2+\frac
{
5
}
{
9
}
*\frac
{
(
352-211.8
)
^2+
(
421-211.8
)
^2+
(
12-211.8
)
^2+
(
162-211.8
)
^2+
(
112-211.8
)
^2
}
{
4
}
+\frac
{
1
}
{
9
}
*
(
161-161
)
^2+\frac
{
2
}
{
9
}
*\frac
{
(
109-137
)
^2+
(
165-137
)
^2
}
{
1
}
$
$
=
16429.1
$
**weekday**
$
weightvar
(
weekday
)
=
\frac
{
2
}
{
9
}
*\frac
{
(
109-94
)
^2+
(
79-94
)
^2
}
{
1
}
+\frac
{
2
}
{
9
}
*\frac
{
(
162-137
)
^2+
(
112-137
)
^2
}
{
1
}
+\frac
{
1
}
{
9
}
*
(
421-421
)
^2+\frac
{
2
}
{
9
}
*\frac
{
(
161-86.5
)
^2+
(
12-86.5
)
^2
}
{
1
}
+\frac
{
2
}
{
9
}
*\frac
{
(
352-258.5
)
^2+
(
165-258.5
)
^2
}
{
1
}
=
6730
$
**weathersit**
$
weightvar
(
weathersit
)
=
\frac
{
4
}
{
9
}
*\frac
{
(
421-174.2
)
^2+
(
165-174.2
)
^2+
(
12-174.2
)
^2+
(
161-174.2
)
^2+
(
112-174.2
)
^2
}
{
4
}
+\frac
{
2
}
{
9
}
*\frac
{
(
352-230.5
)
^2+
(
109-230.5
)
^2
}
{
1
}
+\frac
{
2
}
{
9
}
*\frac
{
(
79-120.5
)
^2+
(
112-120.5
)
^2
}
{
1
}
=
19646.83
$
sinc
the
weekday
featur
ha
the
lowest
varianc
,
thi
featur
is
use
to
split
the
dataset
on
and
henc
serv
as
root
node
.
though
due
to
random
sampl
,
thi
exampl
is
not
that
robust
(
for
instanc
there
is
no
instanc
with
weekday
==
3
)
it
should
convey
the
concept
behind
the
data
split
use
varianc
as
split
measur
.
sinc
we
now
have
introduc
the
concept
of
how
the
measur
of
varianc
can
be
use
to
split
a
dataset
with
a
continu
target
featur
,
we
will
now
adapt
the
pseudocod
for
classif
tree
such
that
our
tree
model
is
abl
to
handl
continu
scale
target
featur
valu
.
As
state
abov
,
there
are
two
chang
we
have
to
make
to
enabl
our
tree
model
to
handl
continu
scale
target
featur
valu
:
**1
.
We
introduc
an
earli
stop
criteria
where
we
say
that
if
the
number
of
instanc
at
a
node
is
$
\leq
5
$
(
we
can
adjust
thi
valu
)
,
return
the
mean
target
featur
valu
of
these
numbers**
**2
.
instead
of
the
inform
gain
we
use
the
varianc
of
a
featur
as
our
new
split
criteria**
henc
the
pseudocod
becom
:
id3
(
D
,
feature_attribut
,
target_attribut
,
min_instances=5
)
creat
a
root
node
r
set
r
to
the
mean
of
the
target
featur
valu
in
D
#
#
#
#
#
#
#
chang
#
#
#
#
#
#
#
#
If
num_inst
<
=
min_inst
:
return
r
els
:
pass
If
feature_attribut
is
empti
:
return
r
els
:
att
=
attribut
from
feature_attribut
with
the
lowest
weight
varianc
#
#
#
#
#
#
#
#
chang
#
#
#
#
#
#
#
#
r
=
att
for
valu
in
att
:
add
a
new
node
below
r
where
node_valu
=
(
att
==
valu
)
sub_d_valu
=
(
att
==
valu
)
If
sub_d_valu
==
empti
:
add
a
leaf
node
l
where
l
equal
the
mean
of
the
target
valu
in
D
els
:
add
sub_tre
with
id3
(
sub_d_valu
,
feature_attribut
=
feature_attribut
without
att
,
target_attribut
,
min_instances=5
)
In
addit
to
the
chang
in
the
actual
algorithm
we
also
have
to
use
anoth
measur
of
accuraci
becaus
we
are
no
longer
deal
with
categor
target
featur
valu
.
that
is
,
we
can
no
longer
simpli
compar
the
predict
class
with
the
real
class
and
calcul
the
percentag
where
we
bang
on
the
target
.
instead
we
are
use
the
root
mean
squar
error
(
rmse
)
to
measur
the
``
accuraci
''
of
our
model
.
the
equat
for
the
rmse
is
:
$
rmse
=
\sqrt
{
\frac
{
\sum_
{
i
\
=
\
i
}
^n
(
t_i
-
model
(
test_i
)
)
^2
}
{
n
}
}
$
where
$
t_i
$
are
the
actual
test
target
featur
valu
of
a
test
dataset
and
$
model
(
test_i
)
$
are
the
valu
predict
by
our
train
regress
tree
model
for
these
$
t_i
$
.
In
gener
,
the
lower
the
rmse
valu
,
the
better
our
model
fit
the
actual
data
.
sinc
we
now
have
adapt
our
princip
id3
[
classif
tree
]
(
http
:
//www.python-course.eu/decision_trees.php
)
algorithm
to
handl
continu
scale
target
featur
and
therewith
have
made
it
to
a
regress
tree
model
,
we
can
start
implement
these
chang
in
python
.
therefor
we
simpli
take
the
classif
tree
model
from
the
previou
chapter
and
implement
the
two
chang
mention
abov
.
regress
decis
tree
from
scratch
in
python
As
announc
for
the
implement
of
our
regress
tree
model
we
will
use
the
uci
bike
share
dataset
where
we
will
use
all
731
instanc
as
well
as
a
subset
of
the
origin
16
attribut
.
As
attribut
we
use
the
featur
:
{
'season
'
,
'holiday
'
,
'weekday
'
,
'workingday
'
,
'wheathersit
'
,
'cnt
'
}
where
the
{
'cnt
'
}
featur
serv
as
our
target
featur
and
repres
the
number
of
total
rent
bike
per
day
.
the
first
five
row
of
the
dataset
look
as
follow
:
import
panda
as
pd
dataset
=
pd.read_csv
(
``
data/day.csv
''
,
usecols=
[
'season
'
,
'holiday
'
,
'weekday
'
,
'workingday
'
,
'weathersit
'
,
'cnt
'
]
)
dataset.sampl
(
frac=1
)
.head
(
)
the
previou
code
return
the
follow
:
season
holiday
weekday
workingday
weathersit
cnt
458
2
0
2
1
1
6772
245
3
0
6
0
1
4484
86
2
0
1
1
1
2028
333
4
0
3
1
1
3613
507
2
0
2
1
2
6073
We
will
now
start
adapt
the
origin
creat
classif
algorithm
.
for
further
comment
to
the
code
I
refer
the
reader
to
the
previou
chapter
about
classif
trees.
``
''
''
make
the
import
of
python
packag
need
``
''
''
import
panda
as
pd
import
numpi
as
np
from
pprint
import
pprint
import
matplotlib.pyplot
as
plt
from
matplotlib
import
style
style.us
(
``
fivethirtyeight
''
)
#
import
the
dataset
and
defin
the
featur
and
target
column
#
dataset
=
pd.read_csv
(
``
data/day.csv
''
,
usecols=
[
'season
'
,
'holiday
'
,
'weekday
'
,
'workingday
'
,
'weathersit
'
,
'cnt
'
]
)
.sampl
(
frac=1
)
mean_data
=
np.mean
(
dataset.iloc
[
:
,-1
]
)
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
``
''
''
calcul
the
varainc
of
a
dataset
thi
function
take
three
argument
.
1.
data
=
the
dataset
for
whose
featur
the
varianc
should
be
calcul
2.
split_attribute_nam
=
the
name
of
the
featur
for
which
the
weight
varianc
should
be
calcul
3.
target_nam
=
the
name
of
the
target
featur
.
the
default
for
thi
exampl
is
``
cnt
''
``
''
''
def
var
(
data
,
split_attribute_nam
,
target_name=
''
cnt
''
)
:
feature_valu
=
np.uniqu
(
data
[
split_attribute_nam
]
)
feature_vari
=
0
for
valu
in
feature_valu
:
#
creat
the
data
subset
--
>
split
the
origin
data
along
the
valu
of
the
split_attribute_nam
featur
#
and
reset
the
index
to
not
run
into
an
error
while
use
the
df.loc
[
]
oper
below
subset
=
data.queri
(
'
{
0
}
==
{
1
}
'.format
(
split_attribute_nam
,
valu
)
)
.reset_index
(
)
#
calcul
the
weight
varianc
of
each
subset
value_var
=
(
len
(
subset
)
/len
(
data
)
)
*np.var
(
subset
[
target_nam
]
,
ddof=1
)
#
calcul
the
weight
varianc
of
the
featur
feature_variance+=value_var
return
feature_vari
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
def
classif
(
data
,
originaldata
,
featur
,
min_inst
,
target_attribute_nam
,
parent_node_class
=
none
)
:
``
''
''
classif
algorithm
:
thi
function
take
the
same
5
paramet
as
the
origin
classif
algorithm
in
the
previou
chapter
plu
one
paramet
(
min_inst
)
which
defin
the
number
of
minim
instanc
per
node
as
earli
stop
criterion.
``
''
''
#
defin
the
stop
criteria
--
>
If
one
of
thi
is
satisfi
,
we
want
to
return
a
leaf
node
#
#
#
#
#
#
#
#
#
#
thi
criterion
is
new
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
If
all
target_valu
have
the
same
valu
,
return
the
mean
valu
of
the
target
featur
for
thi
dataset
if
len
(
data
)
<
=
int
(
min_inst
)
:
return
np.mean
(
data
[
target_attribute_nam
]
)
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
If
the
dataset
is
empti
,
return
the
mean
target
featur
valu
in
the
origin
dataset
elif
len
(
data
)
==0
:
return
np.mean
(
originaldata
[
target_attribute_nam
]
)
#
If
the
featur
space
is
empti
,
return
the
mean
target
featur
valu
of
the
direct
parent
node
--
>
note
that
#
the
direct
parent
node
is
that
node
which
ha
call
the
current
run
of
the
algorithm
and
henc
#
the
mean
target
featur
valu
is
store
in
the
parent_node_class
variabl
.
elif
len
(
featur
)
==0
:
return
parent_node_class
#
If
none
of
the
abov
hold
true
,
grow
the
tree
!
els
:
#
set
the
default
valu
for
thi
node
--
>
the
mean
target
featur
valu
of
the
current
node
parent_node_class
=
np.mean
(
data
[
target_attribute_nam
]
)
#
select
the
featur
which
best
split
the
dataset
item_valu
=
[
var
(
data
,
featur
)
for
featur
in
featur
]
#
return
the
varianc
for
featur
in
the
dataset
best_feature_index
=
np.argmin
(
item_valu
)
best_featur
=
featur
[
best_feature_index
]
#
creat
the
tree
structur
.
the
root
get
the
name
of
the
featur
(
best_featur
)
with
the
minimum
varianc
.
tree
=
{
best_featur
:
{
}
}
#
remov
the
featur
with
the
lowest
varianc
from
the
featur
space
featur
=
[
i
for
i
in
featur
if
i
!
=
best_featur
]
#
grow
a
branch
under
the
root
node
for
each
possibl
valu
of
the
root
node
featur
for
valu
in
np.uniqu
(
data
[
best_featur
]
)
:
valu
=
valu
#
split
the
dataset
along
the
valu
of
the
featur
with
the
lowest
varianc
and
therewith
creat
sub_dataset
sub_data
=
data.wher
(
data
[
best_featur
]
==
valu
)
.dropna
(
)
#
call
the
calssif
algorithm
for
each
of
those
sub_dataset
with
the
new
paramet
--
>
here
the
recurs
come
in
!
subtre
=
classif
(
sub_data
,
originaldata
,
featur
,
min_inst
,
'cnt
'
,
parent_node_class
=
parent_node_class
)
#
add
the
sub
tree
,
grown
from
the
sub_dataset
to
the
tree
under
the
root
node
tree
[
best_featur
]
[
valu
]
=
subtre
return
tree
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
``
''
''
predict
queri
instanc
``
''
''
def
predict
(
queri
,
tree
,
default
=
mean_data
)
:
for
key
in
list
(
query.key
(
)
)
:
if
key
in
list
(
tree.key
(
)
)
:
tri
:
result
=
tree
[
key
]
[
queri
[
key
]
]
except
:
return
default
result
=
tree
[
key
]
[
queri
[
key
]
]
if
isinst
(
result
,
dict
)
:
return
predict
(
queri
,
result
)
els
:
return
result
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
``
''
''
creat
a
train
as
well
as
a
test
set
``
''
''
def
train_test_split
(
dataset
)
:
training_data
=
dataset.iloc
[
:
int
(
0.7*len
(
dataset
)
)
]
.reset_index
(
drop=tru
)
#
We
drop
the
index
respect
relabel
the
index
#
start
form
0
,
becaus
we
do
not
want
to
run
into
error
regard
the
row
label
/
index
testing_data
=
dataset.iloc
[
int
(
0.7*len
(
dataset
)
)
:
]
.reset_index
(
drop=tru
)
return
training_data
,
testing_data
training_data
=
train_test_split
(
dataset
)
[
0
]
testing_data
=
train_test_split
(
dataset
)
[
1
]
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
``
''
''
comput
the
rmse
``
''
''
def
test
(
data
,
tree
)
:
#
creat
new
queri
instanc
by
simpli
remov
the
target
featur
column
from
the
origin
dataset
and
#
convert
it
to
a
dictionari
queri
=
data.iloc
[
:
,
:
-1
]
.to_dict
(
orient
=
``
record
''
)
#
creat
a
empti
datafram
in
whose
column
the
predict
of
the
tree
are
store
predict
=
[
]
#
calcul
the
rmse
for
i
in
rang
(
len
(
data
)
)
:
predicted.append
(
predict
(
queri
[
i
]
,
tree
,
mean_data
)
)
rmse
=
np.sqrt
(
np.sum
(
(
(
data.iloc
[
:
,-1
]
-predict
)
**2
)
/len
(
data
)
)
)
return
rmse
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
``
''
''
train
the
tree
,
print
the
tree
and
predict
the
accuraci
``
''
''
tree
=
classif
(
training_data
,
training_data
,
training_data.column
[
:
-1
]
,5
,
'cnt
'
)
pprint
(
tree
)
print
(
'
#
'*50
)
print
(
'root
mean
squar
error
(
rmse
)
:
'
,
test
(
testing_data
,
tree
)
)
{
'season
'
:
{
1
:
{
'weathersit
'
:
{
1.0
:
{
'workingday
'
:
{
0.0
:
{
'holiday
'
:
{
0.0
:
{
'weekday
'
:
{
0.0
:
2398.1071428571427
,
6.0
:
2398.1071428571427
}
}
,
1.0
:
2540.0
}
}
,
1.0
:
{
'holiday
'
:
{
0.0
:
{
'weekday
'
:
{
1.0
:
3284.28
,
2.0
:
3284.28
,
3.0
:
3284.28
,
4.0
:
3284.28
,
5.0
:
3284.28
}
}
}
}
}
}
,
2.0
:
{
'holiday
'
:
{
0.0
:
{
'weekday
'
:
{
0.0
:
2586.8
,
1.0
:
2183.6666666666665
,
2.0
:
{
'workingday
'
:
{
1.0
:
2140.6666666666665
}
}
,
3.0
:
{
'workingday
'
:
{
1.0
:
2049.0
}
}
,
4.0
:
{
'workingday
'
:
{
1.0
:
3105.714285714286
}
}
,
5.0
:
{
'workingday
'
:
{
1.0
:
2844.5454545454545
}
}
,
6.0
:
{
'workingday
'
:
{
0.0
:
1757.111111111111
}
}
}
}
,
1.0
:
1040.0
}
}
,
3.0
:
473.5
}
}
,
2
:
{
'weathersit
'
:
{
1.0
:
{
'workingday
'
:
{
0.0
:
{
'weekday
'
:
{
0.0
:
{
'holiday
'
:
{
0.0
:
5728.2
}
}
,
1.0
:
5503.666666666667
,
5.0
:
3126.0
,
6.0
:
{
'holiday
'
:
{
0.0
:
6206.142857142857
}
}
}
}
,
1.0
:
{
'holiday
'
:
{
0.0
:
{
'weekday
'
:
{
1.0
:
5340.06
,
2.0
:
5340.06
,
3.0
:
5340.06
,
4.0
:
5340.06
,
5.0
:
5340.06
}
}
}
}
}
}
,
2.0
:
{
'holiday
'
:
{
0.0
:
{
'workingday
'
:
{
0.0
:
{
'weekday
'
:
{
0.0
:
4737.0
,
6.0
:
4349.7692307692305
}
}
,
1.0
:
{
'weekday
'
:
{
1.0
:
4446.294117647059
,
2.0
:
4446.294117647059
,
3.0
:
4446.294117647059
,
4.0
:
4446.294117647059
,
5.0
:
5975.333333333333
}
}
}
}
}
}
,
3.0
:
1169.0
}
}
,
3
:
{
'weathersit
'
:
{
1.0
:
{
'holiday
'
:
{
0.0
:
{
'workingday
'
:
{
0.0
:
{
'weekday
'
:
{
0.0
:
5715.0
,
6.0
:
5715.0
}
}
,
1.0
:
{
'weekday
'
:
{
1.0
:
6148.342857142857
,
2.0
:
6148.342857142857
,
3.0
:
6148.342857142857
,
4.0
:
6148.342857142857
,
5.0
:
6148.342857142857
}
}
}
}
,
1.0
:
7403.0
}
}
,
2.0
:
{
'workingday
'
:
{
0.0
:
{
'holiday
'
:
{
0.0
:
{
'weekday
'
:
{
0.0
:
4537.5
,
6.0
:
5028.8
}
}
,
1.0
:
4697.0
}
}
,
1.0
:
{
'holiday
'
:
{
0.0
:
{
'weekday
'
:
{
1.0
:
6745.25
,
2.0
:
5222.4
,
3.0
:
5554.0
,
4.0
:
4580.0
,
5.0
:
5389.409090909091
}
}
}
}
}
}
,
3.0
:
2276.0
}
}
,
4
:
{
'weathersit
'
:
{
1.0
:
{
'holiday
'
:
{
0.0
:
{
'workingday
'
:
{
0.0
:
{
'weekday
'
:
{
0.0
:
4974.772727272727
,
6.0
:
4974.772727272727
}
}
,
1.0
:
{
'weekday
'
:
{
1.0
:
5174.906976744186
,
2.0
:
5174.906976744186
,
3.0
:
5174.906976744186
,
4.0
:
5174.906976744186
,
5.0
:
5174.906976744186
}
}
}
}
,
1.0
:
3101.25
}
}
,
2.0
:
{
'weekday
'
:
{
0.0
:
3795.6666666666665
,
1.0
:
4536.0
,
2.0
:
{
'holiday
'
:
{
0.0
:
{
'workingday
'
:
{
1.0
:
4440.875
}
}
}
}
,
3.0
:
5446.4
,
4.0
:
5888.4
,
5.0
:
5773.6
,
6.0
:
4215.8
}
}
,
3.0
:
{
'weekday
'
:
{
1.0
:
1393.5
,
2.0
:
2946.6666666666665
,
3.0
:
1840.5
,
6.0
:
627.0
}
}
}
}
}
}
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
root
mean
squar
error
(
rmse
)
:
1623.9891244058906
abov
we
can
see
rmse
for
a
minimum
number
of
5
instanc
per
node
.
but
for
the
time
be
,
we
have
no
idea
how
bad
or
good
that
is
.
To
get
a
feel
about
the
``
accuraci
''
of
our
model
we
can
plot
kind
of
a
learn
curv
where
we
plot
the
number
of
minim
instanc
against
the
rmse.
``
''
''
plot
the
rmse
with
respect
to
the
minimum
number
of
instanc
``
''
''
fig
=
plt.figur
(
)
ax0
=
fig.add_subplot
(
111
)
rmse_test
=
[
]
rmse_train
=
[
]
for
i
in
rang
(
1,100
)
:
tree
=
classif
(
training_data
,
training_data
,
training_data.column
[
:
-1
]
,
i
,
'cnt
'
)
rmse_test.append
(
test
(
testing_data
,
tree
)
)
rmse_train.append
(
test
(
training_data
,
tree
)
)
ax0.plot
(
rang
(
1,100
)
,
rmse_test
,
label='test_data
'
)
ax0.plot
(
rang
(
1,100
)
,
rmse_train
,
label='train_data
'
)
ax0.legend
(
)
ax0.set_titl
(
'rmse
with
respect
to
the
minumim
number
of
instanc
per
node
'
)
ax0.set_xlabel
(
'
#
instanc
'
)
ax0.set_ylabel
(
'rmse
'
)
plt.show
(
)
As
we
can
see
,
increas
the
minimum
number
of
instanc
per
node
lead
to
a
lower
rmse
of
our
test
data
until
we
reach
approxim
the
number
of
50
instanc
per
node
.
here
the
test_data
curv
kind
of
flatten
out
and
an
addit
increas
in
the
minimum
number
of
instanc
per
leaf
doe
not
dramat
decreas
the
rmse
of
our
test
set
.
let
plot
the
tree
with
a
minimum
instanc
number
of
50.
tree
=
classif
(
training_data
,
training_data
,
training_data.column
[
:
-1
]
,50
,
'cnt
'
)
pprint
(
tree
)
{
'season
'
:
{
1
:
{
'weathersit
'
:
{
1.0
:
{
'workingday
'
:
{
0.0
:
2407.5666666666666
,
1.0
:
3284.28
}
}
,
2.0
:
2331.74
,
3.0
:
473.5
}
}
,
2
:
{
'weathersit
'
:
{
1.0
:
{
'workingday
'
:
{
0.0
:
5850.178571428572
,
1.0
:
5340.06
}
}
,
2.0
:
4419.595744680851
,
3.0
:
1169.0
}
}
,
3
:
{
'weathersit
'
:
{
1.0
:
{
'holiday
'
:
{
0.0
:
{
'workingday
'
:
{
0.0
:
5715.0
,
1.0
:
{
'weekday
'
:
{
1.0
:
5996.090909090909
,
2.0
:
6093.058823529412
,
3.0
:
6043.6
,
4.0
:
6538.428571428572
,
5.0
:
6050.2307692307695
}
}
}
}
,
1.0
:
7403.0
}
}
,
2.0
:
5242.617647058823
,
3.0
:
2276.0
}
}
,
4
:
{
'weathersit
'
:
{
1.0
:
{
'holiday
'
:
{
0.0
:
{
'workingday
'
:
{
0.0
:
4974.772727272727
,
1.0
:
5174.906976744186
}
}
,
1.0
:
3101.25
}
}
,
2.0
:
4894.861111111111
,
3.0
:
1961.6
}
}
}
}
So
that
our
final
regress
tree
model
.
congratul
-
done
!
regress
tree
in
sklearn
sinc
we
have
now
build
a
regress
tree
model
from
scratch
we
will
use
sklearn
's
prepackag
regress
tree
model
[
sklearn.tree.decisiontreeregressor
]
(
http
:
//scikit-learn.org/stable/modules/generated/sklearn.tree.decisiontreeregressor.html
#
sklearn.tree.decisiontreeregressor
)
.
the
procedur
follow
the
gener
sklearn
api
and
is
as
alway
:
1
.
import
the
model
2
.
parametr
the
model
3
.
preprocess
the
data
and
creat
a
descript
featur
set
as
well
as
a
target
featur
set
4
.
train
the
model
5
.
predict
new
queri
instanc
for
conveni
we
will
use
the
train
and
test
data
from
abov
.
#
import
the
regress
tree
model
from
sklearn.tre
import
decisiontreeregressor
#
parametr
the
model
#
We
will
use
the
mean
squer
error
==
varinc
as
splite
criteria
and
set
the
minimum
number
#
of
instanc
per
leaf
=
5
regression_model
=
decisiontreeregressor
(
criterion=
''
mse
''
,
min_samples_leaf=5
)
#
fit
the
model
regression_model.fit
(
training_data.iloc
[
:
,
:
-1
]
,
training_data.iloc
[
:
,-1
:
]
)
#
predict
unseen
queri
instanc
predict
=
regression_model.predict
(
testing_data.iloc
[
:
,
:
-1
]
)
#
comput
and
plot
the
rmse
rmse
=
np.sqrt
(
np.sum
(
(
(
testing_data.iloc
[
:
,-1
]
-predict
)
**2
)
/len
(
testing_data.iloc
[
:
,-1
]
)
)
)
rmse
the
previou
code
return
the
follow
result
:
1592.7501629176463
with
a
parameter
minimum
number
of
5
instanc
per
leaf
node
,
we
get
nearli
the
same
rmse
as
with
our
own
built
model
abov
.
also
for
thi
model
we
will
plot
the
rmse
against
the
minimum
number
of
instanc
per
leaf
node
to
evalu
the
minimum
number
of
instanc
paramet
which
yield
the
minimum
rmse.
``
''
''
plot
the
rmse
with
respect
to
the
minimum
number
of
instanc
``
''
''
fig
=
plt.figur
(
)
ax0
=
fig.add_subplot
(
111
)
rmse_train
=
[
]
rmse_test
=
[
]
for
i
in
rang
(
1,100
)
:
#
paramter
the
model
and
let
i
be
the
number
of
minimum
instanc
per
leaf
node
regression_model
=
decisiontreeregressor
(
criterion=
''
mse
''
,
min_samples_leaf=i
)
#
train
the
model
regression_model.fit
(
training_data.iloc
[
:
,
:
-1
]
,
training_data.iloc
[
:
,-1
:
]
)
#
predict
queri
instanc
predicted_train
=
regression_model.predict
(
training_data.iloc
[
:
,
:
-1
]
)
predicted_test
=
regression_model.predict
(
testing_data.iloc
[
:
,
:
-1
]
)
#
calcul
and
append
the
rmse
rmse_train.append
(
np.sqrt
(
np.sum
(
(
(
training_data.iloc
[
:
,-1
]
-predicted_train
)
**2
)
/len
(
training_data.iloc
[
:
,-1
]
)
)
)
)
rmse_test.append
(
np.sqrt
(
np.sum
(
(
(
testing_data.iloc
[
:
,-1
]
-predicted_test
)
**2
)
/len
(
testing_data.iloc
[
:
,-1
]
)
)
)
)
ax0.plot
(
rang
(
1,100
)
,
rmse_test
,
label='test_data
'
)
ax0.plot
(
rang
(
1,100
)
,
rmse_train
,
label='train_data
'
)
ax0.legend
(
)
ax0.set_titl
(
'rmse
with
respect
to
the
minumim
number
of
instanc
per
node
'
)
ax0.set_xlabel
(
'
#
instanc
'
)
ax0.set_ylabel
(
'rmse
'
)
plt.show
(
)
use
sklearn
prepackag
regress
tree
model
yield
a
minimum
rmse
with
$
\approx
$
10
instanc
per
node
.
though
,
the
valu
for
the
minimum
rmse
with
respect
to
the
number
of
instanc
are
$
\approx
$
the
same
as
comput
with
our
own
creat
model
.
addit
,
the
rmse
of
sklearn
decis
tree
model
also
flatten
out
for
larg
number
of
instanc
per
node
.
**refer
:
**
+
http
:
//www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/
+
http
:
//nbviewer.jupyter.org/gist/jwdink/9715a1a30e8c7f50a572
+
john
D.
kelleh
,
brian
mac
name
,
aoif
d'arci
,
2015
.
*machin
learn
for
predicti
data
analytics*
.
cambridg
,
massachusett
:
the
mit
press
.
+
lior
rokach
,
ode
maimon
,
2015
.
*data
mine
with
decis
tree
.
*
2nd
Ed
.
ben-gurion
,
israel
,
tel-aviv
,
israel
:
wolrd
scientif
.
+
tom
M.
mitchel
,
1997
.
*machin
learning*
.
new
york
,
NY
,
usa
:
mcgraw-hil
.
previou
chapter
:
decis
tree
next
chapter
:
random
forest
Â©
2011
-
2018
,
bernd
klein
,
bodenseo
;
design
by
denis
mitchinson
adapt
for
python-course.eu
by
bernd
klein
