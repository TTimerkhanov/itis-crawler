machin
learn
with
python
:
k-nearest
neighbor
classifi
in
python
python
machin
learn
tutori
machin
learn
machin
learn
terminologyk-nearest
neighbor
classifierneur
network
from
scratch
in
pythonneur
network
in
python
use
numypybackpropag
in
neural
networksconfus
matrixtrain
and
test
with
mnistdropout
neural
networksneur
network
with
scikitmachin
learn
with
scikit
and
pythonintroduct
naiv
bay
classifierna
bay
classifi
with
scikitintroduct
into
text
classif
use
naiv
bayespython
implement
of
text
classificationdecis
treesregress
treesrandom
forestsboost
algorithmprincip
compon
analysislinear
discrimin
analysisexpect
maxim
and
gaussian
mixtur
modelintroduct
into
tensorflow
k-nearest
neighbor
the
k-nn
is
an
instance-bas
classifi
.
the
underli
idea
is
that
the
likelihood
that
two
instanc
of
the
instanc
space
belong
to
the
same
categori
or
class
increas
with
the
proxim
of
the
instanc
.
proxim
or
close
can
be
defin
with
a
distanc
or
similar
function
.
learn
``
tell
me
and
I
forget
,
teach
me
and
I
may
rememb
,
involv
me
and
I
learn
.
''
(
benjamin
franklin
)
``
the
more
I
read
,
the
more
I
acquir
,
the
more
certain
I
am
that
I
know
noth
.
''
(
voltair
)
``
you
live
and
learn
.
At
ani
rate
,
you
live
.
''
(
dougla
adam
,
mostli
harmless
)
If
learn
mean
live
,
some
comput
live
!
``
In
learn
you
will
teach
,
and
in
teach
you
will
learn
.
''
(
phil
collin
)
thi
websit
is
creat
by
:
python
train
cours
in
toronto
,
canada
On
site
train
in
europ
,
canada
and
the
US
.
thi
websit
is
free
of
annoy
ad
.
We
want
to
keep
it
like
thi
.
you
can
help
with
your
donat
:
the
need
for
donat
bernd
klein
on
facebook
search
thi
websit
:
classroom
train
cours
thi
websit
contain
a
free
and
extens
onlin
tutori
by
bernd
klein
,
use
materi
from
hi
classroom
python
train
cours
.
If
you
are
interest
in
an
instructor-l
classroom
train
cours
,
you
may
have
a
look
at
the
python
class
by
bernd
klein
at
bodenseo
.
©
kabliczech
-
fotolia.com
quot
of
the
day
:
''
If
you
use
the
origin
world
wide
web
program
,
you
never
see
a
url
or
have
to
deal
with
html
.
that
wa
a
surpris
to
me
that
peopl
were
prepar
to
painstakingli
write
html
.
''
(
tim
berner
lee
)
If
you
have
the
choic
work
with
python
2
or
python
3
,
we
recomend
to
switch
to
python
3
!
you
can
read
our
python
tutori
to
see
what
the
differ
are
.
data
protect
declar
data
protect
declar
previou
chapter
:
machin
learn
terminolog
next
chapter
:
neural
network
from
scratch
in
python
k-nearest-neighbor
classifi
``
show
me
who
your
friend
are
and
I
’
ll
tell
you
who
you
are
?
''
the
concept
of
the
k-nearest
neighbor
classifi
can
hardli
be
simpler
describ
.
thi
is
an
old
say
,
which
can
be
found
in
mani
languag
and
mani
cultur
.
It
's
also
metnion
in
other
word
in
the
bibl
:
``
He
who
walk
with
wise
men
will
be
wise
,
but
the
companion
of
fool
will
suffer
harm
''
(
proverb
13:20
)
thi
mean
that
the
concept
of
the
k-nearest
neighbor
classifi
is
part
of
our
everyday
life
and
judg
:
imagin
you
meet
a
group
of
peopl
,
they
are
all
veri
young
,
stylish
and
sportiv
.
they
talk
about
there
friend
ben
,
who
is
n't
with
them
.
So
,
what
is
your
imagin
of
ben
?
right
,
you
imagin
him
as
be
yong
,
stylish
and
sportiv
as
well
.
If
you
learn
that
ben
live
in
a
neighborhood
where
peopl
vote
conserv
and
that
the
averag
incom
is
abov
200000
dollar
a
year
?
both
hi
neighbor
make
even
more
than
300,000
dollar
per
year
?
what
do
you
think
of
ben
?
most
probabl
,
you
do
not
consid
him
to
be
an
underdog
and
you
may
suspect
him
to
be
a
conserv
as
well
?
the
principl
behind
nearest
neighbor
classif
consist
in
find
a
predefin
number
,
i.e
.
the
'
k
'
-
of
train
sampl
closest
in
distanc
to
a
new
sampl
,
which
ha
to
be
classifi
.
the
label
of
the
new
sampl
will
be
defin
from
these
neighbor
.
k-nearest
neighbor
classifi
have
a
fix
user
defin
constant
for
the
number
of
neighbor
which
have
to
be
determin
.
there
are
also
radius-bas
neighbor
learn
algorithm
,
which
have
a
vari
number
of
neighbor
base
on
the
local
densiti
of
point
,
all
the
sampl
insid
of
a
fix
radiu
.
the
distanc
can
,
in
gener
,
be
ani
metric
measur
:
standard
euclidean
distanc
is
the
most
common
choic
.
neighbors-bas
method
are
known
as
non-gener
machin
learn
method
,
sinc
they
simpli
``
rememb
''
all
of
it
train
data
.
classif
can
be
comput
by
a
major
vote
of
the
nearest
neighbor
of
the
unknown
sampl
.
the
k-nn
algorithm
is
among
the
simplest
of
all
machin
learn
algorithm
,
but
despit
it
simplic
,
it
ha
been
quit
success
in
a
larg
number
of
classif
and
regress
problem
,
for
exampl
charact
recognit
or
imag
analysi
.
now
let
's
get
a
littl
bit
more
mathemat
:
the
k-nearest-neighbor
classifi
(
k-nn
)
work
directli
on
the
learn
sampl
,
instead
of
creat
rule
compar
to
other
classif
method
.
nearest
neighbor
algorithm
:
given
a
set
of
categori
$
\
{
c_1
,
c_2
,
...
c_n\
}
$
,
also
call
class
,
e.g
.
{
``
male
''
,
``
femal
''
}
.
there
is
also
a
learnset
$
LS
$
consist
of
label
instanc
.
the
task
of
classif
consist
in
assign
a
categori
or
class
to
an
arbitrari
instanc
.
If
the
instanc
$
o
$
is
an
element
of
$
LS
$
,
the
label
of
the
instanc
will
be
use
.
now
,
we
will
look
at
the
case
where
$
o
$
is
not
in
$
LS
$
:
$
o
$
is
compar
with
all
instanc
of
$
LS
$
.
A
distanc
metric
is
use
for
comparison
.
We
determin
the
$
k
$
closest
neighbor
of
$
o
$
,
i.e
.
the
item
with
the
smallest
distanc
.
$
k
$
is
a
user
defin
constant
and
a
posit
integ
,
which
is
usual
small
.
the
most
common
class
of
$
LS
$
will
be
assign
to
the
instanc
$
o
$
.
If
k
=
1
,
then
the
object
is
simpli
assign
to
the
class
of
that
singl
nearest
neighbor
.
the
algorithm
for
the
k-nearest
neighbor
classifi
is
among
the
simplest
of
all
machin
learn
algorithm
.
k-nn
is
a
type
of
instance-bas
learn
,
or
lazi
learn
,
where
the
function
is
onli
approxim
local
and
all
the
comput
are
perform
,
when
we
do
the
actual
classif
.
k-nearest-neighbor
from
scratchprepar
the
dataset
befor
we
actual
start
with
write
a
nearest
neighbor
classifi
,
we
need
to
think
about
the
data
,
i.e
.
the
learnset
.
We
will
use
the
``
iri
''
dataset
provid
by
the
dataset
of
the
sklearn
modul
.
the
data
set
consist
of
50
sampl
from
each
of
three
speci
of
iri
iri
setosa
,
iri
virginica
and
iri
versicolor
.
four
featur
were
measur
from
each
sampl
:
the
length
and
the
width
of
the
sepal
and
petal
,
in
centimetr
.
import
numpi
as
np
from
sklearn
import
dataset
iri
=
datasets.load_iri
(
)
iris_data
=
iris.data
iris_label
=
iris.target
print
(
iris_data
[
0
]
,
iris_data
[
79
]
,
iris_data
[
100
]
)
print
(
iris_label
[
0
]
,
iris_label
[
79
]
,
iris_label
[
100
]
)
[
5.1
3.5
1.4
0.2
]
[
5.7
2.6
3.5
1
.
]
[
6.3
3.3
6
.
2.5
]
0
1
2
We
creat
a
learnset
from
the
set
abov
.
We
use
permut
from
np.random
to
split
the
data
randomli
.
np.random.se
(
42
)
indic
=
np.random.permut
(
len
(
iris_data
)
)
n_training_sampl
=
12
learnset_data
=
iris_data
[
indic
[
:
-n_training_sampl
]
]
learnset_label
=
iris_label
[
indic
[
:
-n_training_sampl
]
]
testset_data
=
iris_data
[
indic
[
-n_training_sampl
:
]
]
testset_label
=
iris_label
[
indic
[
-n_training_sampl
:
]
]
print
(
learnset_data
[
:4
]
,
learnset_label
[
:4
]
)
print
(
testset_data
[
:4
]
,
testset_label
[
:4
]
)
[
[
6.1
2.8
4.7
1.2
]
[
5.7
3.8
1.7
0.3
]
[
7.7
2.6
6.9
2.3
]
[
6
.
2.9
4.5
1.5
]
]
[
1
0
2
1
]
[
[
5.7
2.8
4.1
1.3
]
[
6.5
3
.
5.5
1.8
]
[
6.3
2.3
4.4
1.3
]
[
6.4
2.9
4.3
1.3
]
]
[
1
2
1
1
]
the
follow
code
is
onli
necessari
to
visual
the
data
of
our
learnset
.
our
data
consist
of
four
valu
per
iri
item
,
so
we
will
reduc
the
data
to
three
valu
by
sum
up
the
third
and
fourth
valu
.
thi
way
,
we
are
capabl
of
depict
the
data
in
3-dimension
space
:
#
follow
line
is
onli
necessari
,
if
you
use
ipython
notebook
!
!
!
%
matplotlib
inlin
import
matplotlib.pyplot
as
plt
from
mpl_toolkits.mplot3d
import
axes3d
colour
=
(
``
r
''
,
``
b
''
)
X
=
[
]
for
iclass
in
rang
(
3
)
:
x.append
(
[
[
]
,
[
]
,
[
]
]
)
for
i
in
rang
(
len
(
learnset_data
)
)
:
if
learnset_label
[
i
]
==
iclass
:
X
[
iclass
]
[
0
]
.append
(
learnset_data
[
i
]
[
0
]
)
X
[
iclass
]
[
1
]
.append
(
learnset_data
[
i
]
[
1
]
)
X
[
iclass
]
[
2
]
.append
(
sum
(
learnset_data
[
i
]
[
2
:
]
)
)
colour
=
(
``
r
''
,
``
g
''
,
``
y
''
)
fig
=
plt.figur
(
)
ax
=
fig.add_subplot
(
111
,
projection='3d
'
)
for
iclass
in
rang
(
3
)
:
ax.scatt
(
X
[
iclass
]
[
0
]
,
X
[
iclass
]
[
1
]
,
X
[
iclass
]
[
2
]
,
c=colour
[
iclass
]
)
plt.show
(
)
determin
the
neighborsto
determin
the
similar
between
two
instanc
,
we
need
a
distanc
function
.
In
our
exampl
,
the
euclidean
distanc
is
ideal
:
def
distanc
(
instance1
,
instance2
)
:
#
just
in
case
,
if
the
instanc
are
list
or
tupl
:
instance1
=
np.array
(
instance1
)
instance2
=
np.array
(
instance2
)
return
np.linalg.norm
(
instance1
-
instance2
)
print
(
distanc
(
[
3
,
5
]
,
[
1
,
1
]
)
)
print
(
distanc
(
learnset_data
[
3
]
,
learnset_data
[
44
]
)
)
4.47213595499958
3.4190641994557516
the
function
'get_neighbor
return
a
list
with
'
k
'
neighbor
,
which
are
closest
to
the
instanc
'test_inst
'
:
def
get_neighbor
(
training_set
,
label
,
test_inst
,
k
,
distance=dist
)
:
``
''
''
get_neighor
calcul
a
list
of
the
k
nearest
neighbor
of
an
instanc
'test_inst
'
.
the
list
neighbor
contain
3-tupl
with
(
index
,
dist
,
label
)
where
index
is
the
index
from
the
training_set
,
dist
is
the
distanc
between
the
test_inst
and
the
instanc
training_set
[
index
]
distanc
is
a
refer
to
a
function
use
to
calcul
the
distanc
``
''
''
distanc
=
[
]
for
index
in
rang
(
len
(
training_set
)
)
:
dist
=
distanc
(
test_inst
,
training_set
[
index
]
)
distances.append
(
(
training_set
[
index
]
,
dist
,
label
[
index
]
)
)
distances.sort
(
key=lambda
x
:
x
[
1
]
)
neighbor
=
distanc
[
:
k
]
return
neighbor
We
will
test
the
function
with
our
iri
sampl
:
for
i
in
rang
(
5
)
:
neighbor
=
get_neighbor
(
learnset_data
,
learnset_label
,
testset_data
[
i
]
,
3
,
distance=dist
)
print
(
i
,
testset_data
[
i
]
,
testset_label
[
i
]
,
neighbor
)
0
[
5.7
2.8
4.1
1.3
]
1
[
(
array
(
[
5.7
,
2.9
,
4.2
,
1.3
]
)
,
0.14142135623730995
,
1
)
,
(
array
(
[
5.6
,
2.7
,
4.2
,
1.3
]
)
,
0.17320508075688815
,
1
)
,
(
array
(
[
5.6
,
3.
,
4.1
,
1.3
]
)
,
0.22360679774997935
,
1
)
]
1
[
6.5
3
.
5.5
1.8
]
2
[
(
array
(
[
6.4
,
3.1
,
5.5
,
1.8
]
)
,
0.1414213562373093
,
2
)
,
(
array
(
[
6.3
,
2.9
,
5.6
,
1.8
]
)
,
0.24494897427831783
,
2
)
,
(
array
(
[
6.5
,
3.
,
5.2
,
2
.
]
)
,
0.3605551275463988
,
2
)
]
2
[
6.3
2.3
4.4
1.3
]
1
[
(
array
(
[
6.2
,
2.2
,
4.5
,
1.5
]
)
,
0.2645751311064586
,
1
)
,
(
array
(
[
6.3
,
2.5
,
4.9
,
1.5
]
)
,
0.574456264653803
,
1
)
,
(
array
(
[
6.
,
2.2
,
4.
,
1
.
]
)
,
0.5916079783099617
,
1
)
]
3
[
6.4
2.9
4.3
1.3
]
1
[
(
array
(
[
6.2
,
2.9
,
4.3
,
1.3
]
)
,
0.20000000000000018
,
1
)
,
(
array
(
[
6.6
,
3.
,
4.4
,
1.4
]
)
,
0.2645751311064587
,
1
)
,
(
array
(
[
6.6
,
2.9
,
4.6
,
1.3
]
)
,
0.3605551275463984
,
1
)
]
4
[
5.6
2.8
4.9
2
.
]
2
[
(
array
(
[
5.8
,
2.7
,
5.1
,
1.9
]
)
,
0.3162277660168375
,
2
)
,
(
array
(
[
5.8
,
2.7
,
5.1
,
1.9
]
)
,
0.3162277660168375
,
2
)
,
(
array
(
[
5.7
,
2.5
,
5.
,
2
.
]
)
,
0.33166247903553986
,
2
)
]
vote
to
get
a
singl
resultw
will
write
a
vote
function
now
.
thi
function
use
the
class
'counter
'
from
collect
to
count
the
quantiti
of
the
class
insid
of
an
instanc
list
.
thi
instanc
list
will
be
the
neighbor
of
cours
.
the
function
'vote
'
return
the
most
common
class
:
from
collect
import
counter
def
vote
(
neighbor
)
:
class_count
=
counter
(
)
for
neighbor
in
neighbor
:
class_count
[
neighbor
[
2
]
]
+=
1
return
class_counter.most_common
(
1
)
[
0
]
[
0
]
We
will
test
'vote
'
on
our
train
sampl
:
for
i
in
rang
(
n_training_sampl
)
:
neighbor
=
get_neighbor
(
learnset_data
,
learnset_label
,
testset_data
[
i
]
,
3
,
distance=dist
)
print
(
``
index
:
``
,
i
,
``
,
result
of
vote
:
``
,
vote
(
neighbor
)
,
``
,
label
:
``
,
testset_label
[
i
]
,
``
,
data
:
``
,
testset_data
[
i
]
)
index
:
0
,
result
of
vote
:
1
,
label
:
1
,
data
:
[
5.7
2.8
4.1
1.3
]
index
:
1
,
result
of
vote
:
2
,
label
:
2
,
data
:
[
6.5
3
.
5.5
1.8
]
index
:
2
,
result
of
vote
:
1
,
label
:
1
,
data
:
[
6.3
2.3
4.4
1.3
]
index
:
3
,
result
of
vote
:
1
,
label
:
1
,
data
:
[
6.4
2.9
4.3
1.3
]
index
:
4
,
result
of
vote
:
2
,
label
:
2
,
data
:
[
5.6
2.8
4.9
2
.
]
index
:
5
,
result
of
vote
:
2
,
label
:
2
,
data
:
[
5.9
3
.
5.1
1.8
]
index
:
6
,
result
of
vote
:
0
,
label
:
0
,
data
:
[
5.4
3.4
1.7
0.2
]
index
:
7
,
result
of
vote
:
1
,
label
:
1
,
data
:
[
6.1
2.8
4
.
1.3
]
index
:
8
,
result
of
vote
:
1
,
label
:
2
,
data
:
[
4.9
2.5
4.5
1.7
]
index
:
9
,
result
of
vote
:
0
,
label
:
0
,
data
:
[
5.8
4
.
1.2
0.2
]
index
:
10
,
result
of
vote
:
1
,
label
:
1
,
data
:
[
5.8
2.6
4
.
1.2
]
index
:
11
,
result
of
vote
:
2
,
label
:
2
,
data
:
[
7.1
3
.
5.9
2.1
]
We
can
see
that
the
predict
correspond
to
the
label
result
,
except
in
case
of
the
item
with
the
index
8
.
'vote_prob
'
is
a
function
like
'vote
'
but
return
the
class
name
and
the
probabl
for
thi
class
:
def
vote_prob
(
neighbor
)
:
class_count
=
counter
(
)
for
neighbor
in
neighbor
:
class_count
[
neighbor
[
2
]
]
+=
1
label
,
vote
=
zip
(
*class_counter.most_common
(
)
)
winner
=
class_counter.most_common
(
1
)
[
0
]
[
0
]
votes4winn
=
class_counter.most_common
(
1
)
[
0
]
[
1
]
return
winner
,
votes4winner/sum
(
vote
)
for
i
in
rang
(
n_training_sampl
)
:
neighbor
=
get_neighbor
(
learnset_data
,
learnset_label
,
testset_data
[
i
]
,
5
,
distance=dist
)
print
(
``
index
:
``
,
i
,
``
,
vote_prob
:
``
,
vote_prob
(
neighbor
)
,
``
,
label
:
``
,
testset_label
[
i
]
,
``
,
data
:
``
,
testset_data
[
i
]
)
index
:
0
,
vote_prob
:
(
1
,
1.0
)
,
label
:
1
,
data
:
[
5.7
2.8
4.1
1.3
]
index
:
1
,
vote_prob
:
(
2
,
1.0
)
,
label
:
2
,
data
:
[
6.5
3
.
5.5
1.8
]
index
:
2
,
vote_prob
:
(
1
,
1.0
)
,
label
:
1
,
data
:
[
6.3
2.3
4.4
1.3
]
index
:
3
,
vote_prob
:
(
1
,
1.0
)
,
label
:
1
,
data
:
[
6.4
2.9
4.3
1.3
]
index
:
4
,
vote_prob
:
(
2
,
1.0
)
,
label
:
2
,
data
:
[
5.6
2.8
4.9
2
.
]
index
:
5
,
vote_prob
:
(
2
,
0.8
)
,
label
:
2
,
data
:
[
5.9
3
.
5.1
1.8
]
index
:
6
,
vote_prob
:
(
0
,
1.0
)
,
label
:
0
,
data
:
[
5.4
3.4
1.7
0.2
]
index
:
7
,
vote_prob
:
(
1
,
1.0
)
,
label
:
1
,
data
:
[
6.1
2.8
4
.
1.3
]
index
:
8
,
vote_prob
:
(
1
,
1.0
)
,
label
:
2
,
data
:
[
4.9
2.5
4.5
1.7
]
index
:
9
,
vote_prob
:
(
0
,
1.0
)
,
label
:
0
,
data
:
[
5.8
4
.
1.2
0.2
]
index
:
10
,
vote_prob
:
(
1
,
1.0
)
,
label
:
1
,
data
:
[
5.8
2.6
4
.
1.2
]
index
:
11
,
vote_prob
:
(
2
,
1.0
)
,
label
:
2
,
data
:
[
7.1
3
.
5.9
2.1
]
the
weight
nearest
neighbour
classifierw
look
onli
at
k
item
in
the
vicin
of
an
unknown
object
„
UO
''
,
and
had
a
major
vote
.
use
the
major
vote
ha
shown
quit
effici
in
our
previou
exampl
,
but
thi
did
n't
take
into
account
the
follow
reason
:
the
farther
a
neighbor
is
,
the
more
it
``
deviat
''
from
the
``
real
''
result
.
Or
in
other
word
,
we
can
trust
the
closest
neighbor
more
than
the
farther
one
.
let
's
assum
,
we
have
11
neighbor
of
an
unknown
item
UO
.
the
closest
five
neighbor
belong
to
a
class
A
and
all
the
other
six
,
which
are
farther
away
belong
to
a
class
B
.
what
class
should
be
assign
to
UO
?
the
previou
approach
say
B
,
becaus
we
have
a
6
to
5
vote
in
favor
of
B
.
On
the
other
hand
the
closest
5
are
all
A
and
thi
should
count
more
.
To
pursu
thi
strategi
,
we
can
assign
weight
to
the
neighbor
in
the
follow
way
:
the
nearest
neighbor
of
an
instanc
get
a
weight
$
1
/
1
$
,
the
second
closest
get
a
weight
of
$
1
/
2
$
and
then
go
on
up
to
$
1/k
$
for
the
farthest
away
neighbor
.
thi
mean
that
we
are
use
the
harmon
seri
as
weight
:
$
$
\sum_
{
i
}
^
{
k
}
{
1/
(
i+1
)
}
=
1
+
\frac
{
1
}
{
2
}
+
\frac
{
1
}
{
3
}
+
...
+
\frac
{
1
}
{
k
}
$
$
We
implement
thi
in
the
follow
function
:
def
vote_harmonic_weight
(
neighbor
,
all_results=tru
)
:
class_count
=
counter
(
)
number_of_neighbor
=
len
(
neighbor
)
for
index
in
rang
(
number_of_neighbor
)
:
class_count
[
neighbor
[
index
]
[
2
]
]
+=
1/
(
index+1
)
label
,
vote
=
zip
(
*class_counter.most_common
(
)
)
#
print
(
label
,
vote
)
winner
=
class_counter.most_common
(
1
)
[
0
]
[
0
]
votes4winn
=
class_counter.most_common
(
1
)
[
0
]
[
1
]
if
all_result
:
total
=
sum
(
class_counter.valu
(
)
,
0.0
)
for
key
in
class_count
:
class_count
[
key
]
/=
total
return
winner
,
class_counter.most_common
(
)
els
:
return
winner
,
votes4winn
/
sum
(
vote
)
for
i
in
rang
(
n_training_sampl
)
:
neighbor
=
get_neighbor
(
learnset_data
,
learnset_label
,
testset_data
[
i
]
,
6
,
distance=dist
)
print
(
``
index
:
``
,
i
,
``
,
result
of
vote
:
``
,
vote_harmonic_weight
(
neighbor
,
all_results=tru
)
)
index
:
0
,
result
of
vote
:
(
1
,
[
(
1
,
1.0
)
]
)
index
:
1
,
result
of
vote
:
(
2
,
[
(
2
,
1.0
)
]
)
index
:
2
,
result
of
vote
:
(
1
,
[
(
1
,
1.0
)
]
)
index
:
3
,
result
of
vote
:
(
1
,
[
(
1
,
1.0
)
]
)
index
:
4
,
result
of
vote
:
(
2
,
[
(
2
,
0.9319727891156463
)
,
(
1
,
0.06802721088435375
)
]
)
index
:
5
,
result
of
vote
:
(
2
,
[
(
2
,
0.8503401360544217
)
,
(
1
,
0.14965986394557826
)
]
)
index
:
6
,
result
of
vote
:
(
0
,
[
(
0
,
1.0
)
]
)
index
:
7
,
result
of
vote
:
(
1
,
[
(
1
,
1.0
)
]
)
index
:
8
,
result
of
vote
:
(
1
,
[
(
1
,
1.0
)
]
)
index
:
9
,
result
of
vote
:
(
0
,
[
(
0
,
1.0
)
]
)
index
:
10
,
result
of
vote
:
(
1
,
[
(
1
,
1.0
)
]
)
index
:
11
,
result
of
vote
:
(
2
,
[
(
2
,
1.0
)
]
)
the
previou
approach
took
onli
the
rank
of
the
neighbor
accord
to
their
distanc
in
account
.
We
can
improv
the
vote
by
use
the
actual
distanc
.
To
thi
purpo
we
will
write
a
new
vote
function
:
def
vote_distance_weight
(
neighbor
,
all_results=tru
)
:
class_count
=
counter
(
)
number_of_neighbor
=
len
(
neighbor
)
for
index
in
rang
(
number_of_neighbor
)
:
dist
=
neighbor
[
index
]
[
1
]
label
=
neighbor
[
index
]
[
2
]
class_count
[
label
]
+=
1
/
(
dist**2
+
1
)
label
,
vote
=
zip
(
*class_counter.most_common
(
)
)
#
print
(
label
,
vote
)
winner
=
class_counter.most_common
(
1
)
[
0
]
[
0
]
votes4winn
=
class_counter.most_common
(
1
)
[
0
]
[
1
]
if
all_result
:
total
=
sum
(
class_counter.valu
(
)
,
0.0
)
for
key
in
class_count
:
class_count
[
key
]
/=
total
return
winner
,
class_counter.most_common
(
)
els
:
return
winner
,
votes4winn
/
sum
(
vote
)
for
i
in
rang
(
n_training_sampl
)
:
neighbor
=
get_neighbor
(
learnset_data
,
learnset_label
,
testset_data
[
i
]
,
6
,
distance=dist
)
print
(
``
index
:
``
,
i
,
``
,
result
of
vote
:
``
,
vote_distance_weight
(
neighbor
,
all_results=tru
)
)
index
:
0
,
result
of
vote
:
(
1
,
[
(
1
,
1.0
)
]
)
index
:
1
,
result
of
vote
:
(
2
,
[
(
2
,
1.0
)
]
)
index
:
2
,
result
of
vote
:
(
1
,
[
(
1
,
1.0
)
]
)
index
:
3
,
result
of
vote
:
(
1
,
[
(
1
,
1.0
)
]
)
index
:
4
,
result
of
vote
:
(
2
,
[
(
2
,
0.8490154592118361
)
,
(
1
,
0.15098454078816387
)
]
)
index
:
5
,
result
of
vote
:
(
2
,
[
(
2
,
0.6736137462184478
)
,
(
1
,
0.3263862537815521
)
]
)
index
:
6
,
result
of
vote
:
(
0
,
[
(
0
,
1.0
)
]
)
index
:
7
,
result
of
vote
:
(
1
,
[
(
1
,
1.0
)
]
)
index
:
8
,
result
of
vote
:
(
1
,
[
(
1
,
1.0
)
]
)
index
:
9
,
result
of
vote
:
(
0
,
[
(
0
,
1.0
)
]
)
index
:
10
,
result
of
vote
:
(
1
,
[
(
1
,
1.0
)
]
)
index
:
11
,
result
of
vote
:
(
2
,
[
(
2
,
1.0
)
]
)
anoth
exampl
for
nearest
neighbor
classificationw
want
to
test
the
previou
function
with
anoth
veri
simpl
dataset
:
train_set
=
[
(
1
,
2
,
2
)
,
(
-3
,
-2
,
0
)
,
(
1
,
1
,
3
)
,
(
-3
,
-3
,
-1
)
,
(
-3
,
-2
,
-0.5
)
,
(
0
,
0.3
,
0.8
)
,
(
-0.5
,
0.6
,
0.7
)
,
(
0
,
0
,
0
)
]
label
=
[
'appl
'
,
'banana
'
,
'appl
'
,
'banana
'
,
'appl
'
,
``
orang
''
,
'orang
'
,
'orang
'
]
k
=
1
for
test_inst
in
[
(
0
,
0
,
0
)
,
(
2
,
2
,
2
)
,
(
-3
,
-1
,
0
)
,
(
0
,
1
,
0.9
)
,
(
1
,
1.5
,
1.8
)
,
(
0.9
,
0.8
,
1.6
)
]
:
neighbor
=
get_neighbor
(
train_set
,
label
,
test_inst
,
2
)
print
(
``
vote
distanc
weight
:
``
,
vote_distance_weight
(
neighbor
)
)
vote
distanc
weight
:
(
'orang
'
,
[
(
'orang
'
,
1.0
)
]
)
vote
distanc
weight
:
(
'appl
'
,
[
(
'appl
'
,
1.0
)
]
)
vote
distanc
weight
:
(
'banana
'
,
[
(
'banana
'
,
0.5294117647058824
)
,
(
'appl
'
,
0.47058823529411764
)
]
)
vote
distanc
weight
:
(
'orang
'
,
[
(
'orang
'
,
1.0
)
]
)
vote
distanc
weight
:
(
'appl
'
,
[
(
'appl
'
,
1.0
)
]
)
vote
distanc
weight
:
(
'appl
'
,
[
(
'appl
'
,
0.5084745762711865
)
,
(
'orang
'
,
0.4915254237288135
)
]
)
knn
in
linguisticsth
next
exampl
come
from
comput
linguist
.
We
show
how
we
can
use
a
k-nearest
neighbor
classifi
to
recogn
misspel
word
.
We
use
a
modul
call
levenshtein
,
which
we
have
implement
in
our
tutori
on
levenshtein
distanc
.
from
levenshtein
import
levenshtein
citi
=
[
]
with
open
(
``
data/city_names.txt
''
)
as
fh
:
for
line
in
fh
:
citi
=
line.strip
(
)
if
``
``
in
citi
:
#
like
freiburg
im
breisgau
add
citi
onli
as
well
cities.append
(
city.split
(
)
[
0
]
)
cities.append
(
citi
)
#
citi
=
citi
[
:20
]
for
citi
in
[
``
freiburg
''
,
``
frieburg
''
,
``
freiborg
''
,
``
hamborg
''
,
``
sahrlui
''
]
:
neighbor
=
get_neighbor
(
citi
,
citi
,
citi
,
2
,
distance=levenshtein
)
print
(
``
vote_distance_weight
:
``
,
vote_distance_weight
(
neighbor
)
)
vote_distance_weight
:
(
'freiburg
'
,
[
(
'freiburg
'
,
0.6666666666666666
)
,
(
'freiberg
'
,
0.3333333333333333
)
]
)
vote_distance_weight
:
(
'freiburg
'
,
[
(
'freiburg
'
,
0.6666666666666666
)
,
(
'lüneburg
'
,
0.3333333333333333
)
]
)
vote_distance_weight
:
(
'freiburg
'
,
[
(
'freiburg
'
,
0.5
)
,
(
'freiberg
'
,
0.5
)
]
)
vote_distance_weight
:
(
'hamburg
'
,
[
(
'hamburg
'
,
0.7142857142857143
)
,
(
'bamberg
'
,
0.28571428571428575
)
]
)
vote_distance_weight
:
(
'saarloui
'
,
[
(
'saarloui
'
,
0.8387096774193549
)
,
(
'bayreuth
'
,
0.16129032258064516
)
]
)
If
you
work
under
linux
(
especi
ubuntu
)
,
you
can
find
a
file
with
a
british-english
dictionari
under
/usr/share/dict/british-english
.
window
user
and
other
can
download
the
file
as
british-english.txt
We
use
extrem
misspel
word
in
the
follow
exampl
.
We
see
that
our
simpl
vote_prob
function
is
do
well
onli
in
two
case
:
In
correct
``
holpposs
''
to
``
helpless
''
and
``
blagrufoo
''
to
``
barefoot
''
.
wherea
our
distanc
vote
is
do
well
in
all
case
.
okay
,
we
have
to
admit
that
we
had
``
liberti
''
in
mind
,
when
we
wrote
``
liberdi
''
,
but
suggest
``
liber
''
is
a
good
choic
.
word
=
[
]
with
open
(
``
british-english.txt
''
)
as
fh
:
for
line
in
fh
:
word
=
line.strip
(
)
words.append
(
word
)
for
word
in
[
``
holp
''
,
``
kundnoss
''
,
``
holpposs
''
,
``
blagrufoo
''
,
``
liberdi
''
]
:
neighbor
=
get_neighbor
(
word
,
word
,
word
,
3
,
distance=levenshtein
)
print
(
``
vote_distance_weight
:
``
,
vote_distance_weight
(
neighbor
,
all_results=fals
)
)
print
(
``
vote_prob
:
``
,
vote_prob
(
neighbor
)
)
vote_distance_weight
:
(
'help
'
,
0.5555555555555556
)
vote_prob
:
(
'help
'
,
0.3333333333333333
)
vote_distance_weight
:
(
'kind
'
,
0.5
)
vote_prob
:
(
'kind
'
,
0.3333333333333333
)
vote_distance_weight
:
(
'helpless
'
,
0.3333333333333333
)
vote_prob
:
(
'helpless
'
,
0.3333333333333333
)
vote_distance_weight
:
(
'barefoot
'
,
0.4333333333333333
)
vote_prob
:
(
'barefoot
'
,
0.3333333333333333
)
vote_distance_weight
:
(
'liber
'
,
0.4
)
vote_prob
:
(
'liber
'
,
0.3333333333333333
)
use
sklearn
for
knnneighbor
is
a
packag
of
the
sklearn
,
which
provid
function
for
nearest
neighbor
classifi
both
for
unsupervis
and
supervis
learn
.
the
class
in
sklearn.neighbor
can
handl
both
numpi
array
and
scipy.spars
matric
as
input
.
for
dens
matric
,
a
larg
number
of
possibl
distanc
metric
are
support
.
for
spars
matric
,
arbitrari
minkowski
metric
are
support
for
search
.
scikit-learn
implement
two
differ
nearest
neighbor
classifi
:
kneighborsclassifi
is
base
on
the
k
nearest
neighbor
of
a
sampl
,
which
ha
to
be
classifi
.
the
number
'
k
'
is
an
integ
valu
specifi
by
the
user
.
thi
is
the
most
frequent
use
classifi
of
both
algorithm
.
radiusneighborsclassifi
is
base
on
the
number
of
neighbor
within
a
fix
radiu
r
for
each
sampl
which
ha
to
be
classifi
.
'
r
'
is
float
valu
specifi
by
the
user
.
thi
classifi
is
less
often
use
.
there
is
no
gener
way
to
defin
an
optim
valu
for
'
k
'
.
thi
valu
depend
on
the
data
.
As
a
gener
rule
we
can
say
that
increas
'
k
'
reduc
the
nois
but
on
the
other
hand
make
the
boundari
less
distinct
.
the
decis
base
on
the
nearest
neighbor
can
be
reach
either
uniform
weight
,
the
class
assign
to
a
queri
sampl
is
calcul
by
a
simpl
major
vote
of
the
k-nearest
neighbor
.
thi
doe
not
take
into
account
that
the
neighbor
closer
to
the
sampl
should
contribut
more
than
the
one
further
away
.
the
weight
can
be
control
by
the
weight
keyword
:
weight
=
'uniform
'
assign
uniform
weight
to
each
neighbor
.
thi
is
also
the
default
valu
.
weight
=
'distanc
'
assign
weight
proport
to
the
invers
of
the
distanc
from
the
queri
sampl
.
It
is
also
possibl
to
suppli
a
user-defin
function
to
comput
the
distanc
.
paramet
n_neighbor
int
,
option
(
default
=
5
)
number
of
neighbor
to
use
by
default
for
meth
:
'kneighbor
'
queri
.
weight
str
or
callabl
,
option
(
default
=
'uniform
'
)
weight
function
use
in
predict
.
possibl
valu
:
'uniform
'
:
uniform
weight
.
all
point
in
each
neighborhood
are
weight
equal
.
'distanc
'
:
weight
point
by
the
invers
of
their
distanc
.
in
thi
case
,
closer
neighbor
of
a
queri
point
will
have
a
greater
influenc
than
neighbor
which
are
further
away
.
[
callabl
]
:
a
user-defin
function
which
accept
an
array
of
distanc
,
and
return
an
array
of
the
same
shape
contain
the
weight
.
algorithm
option
algorithm
use
to
comput
the
nearest
neighbor
:
'ball_tre
'
will
use
:
class
:
'balltre
'
'kd_tree
'
will
use
:
class
:
'kdtree
'
'brute
'
will
use
a
brute-forc
search
.
'auto
'
will
attempt
to
decid
the
most
appropri
algorithm
base
on
the
valu
pass
to
:
meth
:
'fit
'
method
.
note
:
fit
on
spars
input
will
overrid
the
set
of
thi
paramet
,
use
brute
forc
.
leaf_siz
int
,
option
(
default
=
30
)
leaf
size
pass
to
balltre
or
kdtree
.
thi
can
affect
the
speed
of
the
construct
and
queri
,
as
well
as
the
memori
requir
to
store
the
tree
.
the
optim
valu
depend
on
the
natur
of
the
problem
.
p
integ
,
option
(
default
=
2
)
power
paramet
for
the
minkowski
metric
.
when
p
=
1
,
thi
is
equival
to
use
manhattan_dist
(
l1
)
,
and
euclidean_dist
(
l2
)
for
p
=
2
.
for
arbitrari
p
,
minkowski_dist
(
l_p
)
is
use
.
metric
string
or
callabl
,
default
'minkowski
'
the
distanc
metric
to
use
for
the
tree
.
the
default
metric
is
minkowski
,
and
with
p=2
is
equival
to
the
standard
euclidean
metric
.
see
the
document
of
the
distancemetr
class
for
a
list
of
avail
metric
.
metric_param
dict
,
option
(
default
=
none
)
addit
keyword
argument
for
the
metric
function
.
n_job
int
,
option
(
default
=
1
)
the
number
of
parallel
job
to
run
for
neighbor
search
.
If
'-1
'
,
then
the
number
of
job
is
set
to
the
number
of
cpu
core
.
doe
n't
affect
:
meth
:
'fit
'
method
.
exampl
with
knnwe
will
use
the
k-nearest
neighbor
classifi
'kneighborsclassifi
'
from
'sklearn.neighbor
'
on
the
iri
data
set
:
#
creat
and
fit
a
nearest-neighbor
classifi
from
sklearn.neighbor
import
kneighborsclassifi
knn
=
kneighborsclassifi
(
)
knn.fit
(
learnset_data
,
learnset_label
)
kneighborsclassifi
(
algorithm='auto
'
,
leaf_size=30
,
metric='minkowski
'
,
metric_params=non
,
n_jobs=1
,
n_neighbors=5
,
p=2
,
weights='uniform
'
)
print
(
``
predict
form
the
classifi
:
''
)
print
(
knn.predict
(
testset_data
)
)
print
(
``
target
valu
:
''
)
print
(
testset_label
)
predict
form
the
classifi
:
[
1
2
1
1
2
2
0
1
1
0
1
2
]
target
valu
:
[
1
2
1
1
2
2
0
1
2
0
1
2
]
learnset_data
[
:5
]
,
learnset_label
[
:5
]
the
python
code
abov
return
the
follow
:
(
array
(
[
[
6.1
,
2.8
,
4.7
,
1.2
]
,
[
5.7
,
3.8
,
1.7
,
0.3
]
,
[
7.7
,
2.6
,
6.9
,
2.3
]
,
[
6.
,
2.9
,
4.5
,
1.5
]
,
[
6.8
,
2.8
,
4.8
,
1.4
]
]
)
,
array
(
[
1
,
0
,
2
,
1
,
1
]
)
)
We
can
see
that
the
sampl
with
the
index
8
is
again
not
recogn
properli
.
previou
chapter
:
machin
learn
terminolog
next
chapter
:
neural
network
from
scratch
in
python
©
2011
-
2018
,
bernd
klein
,
bodenseo
;
design
by
denis
mitchinson
adapt
for
python-course.eu
by
bernd
klein
