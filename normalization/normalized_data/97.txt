machin
learn
with
python
:
dropout
neural
network
in
python
python
machin
learn
tutori
machin
learn
machin
learn
terminologyk-nearest
neighbor
classifierneur
network
from
scratch
in
pythonneur
network
in
python
use
numypybackpropag
in
neural
networksconfus
matrixtrain
and
test
with
mnistdropout
neural
networksneur
network
with
scikitmachin
learn
with
scikit
and
pythonintroduct
naiv
bay
classifierna
bay
classifi
with
scikitintroduct
into
text
classif
use
naiv
bayespython
implement
of
text
classificationdecis
treesregress
treesrandom
forestsboost
algorithmprincip
compon
analysislinear
discrimin
analysisexpect
maxim
and
gaussian
mixtur
modelintroduct
into
tensorflow
python
In
greek
mytholog
,
python
is
the
name
of
a
a
huge
serpent
and
sometim
a
dragon
.
python
had
been
kill
by
the
god
apollo
at
delphi
.
python
wa
creat
out
of
the
slime
and
mud
left
after
the
great
flood
.
He
wa
appoint
by
gaia
(
mother
earth
)
to
guard
the
oracl
of
delphi
,
known
as
pytho
.
the
program
languag
python
ha
not
been
creat
out
of
slime
and
mud
but
out
of
the
program
languag
abc
.
It
ha
been
devis
by
a
dutch
programm
,
name
guido
van
rossum
,
in
amsterdam
.
origin
of
python
guido
van
rossum
wrote
the
follow
about
the
origin
of
python
in
a
foreword
for
the
book
``
program
python
''
by
mark
lutz
in
1996
:
``
over
six
year
ago
,
in
decemb
1989
,
I
wa
look
for
a
``
hobbi
''
program
project
that
would
keep
me
occupi
dure
the
week
around
christma
.
My
offic
(
a
government-run
research
lab
in
amsterdam
)
would
be
close
,
but
I
had
a
home
comput
,
and
not
much
els
on
my
hand
.
I
decid
to
write
an
interpret
for
the
new
script
languag
I
had
been
think
about
late
:
a
descend
of
abc
that
would
appeal
to
unix/c
hacker
.
I
chose
python
as
a
work
titl
for
the
project
,
be
in
a
slightli
irrever
mood
(
and
a
big
fan
of
monti
python
's
fli
circu
)
.
''
thi
websit
is
free
of
annoy
ad
.
We
want
to
keep
it
like
thi
.
you
can
help
with
your
donat
:
the
need
for
donat
bernd
klein
on
facebook
search
thi
websit
:
classroom
train
cours
thi
websit
contain
a
free
and
extens
onlin
tutori
by
bernd
klein
,
use
materi
from
hi
classroom
python
train
cours
.
If
you
are
interest
in
an
instructor-l
classroom
train
cours
,
you
may
have
a
look
at
the
python
class
by
bernd
klein
at
bodenseo
.
Â©
kabliczech
-
fotolia.com
quot
of
the
day
:
''
Do
n't
have
good
idea
if
you
are
n't
will
to
be
respons
for
them.
``
(
alan
perli
)
If
you
have
the
choic
work
with
python
2
or
python
3
,
we
recomend
to
switch
to
python
3
!
you
can
read
our
python
tutori
to
see
what
the
differ
are
.
data
protect
declar
data
protect
declar
previou
chapter
:
train
and
test
with
mnist
next
chapter
:
neural
network
with
scikit
dropout
neural
networksintroduct
the
term
``
dropout
''
is
use
for
a
techniqu
which
drop
out
some
node
of
the
network
.
drop
out
can
be
seen
as
temporarili
deactiv
or
ignor
neuron
of
the
network
.
thi
techniqu
is
appli
in
the
train
phase
to
reduc
overfit
effect
.
overfit
is
an
error
which
occur
when
a
network
is
too
close
fit
to
a
limit
set
of
input
sampl
.
the
basic
idea
behind
dropout
neural
network
is
to
dropout
node
so
that
the
network
can
concentr
on
other
featur
.
think
about
it
like
thi
.
you
watch
lot
of
film
from
your
favourit
actor
.
At
some
point
you
listen
to
the
radio
and
here
somebodi
in
an
interview
.
you
do
n't
recogn
your
favourit
actor
,
becaus
you
have
seen
onli
movi
and
your
are
a
visual
type
.
now
,
imagin
that
you
can
onli
listen
to
the
audio
track
of
the
film
.
In
thi
case
you
will
have
to
learn
to
differenti
the
voic
of
the
actress
and
actor
.
So
by
drop
out
the
visual
part
you
are
forc
tp
focu
on
the
sound
featur
!
thi
techniqu
ha
been
first
propos
in
a
paper
``
dropout
:
A
simpl
way
to
prevent
neural
network
from
overfit
''
by
nitish
srivastava
,
geoffrey
hinton
,
alex
krizhevski
,
ilya
sutskev
and
ruslan
salakhutdinov
in
2014
We
will
implement
in
our
tutori
on
machin
learn
in
python
a
python
class
which
is
capabl
of
dropout
.
modifi
the
weight
arraysif
we
deactiv
a
node
,
we
have
to
modifi
the
weight
array
accordingli
.
To
demonstr
how
thi
can
be
accomplish
,
we
will
use
a
network
with
three
input
node
,
four
hidden
and
two
output
node
:
At
first
,
we
will
have
a
look
at
the
weight
array
between
the
input
and
the
hidden
layer
.
We
call
thi
array
'wih
'
(
weight
between
input
and
hidden
layer
)
.
let
's
deactiv
(
drop
out
)
the
node
$
i_2
$
.
We
can
see
in
the
follow
diagram
what
's
happen
:
thi
mean
that
we
have
to
take
out
everi
second
product
of
the
summat
,
which
mean
that
we
have
to
delet
the
whole
second
column
of
the
matrix
.
the
second
element
from
the
input
vector
ha
to
be
delet
as
well
.
now
we
will
examin
what
happen
if
we
take
out
a
hidden
node
.
We
take
out
the
first
hidden
node
,
i.e
.
$
h_1
$
.
In
thi
case
,
we
can
remov
the
complet
first
line
of
our
weight
matrix
:
take
out
a
hidden
node
affect
the
next
weight
matrix
as
well
.
let
's
have
a
look
at
what
is
happen
in
the
network
graph
:
It
is
easi
to
see
that
the
first
column
of
the
who
weight
matrix
ha
to
be
remov
again
:
So
far
we
have
arbitrarili
chosen
one
node
to
deactiv
.
the
dropout
approach
mean
that
we
randomli
choos
a
certain
number
of
node
from
the
input
and
the
hidden
layer
,
which
remain
activ
and
turn
off
the
other
node
of
these
layer
.
after
thi
we
can
train
a
part
of
our
learn
set
with
thi
network
.
the
next
step
consist
in
activ
all
the
node
again
and
randomli
chose
other
node
.
It
is
also
possibl
to
train
the
whole
train
set
with
the
randomli
creat
dropout
network
.
We
present
three
possibl
randomli
chosen
dropout
network
in
the
follow
three
diagram
:
now
it
is
time
to
think
about
a
possibl
python
implement
.
We
will
start
with
the
weight
matrix
between
input
and
hidden
layer
.
We
will
randomli
creat
a
weight
matrix
for
10
input
node
and
5
hidden
node
.
We
fill
our
matrix
with
random
number
between
-10
and
10
,
which
are
not
proper
weight
valu
,
but
thi
way
we
can
see
better
what
is
go
on
:
import
numpi
as
np
import
random
input_nod
=
10
hidden_nod
=
5
output_nod
=
7
wih
=
np.random.randint
(
-10
,
10
,
(
hidden_nod
,
input_nod
)
)
wih
the
previou
python
code
return
the
follow
output
:
array
(
[
[
-6
,
-8
,
-3
,
-7
,
2
,
-9
,
-3
,
-5
,
-6
,
4
]
,
[
5
,
3
,
7
,
-4
,
4
,
8
,
-2
,
-4
,
7
,
7
]
,
[
9
,
-7
,
4
,
0
,
4
,
0
,
-3
,
-6
,
-2
,
7
]
,
[
-8
,
-9
,
-4
,
-5
,
-9
,
8
,
-8
,
-8
,
-2
,
-3
]
,
[
3
,
-10
,
0
,
-3
,
4
,
0
,
0
,
2
,
-7
,
-9
]
]
)
We
will
choos
now
the
activ
node
for
the
input
layer
.
We
calcul
random
indic
for
the
activ
node
:
active_input_percentag
=
0.7
active_input_nod
=
int
(
input_nod
*
active_input_percentag
)
active_input_indic
=
sort
(
random.sampl
(
rang
(
0
,
input_nod
)
,
active_input_nod
)
)
active_input_indic
the
abov
python
code
return
the
follow
result
:
[
0
,
1
,
2
,
5
,
7
,
8
,
9
]
We
learn
abov
that
we
have
to
remov
the
column
$
j
$
,
if
the
node
$
i_j
$
is
remov
.
We
can
easili
accomplish
thi
for
all
deactiv
node
by
use
the
slice
oper
with
the
activ
node
:
wih_old
=
wih.copi
(
)
wih
=
wih
[
:
,
active_input_indic
]
wih
thi
get
us
the
follow
result
:
array
(
[
[
-6
,
-8
,
-3
,
-9
,
-5
,
-6
,
4
]
,
[
5
,
3
,
7
,
8
,
-4
,
7
,
7
]
,
[
9
,
-7
,
4
,
0
,
-6
,
-2
,
7
]
,
[
-8
,
-9
,
-4
,
8
,
-8
,
-2
,
-3
]
,
[
3
,
-10
,
0
,
0
,
2
,
-7
,
-9
]
]
)
As
we
have
mention
befor
,
we
will
have
to
modifi
both
the
'wih
'
and
the
'who
'
matrix
:
who
=
np.random.randint
(
-10
,
10
,
(
output_nod
,
hidden_nod
)
)
print
(
who
)
active_hidden_percentag
=
0.7
active_hidden_nod
=
int
(
hidden_nod
*
active_hidden_percentag
)
active_hidden_indic
=
sort
(
random.sampl
(
rang
(
0
,
hidden_nod
)
,
active_hidden_nod
)
)
print
(
active_hidden_indic
)
who_old
=
who.copi
(
)
who
=
who
[
:
,
active_hidden_indic
]
print
(
who
)
[
[
3
6
-3
-9
4
]
[
-10
1
2
5
7
]
[
-8
1
-3
6
3
]
[
-3
-3
6
-5
-3
]
[
-4
-9
8
-3
5
]
[
8
4
-8
2
7
]
[
-2
2
3
-8
-5
]
]
[
0
,
2
,
3
]
[
[
3
-3
-9
]
[
-10
2
5
]
[
-8
-3
6
]
[
-3
6
-5
]
[
-4
8
-3
]
[
8
-8
2
]
[
-2
3
-8
]
]
We
have
to
chang
wih
accordingli
:
wih
=
wih
[
active_hidden_indic
]
wih
the
abov
code
return
the
follow
:
array
(
[
[
-6
,
-8
,
-3
,
-9
,
-5
,
-6
,
4
]
,
[
9
,
-7
,
4
,
0
,
-6
,
-2
,
7
]
,
[
-8
,
-9
,
-4
,
8
,
-8
,
-2
,
-3
]
]
)
the
follow
python
code
summar
the
sniplet
from
abov
:
import
numpi
as
np
import
random
input_nod
=
10
hidden_nod
=
5
output_nod
=
7
wih
=
np.random.randint
(
-10
,
10
,
(
hidden_nod
,
input_nod
)
)
print
(
``
wih
:
\n
''
,
wih
)
who
=
np.random.randint
(
-10
,
10
,
(
output_nod
,
hidden_nod
)
)
print
(
``
who
:
\n
''
,
who
)
active_input_percentag
=
0.7
active_hidden_percentag
=
0.7
active_input_nod
=
int
(
input_nod
*
active_input_percentag
)
active_input_indic
=
sort
(
random.sampl
(
rang
(
0
,
input_nod
)
,
active_input_nod
)
)
print
(
``
\nactiv
input
indic
:
``
,
active_input_indic
)
active_hidden_nod
=
int
(
hidden_nod
*
active_hidden_percentag
)
active_hidden_indic
=
sort
(
random.sampl
(
rang
(
0
,
hidden_nod
)
,
active_hidden_nod
)
)
print
(
``
activ
hidden
indic
:
``
,
active_hidden_indic
)
wih_old
=
wih.copi
(
)
wih
=
wih
[
:
,
active_input_indic
]
print
(
``
\nwih
after
deactiv
input
node
:
\n
''
,
wih
)
wih
=
wih
[
active_hidden_indic
]
print
(
``
\nwih
after
deactiv
hidden
node
:
\n
''
,
wih
)
who_old
=
who.copi
(
)
who
=
who
[
:
,
active_hidden_indic
]
print
(
``
\nwih
after
deactiv
hidden
node
:
\n
''
,
who
)
wih
:
[
[
-4
9
3
5
-9
5
-3
0
9
1
]
[
4
7
-7
3
-4
7
4
-5
6
2
]
[
5
8
1
-10
-8
-6
7
-4
-6
8
]
[
6
-3
7
4
-7
-4
0
8
9
1
]
[
6
-1
4
-3
5
-5
-5
5
4
-7
]
]
who
:
[
[
-6
2
-2
4
0
]
[
-5
-3
3
-4
-10
]
[
4
6
-7
-7
-1
]
[
-4
-1
-10
0
-8
]
[
8
-2
9
-8
-9
]
[
-6
0
-2
1
-8
]
[
1
-4
-2
-6
-5
]
]
activ
input
indic
:
[
1
,
3
,
4
,
5
,
7
,
8
,
9
]
activ
hidden
indic
:
[
0
,
1
,
2
]
wih
after
deactiv
input
node
:
[
[
9
5
-9
5
0
9
1
]
[
7
3
-4
7
-5
6
2
]
[
8
-10
-8
-6
-4
-6
8
]
[
-3
4
-7
-4
8
9
1
]
[
-1
-3
5
-5
5
4
-7
]
]
wih
after
deactiv
hidden
node
:
[
[
9
5
-9
5
0
9
1
]
[
7
3
-4
7
-5
6
2
]
[
8
-10
-8
-6
-4
-6
8
]
]
wih
after
deactiv
hidden
node
:
[
[
-6
2
-2
]
[
-5
-3
3
]
[
4
6
-7
]
[
-4
-1
-10
]
[
8
-2
9
]
[
-6
0
-2
]
[
1
-4
-2
]
]
import
numpi
as
np
import
random
from
scipy.speci
import
expit
as
activation_funct
from
scipy.stat
import
truncnorm
def
truncated_norm
(
mean=0
,
sd=1
,
low=0
,
upp=10
)
:
return
truncnorm
(
(
low
-
mean
)
/
sd
,
(
upp
-
mean
)
/
sd
,
loc=mean
,
scale=sd
)
class
neuralnetwork
:
def
__init__
(
self
,
no_of_in_nod
,
no_of_out_nod
,
no_of_hidden_nod
,
learning_r
,
bias=non
)
:
self.no_of_in_nod
=
no_of_in_nod
self.no_of_out_nod
=
no_of_out_nod
self.no_of_hidden_nod
=
no_of_hidden_nod
self.learning_r
=
learning_r
self.bia
=
bia
self.create_weight_matric
(
)
def
create_weight_matric
(
self
)
:
X
=
truncated_norm
(
mean=2
,
sd=1
,
low=-0.5
,
upp=0.5
)
bias_nod
=
1
if
self.bia
els
0
n
=
(
self.no_of_in_nod
+
bias_nod
)
*
self.no_of_hidden_nod
X
=
truncated_norm
(
mean=2
,
sd=1
,
low=-0.5
,
upp=0.5
)
self.wih
=
x.rv
(
n
)
.reshap
(
(
self.no_of_hidden_nod
,
self.no_of_in_nod
+
bias_nod
)
)
n
=
(
self.no_of_hidden_nod
+
bias_nod
)
*
self.no_of_out_nod
X
=
truncated_norm
(
mean=2
,
sd=1
,
low=-0.5
,
upp=0.5
)
self.who
=
x.rv
(
n
)
.reshap
(
(
self.no_of_out_nod
,
(
self.no_of_hidden_nod
+
bias_nod
)
)
)
def
dropout_weight_matric
(
self
,
active_input_percentage=0.70
,
active_hidden_percentage=0.70
)
:
#
restor
wih
array
,
if
it
had
been
use
for
dropout
self.wih_orig
=
self.wih.copi
(
)
self.no_of_in_nodes_orig
=
self.no_of_in_nod
self.no_of_hidden_nodes_orig
=
self.no_of_hidden_nod
self.who_orig
=
self.who.copi
(
)
active_input_nod
=
int
(
self.no_of_in_nod
*
active_input_percentag
)
active_input_indic
=
sort
(
random.sampl
(
rang
(
0
,
self.no_of_in_nod
)
,
active_input_nod
)
)
active_hidden_nod
=
int
(
self.no_of_hidden_nod
*
active_hidden_percentag
)
active_hidden_indic
=
sort
(
random.sampl
(
rang
(
0
,
self.no_of_hidden_nod
)
,
active_hidden_nod
)
)
self.wih
=
self.wih
[
:
,
active_input_indic
]
[
active_hidden_indic
]
self.who
=
self.who
[
:
,
active_hidden_indic
]
self.no_of_hidden_nod
=
active_hidden_nod
self.no_of_in_nod
=
active_input_nod
return
active_input_indic
,
active_hidden_indic
def
weight_matrices_reset
(
self
,
active_input_indic
,
active_hidden_indic
)
:
``
''
''
self.wih
and
self.who
contain
the
newli
adapt
valu
from
the
activ
node
.
We
have
to
reconstruct
the
origin
weight
matric
by
assign
the
new
valu
from
the
activ
node
``
''
''
temp
=
self.wih_orig.copi
(
)
[
:
,active_input_indic
]
temp
[
active_hidden_indic
]
=
self.wih
self.wih_orig
[
:
,
active_input_indic
]
=
temp
self.wih
=
self.wih_orig.copi
(
)
self.who_orig
[
:
,
active_hidden_indic
]
=
self.who
self.who
=
self.who_orig.copi
(
)
self.no_of_in_nod
=
self.no_of_in_nodes_orig
self.no_of_hidden_nod
=
self.no_of_hidden_nodes_orig
def
train_singl
(
self
,
input_vector
,
target_vector
)
:
``
''
''
input_vector
and
target_vector
can
be
tupl
,
list
or
ndarray
``
''
''
if
self.bia
:
#
ad
bia
node
to
the
end
of
the
input_vector
input_vector
=
np.concaten
(
(
input_vector
,
[
self.bia
]
)
)
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
target_vector
=
np.array
(
target_vector
,
ndmin=2
)
.T
output_vector1
=
np.dot
(
self.wih
,
input_vector
)
output_vector_hidden
=
activation_funct
(
output_vector1
)
if
self.bia
:
output_vector_hidden
=
np.concaten
(
(
output_vector_hidden
,
[
[
self.bia
]
]
)
)
output_vector2
=
np.dot
(
self.who
,
output_vector_hidden
)
output_vector_network
=
activation_funct
(
output_vector2
)
output_error
=
target_vector
-
output_vector_network
#
updat
the
weight
:
tmp
=
output_error
*
output_vector_network
*
(
1.0
-
output_vector_network
)
tmp
=
self.learning_r
*
np.dot
(
tmp
,
output_vector_hidden.t
)
self.who
+=
tmp
#
calcul
hidden
error
:
hidden_error
=
np.dot
(
self.who.t
,
output_error
)
#
updat
the
weight
:
tmp
=
hidden_error
*
output_vector_hidden
*
(
1.0
-
output_vector_hidden
)
if
self.bia
:
x
=
np.dot
(
tmp
,
input_vector.t
)
[
:
-1
,
:
]
els
:
x
=
np.dot
(
tmp
,
input_vector.t
)
self.wih
+=
self.learning_r
*
x
def
train
(
self
,
data_array
,
labels_one_hot_array
,
epochs=1
,
active_input_percentage=0.70
,
active_hidden_percentage=0.70
,
no_of_dropout_test
=
10
)
:
partition_length
=
int
(
len
(
data_array
)
/
no_of_dropout_test
)
for
epoch
in
rang
(
epoch
)
:
print
(
``
epoch
:
``
,
epoch
)
for
start
in
rang
(
0
,
len
(
data_array
)
,
partition_length
)
:
active_in_indic
,
active_hidden_indic
=
\
self.dropout_weight_matric
(
active_input_percentag
,
active_hidden_percentag
)
for
i
in
rang
(
start
,
start
+
partition_length
)
:
self.train_singl
(
data_array
[
i
]
[
active_in_indic
]
,
labels_one_hot_array
[
i
]
)
self.weight_matrices_reset
(
active_in_indic
,
active_hidden_indic
)
def
confusion_matrix
(
self
,
data_array
,
label
)
:
cm
=
{
}
for
i
in
rang
(
len
(
data_array
)
)
:
re
=
self.run
(
data_array
[
i
]
)
res_max
=
res.argmax
(
)
target
=
label
[
i
]
[
0
]
if
(
target
,
res_max
)
in
cm
:
cm
[
(
target
,
res_max
)
]
+=
1
els
:
cm
[
(
target
,
res_max
)
]
=
1
return
cm
def
run
(
self
,
input_vector
)
:
#
input_vector
can
be
tupl
,
list
or
ndarray
if
self.bia
:
#
ad
bia
node
to
the
end
of
the
input_vector
input_vector
=
np.concaten
(
(
input_vector
,
[
self.bia
]
)
)
input_vector
=
np.array
(
input_vector
,
ndmin=2
)
.T
output_vector
=
np.dot
(
self.wih
,
input_vector
)
output_vector
=
activation_funct
(
output_vector
)
if
self.bia
:
output_vector
=
np.concaten
(
(
output_vector
,
[
[
self.bia
]
]
)
)
output_vector
=
np.dot
(
self.who
,
output_vector
)
output_vector
=
activation_funct
(
output_vector
)
return
output_vector
def
evalu
(
self
,
data
,
label
)
:
correct
,
wrong
=
0
,
0
for
i
in
rang
(
len
(
data
)
)
:
re
=
self.run
(
data
[
i
]
)
res_max
=
res.argmax
(
)
if
res_max
==
label
[
i
]
:
correct
+=
1
els
:
wrong
+=
1
return
correct
,
wrong
import
pickl
with
open
(
``
data/mnist/pickled_mnist.pkl
''
,
``
br
''
)
as
fh
:
data
=
pickle.load
(
fh
)
train_img
=
data
[
0
]
test_img
=
data
[
1
]
train_label
=
data
[
2
]
test_label
=
data
[
3
]
train_labels_one_hot
=
data
[
4
]
test_labels_one_hot
=
data
[
5
]
image_s
=
28
#
width
and
length
no_of_different_label
=
10
#
i.e
.
0
,
1
,
2
,
3
,
...
,
9
image_pixel
=
image_s
*
image_s
part
=
10
partition_length
=
int
(
len
(
train_img
)
/
part
)
print
(
partition_length
)
start
=
0
for
start
in
rang
(
0
,
len
(
train_img
)
,
partition_length
)
:
print
(
start
,
start
+
partition_length
)
6000
0
6000
6000
12000
12000
18000
18000
24000
24000
30000
30000
36000
36000
42000
42000
48000
48000
54000
54000
60000
epoch
=
3
simple_network
=
neuralnetwork
(
no_of_in_nod
=
image_pixel
,
no_of_out_nod
=
10
,
no_of_hidden_nod
=
100
,
learning_r
=
0.1
)
simple_network.train
(
train_img
,
train_labels_one_hot
,
active_input_percentage=1
,
active_hidden_percentage=1
,
no_of_dropout_test
=
100
,
epochs=epoch
)
epoch
:
0
epoch
:
1
epoch
:
2
correct
,
wrong
=
simple_network.evalu
(
train_img
,
train_label
)
print
(
``
accruraci
train
:
``
,
correct
/
(
correct
+
wrong
)
)
correct
,
wrong
=
simple_network.evalu
(
test_img
,
test_label
)
print
(
``
accruraci
:
test
''
,
correct
/
(
correct
+
wrong
)
)
accruraci
train
:
0.9317833333333333
accruraci
:
test
0.9296
previou
chapter
:
train
and
test
with
mnist
next
chapter
:
neural
network
with
scikit
Â©
2011
-
2018
,
bernd
klein
,
bodenseo
;
design
by
denis
mitchinson
adapt
for
python-course.eu
by
bernd
klein
